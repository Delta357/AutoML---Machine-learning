title,score,id,subreddit,url,num_comments,body,created,timestamp,text_clean
[D] Simple Questions Thread,14,qorekl,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qorekl/d_simple_questions_thread/,54,"Please post your questions here instead of creating a new thread  Encourage others who create new posts for questions to post here instead 

Thread will stay alive until next one so keep posting after the date in the title 

Thanks to everyone for answering questions in the previous thread ",1636300818.0,2021-11-07 17:00:18,please post questions instead creating new thread encourage others create new posts questions post instead thread stay alive next one keep posting date title thanks everyone answering questions previous thread
[P][R] Rocket-recycling with Reinforcement Learning,607,qt2tws,MachineLearning,https://v.redd.it/enkc1p6oldz71,33,nan,1636815127.0,2021-11-13 15:52:07,nan
[Project] PyTorch Implementations of 37 GAN papers (including BigGAN and StyleGAN2),334,qt10az,MachineLearning,https://i.redd.it/fjf94vuj4dz71.png,22,nan,1636809198.0,2021-11-13 14:13:18,nan
[P] Using Talknet to clone Dreams' voice.,70,qt9yql,MachineLearning,https://v.redd.it/hmw9gpizefz71,19,nan,1636837002.0,2021-11-13 21:56:42,nan
[R] StyleGAN of All Trades: Image Manipulation with Only Pretrained StyleGAN,699,qsw47b,MachineLearning,https://i.redd.it/arv5dyfjfbz71.jpg,12,nan,1636788679.0,2021-11-13 08:31:19,nan
[R] Pruning Attention Heads of Transformer Models Using A* Search: A Novel Approach to Compress Big NLP Architectures,8,qtl5fx,MachineLearning,https://arxiv.org/abs/2110.15225,1,nan,1636877009.0,2021-11-14 09:03:29,nan
Walk-Forward Target Encoding and Data Leakage [D],3,qtiqlw,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qtiqlw/walkforward_target_encoding_and_data_leakage_d/,1,"Hi 

I am working on a time series problem where I need to also test historical predictions through time  using a walk forward procedure   There is a  data leakage  when dealing with time series and using simple target encoding features like  

df groupby col \[target\] transform mean  

 xB 

I am wondering if anyone has built a custom function to build these target encoding features without leaking data  Kind of like a walk forward target encoding function ",1636866927.0,2021-11-14 06:15:27,hi working time series problem need also test historical predictions time using walk forward procedure data leakage dealing time series using simple target encoding features like df groupby col \[target\] transform mean xb wondering anyone built custom function build target encoding features without leaking data kind like walk forward target encoding function
[N] Introduction to Data Science book updated,0,qtlmql,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qtlmql/n_introduction_to_data_science_book_updated/,0,"Hello All 

I have updated my ongoing Data Science book  The following sections were added 

  [Vectors] https  datascience book gitlab io book html _vectors 

  [Matrices] https  datascience book gitlab io book html _matrices 

  [Sigmoid] https  datascience book gitlab io book html _sigmoid 

  [K Means Clustering] https  datascience book gitlab io book html _k_means_clustering 

  [Gradient Descent] https  datascience book gitlab io book html _gradient_descent 

I  would be very happy if you people read it and suggest feedback and  improvements  One can get the entire book in PDF and epub format [here] https  datascience book gitlab io  

Thank you ",1636879153.0,2021-11-14 09:39:13,hello updated ongoing data science book following sections added [vectors] datascience book gitlab io book html _vectors [matrices] datascience book gitlab io book html _matrices [sigmoid] datascience book gitlab io book html _sigmoid [k means clustering] datascience book gitlab io book html _k_means_clustering [gradient descent] datascience book gitlab io book html _gradient_descent would happy people read suggest feedback improvements one get entire book pdf epub format [here] datascience book gitlab io thank
[P] Cedille: The largest French language model,15,qt5ekt,MachineLearning,https://github.com/coteries/cedille-ai,1,nan,1636823040.0,2021-11-13 18:04:00,nan
[D] Analysis of ICLR 2022 Review Scores,24,qszmuu,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qszmuu/d_analysis_of_iclr_2022_review_scores/,1,"We analysed the relationship between ICLR  review scores and factors such as social media popularity and presence in Arxiv  [twitter thread] https  twitter com labmlai status   

Here are some of the results of the analysis 

* Papers that were present on Arxiv had higher recommendation scores 

Mean review scores     papers on Arxiv       papers not on Arxiv 

https  preview redd it kwatkcz png width= format=png auto=webp s=acabbddfeaddbbeaef

* Papers that were shared on Twitter  with  =  likes  also had higher recommendation scores than other papers 

Mean review scores     Tweeted       not Tweeted 

https  preview redd it mxiqzvkcz png width= format=png auto=webp s=ebeaddbbfcbfefeed

* Papers that had source code available  or promised to upload soon with empty repos  also got better reviews 

Mean review scores     with code       without code 

https  preview redd it xeyznzykcz png width= format=png auto=webp s=aabffaaefefccabfa

* Scatter plot of likes on Twitter against the review score 

  There is a small positive correlation between Twitter likes and review scores  The correlation   
  coefficient is   

https  preview redd it yvnxrgulcz png width= format=png auto=webp s=cbbcddbadaceecdcdbbcd

All ICLR  submissions sorted by review scores can be found here   [https  papers labml ai papers iclr\_ sort\_by=conference\_score dsc=] https  papers labml ai papers iclr_ sort_by=conference_score dsc= ",1636803906.0,2021-11-13 12:45:06,analysed relationship iclr review scores factors social media popularity presence arxiv [twitter thread] twitter com labmlai status results analysis * papers present arxiv higher recommendation scores mean review scores papers arxiv papers arxiv preview redd kwatkcz png width= format=png auto=webp s=acabbddfeaddbbeaef * papers shared twitter = likes also higher recommendation scores papers mean review scores tweeted tweeted preview redd mxiqzvkcz png width= format=png auto=webp s=ebeaddbbfcbfefeed * papers source code available promised upload soon empty repos also got better reviews mean review scores code without code preview redd xeyznzykcz png width= format=png auto=webp s=aabffaaefefccabfa * scatter plot likes twitter review score small positive correlation twitter likes review scores correlation coefficient preview redd yvnxrgulcz png width= format=png auto=webp s=cbbcddbadaceecdcdbbcd iclr submissions sorted review scores found [ papers labml ai papers iclr\_ sort\_by=conference\_score dsc=] papers labml ai papers iclr_ sort_by=conference_score dsc=
[P] Lyric Studio - Artificial Intelligence Song Lyrics,1,qtik3a,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qtik3a/p_lyric_studio_artificial_intelligence_song_lyrics/,0,"I created this demo in two days  last weekend   created an AI Generated Song Lyrics app  Thought this subreddit would love it   PyTorch for text generation  and also does live sentiment analysis with background shading

Link  [https  LyricStudio com] https  LyricStudio com 

Do you have any feedback or improvements ",1636866248.0,2021-11-14 06:04:08,created demo two days last weekend created ai generated song lyrics app thought subreddit would love pytorch text generation also live sentiment analysis background shading link [ lyricstudio com] lyricstudio com feedback improvements
"[D] Is capacity of RAM the only important factor, or is the speed and bandwidth also important for deep learning models?",3,qta1dt,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qta1dt/d_is_capacity_of_ram_the_only_important_factor_or/,6,"I will soon be building a deep learning machine  but given the introduction of DDR in the new intel platforms  I am unsure whether the extra cost is worth it  DDR introduces higher speed with much more bandwidth  but is this important for deep learning  Or is the cost not worth it 

For example  for the same cost  I can get higher capacity of RAM  say GB DDR at average speed  or I can get smaller capacity  say GB DDR  but with double the speed and double the bandwidth 

Which is more important ",1636837217.0,2021-11-13 22:00:17,soon building deep learning machine given introduction ddr new intel platforms unsure whether extra cost worth ddr introduces higher speed much bandwidth important deep learning cost worth example cost get higher capacity ram say gb ddr average speed get smaller capacity say gb ddr double speed double bandwidth important
[D] (Paper Overview) MAE: Masked Autoencoders Are Scalable Vision Learners,6,qt4y6g,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qt4y6g/d_paper_overview_mae_masked_autoencoders_are/,0," **Video**

[https  youtu be LKixqSPz] https  youtu be LKixqSPz 

**Paper**

[https  arxiv org abs  ] https  arxiv org abs   

**Abstract**

 This paper shows that masked autoencoders  MAE  are scalable self supervised learners for computer vision  Our MAE approach is simple  we mask random patches of the input image and reconstruct the missing pixels  It is based on two core designs  First  we develop an asymmetric encoder decoder architecture  with an encoder that operates only on the visible subset of patches  without mask tokens   along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens  Second  we find that masking a high proportion of the input image  e g     yields a nontrivial and meaningful self supervisory task  Coupling these two designs enables us to train large models efficiently and effectively  we accelerate training  by x or more  and improve accuracy  Our scalable approach allows for learning high capacity models that generalize well  e g  a vanilla ViT Huge model achieves the best accuracy     among methods that use only ImageNet K data  Transfer performance in downstream tasks outperforms supervised pre training and shows promising scaling behavior ",1636821678.0,2021-11-13 17:41:18,**video** [ youtu lkixqspz] youtu lkixqspz **paper** [ arxiv org abs ] arxiv org abs **abstract** paper shows masked autoencoders mae scalable self supervised learners computer vision mae approach simple mask random patches input image reconstruct missing pixels based two core designs first develop asymmetric encoder decoder architecture encoder operates visible subset patches without mask tokens along lightweight decoder reconstructs original image latent representation mask tokens second find masking high proportion input image e g yields nontrivial meaningful self supervisory task coupling two designs enables us train large models efficiently effectively accelerate training x improve accuracy scalable approach allows learning high capacity models generalize well e g vanilla vit huge model achieves best accuracy among methods use imagenet k data transfer performance downstream tasks outperforms supervised pre training shows promising scaling behavior
[D] Algorithms for correlation of events/issues,3,qt90rt,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qt90rt/d_algorithms_for_correlation_of_eventsissues/,0,"Generally any application software co exists with multiple other software where a problem with one software can have a cascading effect on some other software  somewhere else in the stack  E g 

  you deploy you application as a pod in kubernetes  orchestration software 
  but the pod or container is not running after sometime because the ec machine  or any virtual machine  on which the pod was scheduled to run has some issues 
  the ec machine or the vm is having some issues because the autoscaling software that is supposed to manage the vms properly is not working correctly 
  the autoscaling software is not working correctly because of some other dependent system not doing this job 
  basically there can be a CHAIN of issues where one issue can have a significant cascading effect on many other dependent software systems 

You can apply this logic to many other places  Imagine your application is not working properly BECAUSE your load balancer is not working properly which is BECAUSE you load balancer vms are having networking issues because you have some datacenter level failures etc  You have all the logs files spread across the stack which reports these issues independently but usually it takes manual effort to CORRELATE them to figure out what is the root cause 

typically the user sees the symptoms at a high level in the stack  e g  application is working properly and then they will start debugging and then finally you figure out somewhere down the stack something is wrong  This usually takes specific expertise  SRE  and takes time to arrive at the root cause 

basically what is happening is here is a CHAIN of events with cascading effect  There are ways to catch this using monitoring dashboards but the problem is usually those monitoring dashboard are setup in a static way  manually setup and typically has a maintenance problems long term  i e  if there is a change in one of the software version which wants you to change a the dashboard accordingly  then you might forget to modify the monitoring dashboard accordingly etc  Also setting up these monitoring dashboards are very specific to the problem  this means you need to setup various different kinds of dashboards for different systems   scenarios 

We want to apply AI to this problem if possible  We want to come up with a well trained AI based system which can tell you which is the actual root cause issue if you give it  different issues that happened around a specific time interval  

E g 

\  We will have a model that detects various issues  anomaly detection based software which we already have   So we now have the list of issues that happened across the stack around a certain time interval  Lets say  issues

\  Now let s say we have  different issues that happens around a particular time interval    mins   we would like to send all the  issues to AI based system and we want it to tell that out of these  issues   or  issues is most probably the root cause which could have caused all the other issues 

If we have such AI based system that will basically CORRELATE multiple issues and tell us the ROOT CAUSE i e  basically which of those issues is likely to have caused the other issues  that will be very useful 

Is this possible  Any thoughts guidance will be greatly appreciated  What are all the typical algorithms approaches people apply for this kind of problem 

Imagine a use case for this where if we have this systems  I can send in a bunch of alerts coming in  assume the alert has the relevant data about the issue attached to it  and this AI system can process it and segregate those alerts in a such a way  it informs the user which alert is actual issue which is causing other  This would mean user can quickly resolve it getting rid of other alerts  There are many such use cases like this",1636834058.0,2021-11-13 21:07:38,generally application software co exists multiple software problem one software cascading effect software somewhere else stack e g deploy application pod kubernetes orchestration software pod container running sometime ec machine virtual machine pod scheduled run issues ec machine vm issues autoscaling software supposed manage vms properly working correctly autoscaling software working correctly dependent system job basically chain issues one issue significant cascading effect many dependent software systems apply logic many places imagine application working properly load balancer working properly load balancer vms networking issues datacenter level failures etc logs files spread across stack reports issues independently usually takes manual effort correlate figure root cause typically user sees symptoms high level stack e g application working properly start debugging finally figure somewhere stack something wrong usually takes specific expertise sre takes time arrive root cause basically happening chain events cascading effect ways catch using monitoring dashboards problem usually monitoring dashboard setup static way manually setup typically maintenance problems long term e change one software version wants change dashboard accordingly might forget modify monitoring dashboard accordingly etc also setting monitoring dashboards specific problem means need setup various different kinds dashboards different systems scenarios want apply ai problem possible want come well trained ai based system tell actual root cause issue give different issues happened around specific time interval e g \ model detects various issues anomaly detection based software already list issues happened across stack around certain time interval lets say issues \ let say different issues happens around particular time interval mins would like send issues ai based system want tell issues issues probably root cause could caused issues ai based system basically correlate multiple issues tell us root cause e basically issues likely caused issues useful possible thoughts guidance greatly appreciated typical algorithms approaches people apply kind problem imagine use case systems send bunch alerts coming assume alert relevant data issue attached ai system process segregate alerts way informs user alert actual issue causing would mean user quickly resolve getting rid alerts many use cases like
[D] Opinion: the recent paper on buggy resizing libraries is misleading,196,qsl5jj,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qsl5jj/d_opinion_the_recent_paper_on_buggy_resizing/,24,"A recent paper [On Buggy Resizing Libraries and Surprising Subtleties in FID Calculation] https  arxiv org abs    claims that the image downsampling methods of OpenCV  Tensorflow and PyTorch are  buggy   and therefore PIL should be used instead for FID estimation  The corresponding twitter post was quite popular during the last week 

I believe that this claim and the main figure of the paper is misleading  because the issue is caused by aliasing and it can be fixed by simply setting the right parameters in the functions they used 

 xB 

[Their main figure] https  preview redd it valmkwdz png width= format=png auto=webp s=cedbcdebbdffb 

 xB 

[My reproduction and fixed results \ bilinear\ ] https  preview redd it tcmvxez png width= format=png auto=webp s=cffeabbdafdb 

On the bottom image  you can see the reproduced and antialiased downsampling results in all the frameworks  In all cases  a single parameter modification was enough to mitigate the issue  I shared the code and my complete opinion in this repository  [https  github com beresandras buggy resizing critique] https  github com beresandras buggy resizing critique 

Though I believe that the discussion on whether antialiasing should be a default in image libraries is valuable  in my opinion none of these methods is  buggy   and the paper presents the issue in a sensationalist way ",1636751991.0,2021-11-12 22:19:51,recent paper [on buggy resizing libraries surprising subtleties fid calculation] arxiv org abs claims image downsampling methods opencv tensorflow pytorch buggy therefore pil used instead fid estimation corresponding twitter post quite popular last week believe claim main figure paper misleading issue caused aliasing fixed simply setting right parameters functions used xb [their main figure] preview redd valmkwdz png width= format=png auto=webp s=cedbcdebbdffb xb [my reproduction fixed results \ bilinear\ ] preview redd tcmvxez png width= format=png auto=webp s=cffeabbdafdb bottom image see reproduced antialiased downsampling results frameworks cases single parameter modification enough mitigate issue shared code complete opinion repository [ github com beresandras buggy resizing critique] github com beresandras buggy resizing critique though believe discussion whether antialiasing default image libraries valuable opinion none methods buggy paper presents issue sensationalist way
"[P] KalidoKit – Face, Pose, and Hand Tracking Kinematics",7,qt5d0g,MachineLearning,https://github.com/yeemachine/kalidokit,1,nan,1636822920.0,2021-11-13 18:02:00,nan
[P] tsflex: flexible and efficient feature extraction for time series,12,qsvzio,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qsvzio/p_tsflex_flexible_and_efficient_feature/,2,"Are you looking for a time series feature extraction package that is both efficient and flexible  [tsflex] https  github com predict idlab tsflex  has got you covered  They just published a release that allows integration with tsfresh as well 

Go check it out 👉 [https  github com predict idlab tsflex releases tag v  ] https  github com predict idlab tsflex releases tag v   ",1636788149.0,2021-11-13 08:22:29,looking time series feature extraction package efficient flexible [tsflex] github com predict idlab tsflex got covered published release allows integration tsfresh well go check 👉 [ github com predict idlab tsflex releases tag v ] github com predict idlab tsflex releases tag v
[R] DeepSteal: Advanced Model Extractions Leveraging Efficient Weight Stealing in Memories,2,qt4wxw,MachineLearning,https://arxiv.org/abs/2111.04625,1,nan,1636821570.0,2021-11-13 17:39:30,nan
[P] Text-to-image ruDALL-E Kandinsky (XXL) 12 billion parameter model checkpoint is apparently available for download,26,qsrdyk,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qsrdyk/p_texttoimage_rudalle_kandinsky_xxl_12_billion/,4,"[Here] https  sbercloud ru ru datahub rugptfamily ru dalle b  is a download page for ruDALL E Kandinsky  XXL   billion parameter model checkpoint  [English translation] https  sbercloud ru translate goog ru datahub rugptfamily ru dalle b _x_tr_sl=auto _x_tr_tl=en _x_tr_hl=en US _x_tr_pto=nui   I did not sign up to try to download the file s   The B parameter model should be available per my interpretation of [this press release] https  www sberbank com news and media press releases article newsID=a f  a aecabc blockID= regionID= lang=en type=NEWS   Edit  According to [this comment] https  www reddit com r MachineLearning comments qsrdyk comment hkfslaj   public availability should be on December  

Prior mention of ruDALL E in this subreddit 

[Text to image models ruDALL E Kandinsky  XXL    billion parameters  and ruDALL E Malevich  XL     billion parameters   A demo for the latter is available ] https  www reddit com r MachineLearning comments qlbye p_texttoimage_models_rudalle_kandinsky_xxl_ 

[ruDALL E model is open source \[P\]] https  www reddit com r MachineLearning comments qmzya rudalle_model_is_opensource_p ",1636771128.0,2021-11-13 03:38:48,[here] sbercloud ru ru datahub rugptfamily ru dalle b download page rudall e kandinsky xxl billion parameter model checkpoint [english translation] sbercloud ru translate goog ru datahub rugptfamily ru dalle b _x_tr_sl=auto _x_tr_tl=en _x_tr_hl=en us _x_tr_pto=nui sign try download file b parameter model available per interpretation [this press release] www sberbank com news media press releases article newsid=a f aecabc blockid= regionid= lang=en type=news edit according [this comment] www reddit com r machinelearning comments qsrdyk comment hkfslaj public availability december prior mention rudall e subreddit [text image models rudall e kandinsky xxl billion parameters rudall e malevich xl billion parameters demo latter available ] www reddit com r machinelearning comments qlbye p_texttoimage_models_rudalle_kandinsky_xxl_ [rudall e model open source \[p\]] www reddit com r machinelearning comments qmzya rudalle_model_is_opensource_p
[D] Nvidia Jetson Thoughts,2,qt1vuy,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qt1vuy/d_nvidia_jetson_thoughts/,10,Has anyone used any of the Nvidia Jetson models for production use cases  We re considering using them for a health application  but I haven t heard much about the developer experience one way or the other  The alternative would probably be running the application on a mobile device ,1636812177.0,2021-11-13 15:02:57,anyone used nvidia jetson models production use cases considering using health application heard much developer experience one way alternative would probably running application mobile device
[D] Can a GIoU loss (generalized intersection over union) be used after an STN module (spatial transformer network)?,2,qt1pnz,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qt1pnz/d_can_a_giou_loss_generalized_intersection_over/,0,I have a model that uses an STN module for number detection and Mean Squared Error loss  But I would like to replace it for GIoU  because MSE doesn t take into account how much of the target area has been detected  only how close individual coordinates are close to the target  But I wonder if this makes sense  Has anyone tried it  or has some insight ,1636811639.0,2021-11-13 14:53:59,model uses stn module number detection mean squared error loss would like replace giou mse take account much target area detected close individual coordinates close target wonder makes sense anyone tried insight
[P] AutoAI – A framework to find the best performing AI/ML model,0,qt5dqt,MachineLearning,https://github.com/blobcity/autoai,0,nan,1636822978.0,2021-11-13 18:02:58,nan
[D] A dilemma of an ML guy in industry,91,qseien,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qseien/d_a_dilemma_of_an_ml_guy_in_industry/,43,I m a Machine Learning Engineer in the healthcare sector and I ve been thinking about this a lot  With the rapid as fuck advancements in research on AI  do I keep up with research more or should I focus on learning about engineering the solutions pipelines  etc    Example  reading a paper using a new GAN vs  reading about a case study ,1636733388.0,2021-11-12 17:09:48,machine learning engineer healthcare sector thinking lot rapid fuck advancements research ai keep research focus learning engineering solutions pipelines etc example reading paper using new gan vs reading case study
"[D] A quick history of GANs - 8 years of GAN evolution, and the intuition behind it explained by Casual GAN Papers",1,qt4k2c,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qt4k2c/d_a_quick_history_of_gans_8_years_of_gan/,0,"This tutorial covers the intuition behind 

* Variational Auto Encoder  VAE 
* The OG GAN
* StyleGAN
* VQGAN

Telegram post  [https  t me casual\_gan ] https  t me casual_gan  

Blog post  [https  www casualganpapers com history of gans survey of popular architectures GAN architectures overview html] https  www casualganpapers com history of gans survey of popular architectures GAN architectures overview html 

https  preview redd it rlfkxwxdz jpg width= format=pjpg auto=webp s=bedbcabfccaeccdefdebd

Subscribe to [Casual GAN Papers] https  t me casual_gan  and follow me on [Twitter] https  twitter com KirillDemochkin  for weekly AI paper summaries and GAN tutorials ",1636820465.0,2021-11-13 17:21:05,tutorial covers intuition behind * variational auto encoder vae * og gan * stylegan * vqgan telegram post [ casual\_gan ] casual_gan blog post [ www casualganpapers com history gans survey popular architectures gan architectures overview html] www casualganpapers com history gans survey popular architectures gan architectures overview html preview redd rlfkxwxdz jpg width= format=pjpg auto=webp s=bedbcabfccaeccdefdebd subscribe [casual gan papers] casual_gan follow [twitter] twitter com kirilldemochkin weekly ai paper summaries gan tutorials
[D] Are there any theoretical analyses on the success of AlphaGo zero?,5,qsv83w,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qsv83w/d_are_there_any_theoretical_analyses_on_the/,1,I am wondering whether there is a theoretical guarantee for the convergence of the network used in AlphaGo zero or for the optimality of the searching algorithm ,1636784987.0,2021-11-13 07:29:47,wondering whether theoretical guarantee convergence network used alphago zero optimality searching algorithm
[D] Causality research in ML is a scam (warning: controversial),188,qs7g4t,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qs7g4t/d_causality_research_in_ml_is_a_scam_warning/,146,"Don t get me wrong  causal inference are *the* most methods for application areas where we observe a bunch of random variable and want to figure out the causal relationship between them 

This rant is not about the method is itself  but how ML research is recently getting exploiting the term  causality  for the sake of the hype and citations  

In ML we have two main paradigms  Supervised learning and RL 

Work on causality  e g  Bernhard Schölkopf  Judea Pearl etc   tells us that is impossible to determine the causal relationship between variables if we only observe them without performing any interaction  Therefore  with supervised learning we cannot learn a causal model but we need to impose one  Period 

Regarding RL  tabular Q learning is guaranteed to converge to the maximum expected reward policy  Period  That s it  nothing else needs to be said about it 

However  despite these two fundamental statements  there is currently growing a hype in general ML research about causality  I am completely fine with causality research as long as it focuses on the application area mentioned in my first sentence  But this recent trend brings the concept into computer vision  NLP  etc    where things become vague quite fast  exaggerated by the fact that research on causality can be already extremely vague and deeply philosophical  e g  what s the practical implication of Newcomb s paradox  

In computer vision no causal model is known  Even the vision processing of humans or animals is very little understood  Moreover  CV tasks are inherently under specified  For instance  is a cartoon drawing of an elephant still an elephant  Or is is out of distribution  OOD   or its own class  or multiple classes  Are we talking about the causal relationship of pixels  patches  or concepts  What makes an elephant ear an elephant ear 

This vagueness  combined with the general trend in ML of throwing a bunch of overly complex math statements into a paper to impress the reviewers  is really concerning 

I bet that there will be hundreds of papers on this topic be published in the next years that contribute very little to our understanding  but will create millions of  self  citations ",1636708549.0,2021-11-12 10:15:49,get wrong causal inference *the* methods application areas observe bunch random variable want figure causal relationship rant method ml research recently getting exploiting term causality sake hype citations ml two main paradigms supervised learning rl work causality e g bernhard schölkopf judea pearl etc tells us impossible determine causal relationship variables observe without performing interaction therefore supervised learning cannot learn causal model need impose one period regarding rl tabular q learning guaranteed converge maximum expected reward policy period nothing else needs said however despite two fundamental statements currently growing hype general ml research causality completely fine causality research long focuses application area mentioned first sentence recent trend brings concept computer vision nlp etc things become vague quite fast exaggerated fact research causality already extremely vague deeply philosophical e g practical implication newcomb paradox computer vision causal model known even vision processing humans animals little understood moreover cv tasks inherently specified instance cartoon drawing elephant still elephant distribution ood class multiple classes talking causal relationship pixels patches concepts makes elephant ear elephant ear vagueness combined general trend ml throwing bunch overly complex math statements paper impress reviewers really concerning bet hundreds papers topic published next years contribute little understanding create millions self citations
[D] Discussion about fine tuning language models for generating SQL queries,0,qszlto,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qszlto/d_discussion_about_fine_tuning_language_models/,0,"Hey you all  hope you are doing well  I have a few things that I wanted to discuss about language modeling now I have a bunch of critical data stored in database format  for my company  I am working on something that can be used to train a language model to convert natural language to a SQL query  Currently we have a rule based system which has tons of if else conditions and  but it works  the only issue I see is that if a new database gets added  the currently existing rule based system will break    

So I am thinking of a way to automate this NLSQL using a machine translation model such as GPT   later followed by something bigger like GPT J  I have looked at a few papers namely PICARD and the corresponding challenge called Spider  

I wanted to discuss a few queries that I have and the genral thoughts that people have about this problem   

  Am I batshit cray  to think finetuning large language models with a new data set would work well   

\   Any alternative s that you would suggest to this   

\   How should I go about this  Just plain use the trainer API from Hugging Face   

\   Other genral thoughts and opinions etc  

Thank you for taking your time to read this  I hope we can have a meaningful conversation about this ",1636803783.0,2021-11-13 12:43:03,hey hope well things wanted discuss language modeling bunch critical data stored database format company working something used train language model convert natural language sql query currently rule based system tons else conditions works issue see new database gets added currently existing rule based system break thinking way automate nlsql using machine translation model gpt later followed something bigger like gpt j looked papers namely picard corresponding challenge called spider wanted discuss queries genral thoughts people problem batshit cray think finetuning large language models new data set would work well \ alternative would suggest \ go plain use trainer api hugging face \ genral thoughts opinions etc thank taking time read hope meaningful conversation
"[P] Language model ruGPT-3 13B is apparently available for download. From an English translation: ""The ruGPT-3 13B model contains 13 billion parameters and is capable of continuing texts in Russian and English, as well as in programming languages.""",2,qstdsm,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qstdsm/p_language_model_rugpt3_13b_is_apparently/,0,"[Download page] https  sbercloud ru ru datahub rugptfamily rugpt  b   [English translation] https  sbercloud ru translate goog ru datahub rugptfamily rugpt  b _x_tr_sl=auto _x_tr_tl=en _x_tr_hl=en US _x_tr_pto=nui   I did not sign up to try to download the file s  

[Reference] https  habr com ru company sberbank blog    [English translation] https  habr com translate goog ru company sberbank blog   _x_tr_sl=auto _x_tr_tl=en _x_tr_hl=en US _x_tr_pto=nui  ",1636778046.0,2021-11-13 05:34:06,[download page] sbercloud ru ru datahub rugptfamily rugpt b [english translation] sbercloud ru translate goog ru datahub rugptfamily rugpt b _x_tr_sl=auto _x_tr_tl=en _x_tr_hl=en us _x_tr_pto=nui sign try download file [reference] habr com ru company sberbank blog [english translation] habr com translate goog ru company sberbank blog _x_tr_sl=auto _x_tr_tl=en _x_tr_hl=en us _x_tr_pto=nui
[D] How do I lower my standards for code quality to match my research team?,92,qs5igm,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qs5igm/d_how_do_i_lower_my_standards_for_code_quality_to/,85,"I recently joined a lab working on research in robotics   reinforcement learning as a research intern  I graduated college recently  all my previous work experience is ML engineering but not active research  rather implementing things people have already done 

I ve been in the lab a few months now and it s become clear to me that the standards for code in research are a lot lower than I expected  I am the only one on my team pushing for things like CI pipelines  refactoring  documentation  regression tests  etc  And when I do raise such concerns the usual response is that such things are not worth spending time on  My supervisor once told me that the job of a researcher is not to write good code  it s to find a correct problem formulation  describe it correctly  and then pass it to a software engineer whose job it is to create the high quality implementation  They emphasised the importance of rapid iteration of the idea rather than going slowly because they re concerned about getting scooped in our current project  This is true to some extent because we are a small and resource limited group whose experiments can take hours to run  working in a popular field  quadruped legged locomotion  

I struggle a lot with this mindset  I hate looking at the crappy code written by myself and by my teammates and I hate debugging it   trying to understand it  A lot of my time is wasted on things like checking which experimental configuration a model was trained from because we don t have automated logging of that  Aesthetically I also just dislike it  I have high personal standards of code quality  Uncle Bob s *Clean Code * anyone   which I feel like I m constantly breaking when I have to write code for work 

I m looking for advice on how I can learn to tolerate this better  or other similar resolutions to the problem  Thanks in advance 

==============================================================

Edit  Thank you all for the kind replies so far  I have received many more responses than I was ever expecting to get  It s going to be difficult to reply to each one but I will do my best to read all of them and consider  I will respond here to some general points I ve seen made repeatedly 

  I m not arrogant enough to think that I know everything about research and coding after  months in a lab  And as many have rightly pointed out there s no need for research code to live up to the standards of long term production code  I do recognize that in this case the problem likely lies  mostly  with me  Hence the title of  how do I lower my standards  and the question of  how do I learn to tolerate something I m not comfortable with   Seeing how popular this post is  I m sure this struggle resonates with many others as well  As a result I hope for there to be less cynicism about my motives  The discussion so far has been largely productive and on topic  I appreciate very much it staying that way 
  To give some additional context  I am working right now as part of a group  I have a main project which I work on with  other person  also a recent graduate   My supervisor actually doesn t look at the code we write at all and as far as I can tell does not care much about code  They seem to implicitly trust that we are able to correctly implement things according to the description  which I believe is true for both of us   At our weekly meetings  we present our progress in the form of videos   graphs   slides  This means that I and my teammate are solely responsible for maintaining the quality of code in our current project  and as mentioned we have philosophical disagreements about how much code quality matters   I know I am not necessarily right  
  The project I am working on is very much not a  one off experiment   Our main codebase is inherited from somebody else who is no longer in the lab  which was itself an iteration on some open source implementation   We are iterating various elements of the robot s sensor setup in addition to the reinforcement learning algorithm and training curriculum  With well known off the shelf envs like those in MuJoCo I d be pretty happy accepting that it was working as intended  With code I write myself or code written by other people which is not tested  it s a lot harder to have that confidence 
  As some have correctly pointed out  using a standard logging tool like W B would be an easy fix to the time spent on resolving experimental configurations  That issue in particular could be resolved easily by setting up the appropriate pipelines and would likely be a net plus to my team  More generally  I could benefit from critically examining the practices I am used to and seeing which have concrete benefits in my current workplace  and then propose those to the team 
  I am a strong proponent of unit tests  Unit tests save my butt from making simple mistakes  It s already incredibly difficult to figure out why experiments fail sometimes  I can at least eliminate some of the possible causes by unit testing code where easy and appropriate  Also  when I do fix a bug  I write a unit test to enforce the fix  and then I can mostly forget about it  That mental real estate can go to things that can t be automated as easily 
  Similarly  I am a big fan of simple CI pipelines like Github Actions  Perhaps the term  CI  comes with a lot of associated bells and whistles which are  rightfully  not used in research code  but to me CI is simply a way to automate the manual running of unit tests  With a bit of know how  Github CI workflows are easy to set up from a cookiecutter template and they save a lot of time as well as mental worry  I like to include linters as well but it s not necessary to the core functionality ",1636699761.0,2021-11-12 07:49:21,recently joined lab working research robotics reinforcement learning research intern graduated college recently previous work experience ml engineering active research rather implementing things people already done lab months become clear standards code research lot lower expected one team pushing things like ci pipelines refactoring documentation regression tests etc raise concerns usual response things worth spending time supervisor told job researcher write good code find correct problem formulation describe correctly pass software engineer whose job create high quality implementation emphasised importance rapid iteration idea rather going slowly concerned getting scooped current project true extent small resource limited group whose experiments take hours run working popular field quadruped legged locomotion struggle lot mindset hate looking crappy code written teammates hate debugging trying understand lot time wasted things like checking experimental configuration model trained automated logging aesthetically also dislike high personal standards code quality uncle bob *clean code * anyone feel like constantly breaking write code work looking advice learn tolerate better similar resolutions problem thanks advance ============================================================== edit thank kind replies far received many responses ever expecting get going difficult reply one best read consider respond general points seen made repeatedly arrogant enough think know everything research coding months lab many rightly pointed need research code live standards long term production code recognize case problem likely lies mostly hence title lower standards question learn tolerate something comfortable seeing popular post sure struggle resonates many others well result hope less cynicism motives discussion far largely productive topic appreciate much staying way give additional context working right part group main project work person also recent graduate supervisor actually look code write far tell care much code seem implicitly trust able correctly implement things according description believe true us weekly meetings present progress form videos graphs slides means teammate solely responsible maintaining quality code current project mentioned philosophical disagreements much code quality matters know necessarily right project working much one experiment main codebase inherited somebody else longer lab iteration open source implementation iterating various elements robot sensor setup addition reinforcement learning algorithm training curriculum well known shelf envs like mujoco pretty happy accepting working intended code write code written people tested lot harder confidence correctly pointed using standard logging tool like w b would easy fix time spent resolving experimental configurations issue particular could resolved easily setting appropriate pipelines would likely net plus team generally could benefit critically examining practices used seeing concrete benefits current workplace propose team strong proponent unit tests unit tests save butt making simple mistakes already incredibly difficult figure experiments fail sometimes least eliminate possible causes unit testing code easy appropriate also fix bug write unit test enforce fix mostly forget mental real estate go things automated easily similarly big fan simple ci pipelines like github actions perhaps term ci comes lot associated bells whistles rightfully used research code ci simply way automate manual running unit tests bit know github ci workflows easy set cookiecutter template save lot time well mental worry like include linters well necessary core functionality
[R] DeepMind’s One Pass ImageNet: A New Benchmark for Resource Efficiency in Deep Learning,13,qse6gc,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qse6gc/r_deepminds_one_pass_imagenet_a_new_benchmark_for/,3,"A DeepMind research team presents the One Pass ImageNet  OPIN  problem  designed to study the space and compute efficiency of deep learning in a streaming setting with constrained data storage and to develop model training systems where each example is passed to the system only once  

Here is a quick read  [DeepMind’s One Pass ImageNet  A New Benchmark for Resource Efficiency in Deep Learning ] https  syncedreview com  p= preview=true _thumbnail_id= 

The paper *One Pass ImageNet* is on [arXiv] https  arxiv org abs  v  ",1636732499.0,2021-11-12 16:54:59,deepmind research team presents one pass imagenet opin problem designed study space compute efficiency deep learning streaming setting constrained data storage develop model training systems example passed system quick read [deepmind’s one pass imagenet new benchmark resource efficiency deep learning ] syncedreview com p= preview=true _thumbnail_id= paper *one pass imagenet* [arxiv] arxiv org abs v
[D] What simulation software would you use to train a custom robot in?,6,qskjq5,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qskjq5/d_what_simulation_software_would_you_use_to_train/,7,"Say I want to use Reinforcement Learning to train a custom  virtual robot to stand  What simulation software would you recommend 
The requirements are the following   

 * Good training times for a billions steps  I would want the step to restart if the robot hit the ground hard  I would like to get lots of training steps in as fast as possible   
 * Inputs and outputs to and from Python  To observe the state of the simulation and take actions at every one of the robots joints  Position  balance  and velocity observations will be needed as well   
 * Ability to observe visual sensors on the robot  
 * Very fine construction of the virtual robot in terms of size and weight of components  Exact positioning of the components  Precise force of the robot s motors at each joint 

I see Gazebo and Mujoco are popular  but I m not sure they can do what I need  If there isn t an option maybe I will write my own physics engine ",1636750267.0,2021-11-12 21:51:07,say want use reinforcement learning train custom virtual robot stand simulation software would recommend requirements following * good training times billions steps would want step restart robot hit ground hard would like get lots training steps fast possible * inputs outputs python observe state simulation take actions every one robots joints position balance velocity observations needed well * ability observe visual sensors robot * fine construction virtual robot terms size weight components exact positioning components precise force robot motors joint see gazebo mujoco popular sure need option maybe write physics engine
"[R] PhD & postdoc positions at UT Austin: ML for complex systems (chaotic time series, cellular automata, & fluid dynamics)",193,qrygiv,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrygiv/r_phd_postdoc_positions_at_ut_austin_ml_for/,17,"Hi  I’m looking for PhD students interested in the intersection of machine learning and physics  particularly chaos and fluid dynamics 
I m also informally looking for postdocs  official ad coming soon  

  About

We are based in the physics department at UT Austin  and are affiliated with the Oden Institute for Computational Engineering   Sciences  Here is a link to [the lab website] https  gilpinlab github io  utm_source=en_us_der 

Projects are pretty flexible based on curiosity and mutual interest  there’s room for more algorithm focused time series mining projects  as well as pencil and paper dynamical systems and control theory problems  As far as applications go  we’re particularly interested in projects that can eventually be used for biological data or fluid dynamics  We’re super open to applicants from uncommon academic or personal backgrounds

Here are some recent examples 
  “Chaos as an interpretable benchmark for forecasting and data driven modelling”  NeurIPS   https  arxiv org abs  
  “Deep reconstruction of strange attractors from time series”  NeurIPS   https  arxiv org abs    
  “Cellular automata as convolutional neural networks”  Phys Rev E   https  arxiv org abs  

  Applying

For grad students  feel free to apply to any of these grad programs at UT Austin 
  The physics department  due   
  The Oden CSEM program  due     
  Other departments  CS  EE  are probably possible  too

For postdocs  please reach out to me informally 

Our physics PhD program does not require physics GRE  normal GRE  or a physics undergrad degree  There are only four core courses  and we have previously had students with undergrads in CS  engineering  bioinformatics  etc  Our quals are research talks  not written exams

If any of this sounds interesting  feel free to email DM me or chat with me at NeurIPS or APS",1636674961.0,2021-11-12 00:56:01,hi i’m looking phd students interested intersection machine learning physics particularly chaos fluid dynamics also informally looking postdocs official ad coming soon based physics department ut austin affiliated oden institute computational engineering sciences link [the lab website] gilpinlab github io utm_source=en_us_der projects pretty flexible based curiosity mutual interest there’s room algorithm focused time series mining projects well pencil paper dynamical systems control theory problems far applications go we’re particularly interested projects eventually used biological data fluid dynamics we’re super open applicants uncommon academic personal backgrounds recent examples “chaos interpretable benchmark forecasting data driven modelling” neurips arxiv org abs “deep reconstruction strange attractors time series” neurips arxiv org abs “cellular automata convolutional neural networks” phys rev e arxiv org abs applying grad students feel free apply grad programs ut austin physics department due oden csem program due departments cs ee probably possible postdocs please reach informally physics phd program require physics gre normal gre physics undergrad degree four core courses previously students undergrads cs engineering bioinformatics etc quals research talks written exams sounds interesting feel free email dm chat neurips aps
"[P] Questions regarding self-supervised learning for music (DINO, MoCo, ...)",5,qshm5b,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qshm5b/p_questions_regarding_selfsupervised_learning_for/,6," 

Hello everyone  First of all  I am pretty inactive on Reddit  so I hope that this is the right place for this post  I am a computer science graduate student focusing on machine learning  working on an interdisciplinary research project regarding the analysis of music 

**TL DR** 

I try to extract general and descriptive music features on various different levels  

* Which self supervised methods can you recommend for limited resource capacities  
* Which data augmentations to use for music  
* General tips and tricks for SSL with music  
* Is it a good idea to use a pretrained backbone in order to get away with a small dataset and compute  e g  OpenAI s JukeBox VQ VAE   
* Why is my DINO setup not converging 

 xB 

**General idea** Rather than solving some specific MIR task  e g  genre classification   my goal is to extract generic  interpretable and descriptive music features  In other words  I want a model that “perceives” and “understands” music in general without giving it a specific goal  I would then further analyze extracted features  e g  to find relations to human music perception  or use them for downstream tasks  Ideally  the outcome is a model that can describe music on various different levels  e g  beat  rhythm  harmony   for example by features extracted from different neural network layers  shallow  low level  deeper  higher level   I know that this is by far no easy task  but it is worth investigating the possibilities and limitations in my opinion 

**Which self supervised learning method ** After some research  self supervised learning  SSL  seems the way to go here  SSL is a research area that gained momentum over the last two years or so and there are multiple proposed methods  however applied mostly in the computer vision area  images   Additionally  SSL methods seem to be rather data hungry and as you might imagine my resources are quite limited  I have a GTX  available locally  but I am also willing to pay in order to train a model on GPU cloud services  e g  Lambda GPU   Since my budget is low  I’d like to do all testing locally or on other free alternatives such as Colab in order to find the right method  hyperparameters  etc  for the actual training on a payed server 

**Idea  Using pretrained JukeBox backbone** My idea is to use the pretrained VQ VAE from OpenAI’s JukeBox  see below  as a backbone and hope that it extracts useful intermediate features  which I can forward to my  comparably small  model  With this I hope to get away with a small dataset   hours of music   and relatively low resource usage  while still achieving reasonable results  Does anyone have experience with such setups  Now to the SSL method itself  ignoring generative models   On the one hand  there are contrastive methods such as CPC  SimCLR or  the less resource hungry   MoCo  On the other hand  there are methods which do not explicitly formulate a contrastive loss such as BYOL or DINO  Especially the latter one seems interesting  Unfortunately  I have no experience in training such models at all  which is why I wanted to ask the community for some tips and feedback suited for my problem  Preferable methods are those which do not require much hyperparameter tuning  time and compute limitations  or much compute during training  e g  large batch sizes with SimCLR   but still achieve good results 

**DINO not working properly** Currently I am testing the DINO method on a very limited setup locally  My dataset consists of a very small portion of the FMA dataset   minutes of music  with a sample size of  seconds and a batch size of  using the LARS optimizer  Additionally  I have adopted the audio augmentation strategy from a paper that applies SimCLR to music  CLMR  see below   I am very happy about ideas for other better music augmentations for my problem  though  However  the model does not seem to converge properly  The loss decreases quite quickly after a few epochs  After a while however the model seems to collapse  and the loss increases rapidly staying high over the remaining epochs  I figure this has something to do with the momentum hyperparameter  i e  how much of the student’s weights get transferred to the teacher after each iteration   When increasing this number  the collapse effect is minimized or is non existent at all  if large enough  but the loss does not really decrease much either  I have logged everything and computed stats for several epochs  if anyone is interested  When splitting the loss up into the teacher entropy and the KL divergence between the student and the teacher  one can see that the collapse is caused by the entropy part  Does anyone have experience with DINO or similar methods and might have a clue why this is happening  As mentioned  I can give more details about the hyperparameters  logs  statistics  etc  Is it possible that this phenomenon is due to the small dataset and batch size on my local machine  and it would diminish on the cloud with a larger dataset and or batch size  Though they mentioned in the paper that they have successfully tested their method even with a batch size of  

I know this is a lengthy post  but I wanted to share as much detail as possible about my goal and resulting problems  Hope anyone might give some feedback 

**Papers** JukeBox  [https  arxiv org abs  ] https  arxiv org abs    CLMR  [https  arxiv org abs  ] https  arxiv org abs    DINO  [https  arxiv org abs  ] https  arxiv org abs   ",1636741959.0,2021-11-12 19:32:39,hello everyone first pretty inactive reddit hope right place post computer science graduate student focusing machine learning working interdisciplinary research project regarding analysis music **tl dr** try extract general descriptive music features various different levels * self supervised methods recommend limited resource capacities * data augmentations use music * general tips tricks ssl music * good idea use pretrained backbone order get away small dataset compute e g openai jukebox vq vae * dino setup converging xb **general idea** rather solving specific mir task e g genre classification goal extract generic interpretable descriptive music features words want model “perceives” “understands” music general without giving specific goal would analyze extracted features e g find relations human music perception use downstream tasks ideally outcome model describe music various different levels e g beat rhythm harmony example features extracted different neural network layers shallow low level deeper higher level know far easy task worth investigating possibilities limitations opinion **which self supervised learning method ** research self supervised learning ssl seems way go ssl research area gained momentum last two years multiple proposed methods however applied mostly computer vision area images additionally ssl methods seem rather data hungry might imagine resources quite limited gtx available locally also willing pay order train model gpu cloud services e g lambda gpu since budget low i’d like testing locally free alternatives colab order find right method hyperparameters etc actual training payed server **idea using pretrained jukebox backbone** idea use pretrained vq vae openai’s jukebox see backbone hope extracts useful intermediate features forward comparably small model hope get away small dataset hours music relatively low resource usage still achieving reasonable results anyone experience setups ssl method ignoring generative models one hand contrastive methods cpc simclr less resource hungry moco hand methods explicitly formulate contrastive loss byol dino especially latter one seems interesting unfortunately experience training models wanted ask community tips feedback suited problem preferable methods require much hyperparameter tuning time compute limitations much compute training e g large batch sizes simclr still achieve good results **dino working properly** currently testing dino method limited setup locally dataset consists small portion fma dataset minutes music sample size seconds batch size using lars optimizer additionally adopted audio augmentation strategy paper applies simclr music clmr see happy ideas better music augmentations problem though however model seem converge properly loss decreases quite quickly epochs however model seems collapse loss increases rapidly staying high remaining epochs figure something momentum hyperparameter e much student’s weights get transferred teacher iteration increasing number collapse effect minimized non existent large enough loss really decrease much either logged everything computed stats several epochs anyone interested splitting loss teacher entropy kl divergence student teacher one see collapse caused entropy part anyone experience dino similar methods might clue happening mentioned give details hyperparameters logs statistics etc possible phenomenon due small dataset batch size local machine would diminish cloud larger dataset batch size though mentioned paper successfully tested method even batch size know lengthy post wanted share much detail possible goal resulting problems hope anyone might give feedback **papers** jukebox [ arxiv org abs ] arxiv org abs clmr [ arxiv org abs ] arxiv org abs dino [ arxiv org abs ] arxiv org abs
[D] Best/Favorite format for writing without a conference in mind?,4,qsh3z0,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qsh3z0/d_bestfavorite_format_for_writing_without_a/,4,"Usually  I have ideas  do some experiments  draft a manuscript  etc  and then afterwards edit it into a format for a particular conference or journal  But I m wondering  among the community  what s your preferred format for writing something if you don t know where you re sending it yet  If something s just on arXiv  what format is visually the easiest to read 

I kinda like the JMLR format  pretty plain  pretty basic  but I can appreciate that it wastes a lot of space on the author list  The NeurIPS seems like the default  definitive format  but I find the bars around the title kind of ugly  The IEEE formats always present visually as being just completely fucking impenetrable  Not a fan of the ICLR format putting all the titles in caps ",1636740560.0,2021-11-12 19:09:20,usually ideas experiments draft manuscript etc afterwards edit format particular conference journal wondering among community preferred format writing something know sending yet something arxiv format visually easiest read kinda like jmlr format pretty plain pretty basic appreciate wastes lot space author list neurips seems like default definitive format find bars around title kind ugly ieee formats always present visually completely fucking impenetrable fan iclr format putting titles caps
[R] prune-then-quantize or quantize-then-prune for post-training optimization of computer vision models.,3,qsi0u2,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qsi0u2/r_prunethenquantize_or_quantizethenprune_for/,3,"As a student that is interested in improving the efficiency  model size  throughput  energy  of pre trained deep learning models in the computer vision domain  I was wondering if there was a clear winning approach on in what order we should prune and quantize a pre trained model 

So basically if you were given a pre trained deep learning model  e g  Resnet   would one of the following approaches lead to better solutions  in terms of the accuracy to efficiency ratio  e g  model size  throughput  energy consumption  

  quantize then prune  with finetuning retraining after every stage 
  prune then quantize  with finetuning retraining after every stage 

I have trouble finding related works that answer this kind of question  To me it seems to be valid question  but maybe I m missing something obvious  

Any input would be appreciated ",1636743107.0,2021-11-12 19:51:47,student interested improving efficiency model size throughput energy pre trained deep learning models computer vision domain wondering clear winning approach order prune quantize pre trained model basically given pre trained deep learning model e g resnet would one following approaches lead better solutions terms accuracy efficiency ratio e g model size throughput energy consumption quantize prune finetuning retraining every stage prune quantize finetuning retraining every stage trouble finding related works answer kind question seems valid question maybe missing something obvious input would appreciated
[R] NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework,18,qs5cr7,MachineLearning,https://arxiv.org/abs/2111.04130,6,nan,1636699070.0,2021-11-12 07:37:50,nan
[D] Micro-Grants,0,qss5os,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qss5os/d_microgrants/,0,Are there micro grant sources besides AI Grant and Unitary Fund ,1636773748.0,2021-11-13 04:22:28,micro grant sources besides ai grant unitary fund
[D] Can Unity3D ML-Agents use GridSearch?,1,qsjngz,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qsjngz/d_can_unity3d_mlagents_use_gridsearch/,1,"Hello 

I am trying to find a way to do gridsearch for hyperparameter tuning for reinforcement learning under the UnityD machine learning platform 

I googled but it feels like no options are available right now 

If there is alternative ways  please share  Thank you  guys ",1636747703.0,2021-11-12 21:08:23,hello trying find way gridsearch hyperparameter tuning reinforcement learning unityd machine learning platform googled feels like options available right alternative ways please share thank guys
[N][CfP] AI for Design and Manufacturing Workshop (ADAM) @ AAAI 2022 (Deadline Extended),2,qsebrh,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qsebrh/ncfp_ai_for_design_and_manufacturing_workshop/,0,"Hello [r MachineLearning] https  www reddit com r MachineLearning  

The deadline for submitting to the AI for Design and Manufacturing Workshop  ADAM    AAAI  has been extended for a week due to multiple requests  If you re working in the intersection of AI and design  manufacturing  scientific computing  and geometric modeling  do consider submitting a  page workshop paper  We invite paper submissions on the following  and related  topics 

* New theory and fundamentals of AI aided design and manufacturing 
* Novel AI based techniques to improve modeling of engineering systems 
* Integration of AI based approaches with engineering prototyping and manufacturing 
* Novel methods to learn from scarce sparse  or heterogenous  or multimodal data 
* Novel ML methods in the computational material and physical sciences 
* Novel ML accelerated optimization for conceptual detailed system design 
* Novel AI enabled generative models for system design and manufacturing 
* ML guided rare event modeling and system uncertainty quantification 
* Development of software  libraries  or benchmark datasets  and
* Identification of key challenges and opportunities for future research 

Workshop website  [https  adam aaai github io ] https  adam aaai github io 

Submission website  [https  openreview net group id=AAAI org  Workshop ADAM] https  openreview net group id=AAAI org  Workshop ADAM 

Submission deadline  November th  ",1636732892.0,2021-11-12 17:01:32,hello [r machinelearning] www reddit com r machinelearning deadline submitting ai design manufacturing workshop adam aaai extended week due multiple requests working intersection ai design manufacturing scientific computing geometric modeling consider submitting page workshop paper invite paper submissions following related topics * new theory fundamentals ai aided design manufacturing * novel ai based techniques improve modeling engineering systems * integration ai based approaches engineering prototyping manufacturing * novel methods learn scarce sparse heterogenous multimodal data * novel ml methods computational material physical sciences * novel ml accelerated optimization conceptual detailed system design * novel ai enabled generative models system design manufacturing * ml guided rare event modeling system uncertainty quantification * development software libraries benchmark datasets * identification key challenges opportunities future research workshop website [ adam aaai github io ] adam aaai github io submission website [ openreview net group id=aaai org workshop adam] openreview net group id=aaai org workshop adam submission deadline november th
[D] Why do CNN Kernel weights reach high values?,3,qse2ov,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qse2ov/d_why_do_cnn_kernel_weights_reach_high_values/,2,"I ve recently read a bunch of literature about network pruning  A common criteria in the field is to select kernels that are to be removed by their L Magnitude  e g  \[\]  \[\]   as the heuristic apparently catches relevant kernels quite well 

Most often the time the CNNs are trainied with some form of weight decay  This is  IMHO  intended to regularize the model and prevent single kernel weights from dominating the entire set of parameters and distribute relevance across multiple channels  Also it is normal to add BatchNorm to the architecture as it stabilizes the training procedure 

I d argue that 

  The only relevant thing for detecting a pattern is the relative weight between the kernel weights  The absolute value does not matter as it will only change the value range   CNN Kernels are a Matrix multiplication and therefore a single scaling factor would do the same as scaling the entire thing   
     As the kernel is followed by a BatchNorm the values get automatically scaled and zero meaned before getting scaled and shifted  so there is less reason for that   When using Conv Bn ReLU ordering 
  With any weight penalty this should lead to continuously   slowly decreasing values of the kernel weights  Except in the BatchNorms which is way more  penalty efficient  than a kernel 

The only reason I could think of is that one will run into some numeric stability issues when one approaches the minimum resolution of the data format  i e  float    Maybe this introduces some sort of noise into the optimization process 

However as the evidence shows that we get higher weights I have to be wrong and would be happy to get shown where my thought process breaks down 

\[\] [Comparing Rewinding and Fine tuning in Neural Network Pruning] https  arxiv org pdf   pdf 

\[\] [Learning efficient convolutional networks through network slimming ] https  openaccess thecvf com content_ICCV_ papers Liu_Learning_Efficient_Convolutional_ICCV__paper pdf ",1636732211.0,2021-11-12 16:50:11,recently read bunch literature network pruning common criteria field select kernels removed l magnitude e g \[\] \[\] heuristic apparently catches relevant kernels quite well often time cnns trainied form weight decay imho intended regularize model prevent single kernel weights dominating entire set parameters distribute relevance across multiple channels also normal add batchnorm architecture stabilizes training procedure argue relevant thing detecting pattern relative weight kernel weights absolute value matter change value range cnn kernels matrix multiplication therefore single scaling factor would scaling entire thing kernel followed batchnorm values get automatically scaled zero meaned getting scaled shifted less reason using conv bn relu ordering weight penalty lead continuously slowly decreasing values kernel weights except batchnorms way penalty efficient kernel reason could think one run numeric stability issues one approaches minimum resolution data format e float maybe introduces sort noise optimization process however evidence shows get higher weights wrong would happy get shown thought process breaks \[\] [comparing rewinding fine tuning neural network pruning] arxiv org pdf pdf \[\] [learning efficient convolutional networks network slimming ] openaccess thecvf com content_iccv_ papers liu_learning_efficient_convolutional_iccv__paper pdf
[Discussion] Is your data quality suffering because of a first-mile reliability problem?,0,qsigso,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qsigso/discussion_is_your_data_quality_suffering_because/,0,"If your data product is fueled by tens to hundreds of external data sources  then this may be relevant to you  When schema changes  volume anomalies  late deliveries plague the first mile  they go on to infect your downstream warehouse tables and business processes  When the reliability of all those data sources are questionable  they cascade into points of failure that are out of your data team’s control   awareness 

If you d like to learn how to improve your data s first mile reliability  check out [**our latest blog post here**] https  databand ai blog data supply chain  utm_source=forum utm_medium=r utm_group=ml ** **",1636744328.0,2021-11-12 20:12:08,data product fueled tens hundreds external data sources may relevant schema changes volume anomalies late deliveries plague first mile go infect downstream warehouse tables business processes reliability data sources questionable cascade points failure data team’s control awareness like learn improve data first mile reliability check [**our latest blog post here**] databand ai blog data supply chain utm_source=forum utm_medium=r utm_group=ml ** **
[P] Create semantic search applications with machine-learning workflows,5,qs98gv,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qs98gv/p_create_semantic_search_applications_with/,0," xB 

https  i redd it infnfbsgz gif

Create semantic search applications with machine learning workflows  The demo above shows how various NLP pipelines can be connected together to build a semantic search application 

txtai executes machine learning workflows to transform data and build AI powered semantic search applications  txtai has support for processing both unstructured and structured data  Structured or tabular data is grouped into rows and columns  This can be a spreadsheet  an API call that returns JSON or XML or even list of key value pairs 

Some example workflows 

* Summarize news articles
* Summarize and translate research papers
* Load and index data via a CSV
* Schedule a recurring job to query an API and index results for semantic search

References 

[Live Demo] https  huggingface co spaces NeuML txtai   
[GitHub] https  github com neuml txtai   
[Article] https  towardsdatascience com run machine learning workflows to transform data and build ai powered text indices with txtai dba   
[Notebook] https  colab research google com github neuml txtai blob master examples _Run_pipeline_workflows ipynb   
[Notebook] https  colab research google com github neuml txtai blob master examples _Transform_tabular_data_with_composable_workflows ipynb ",1636716646.0,2021-11-12 12:30:46,xb redd infnfbsgz gif create semantic search applications machine learning workflows demo shows various nlp pipelines connected together build semantic search application txtai executes machine learning workflows transform data build ai powered semantic search applications txtai support processing unstructured structured data structured tabular data grouped rows columns spreadsheet api call returns json xml even list key value pairs example workflows * summarize news articles * summarize translate research papers * load index data via csv * schedule recurring job query api index results semantic search references [live demo] huggingface co spaces neuml txtai [github] github com neuml txtai [article] towardsdatascience com run machine learning workflows transform data build ai powered text indices txtai dba [notebook] colab research google com github neuml txtai blob master examples _run_pipeline_workflows ipynb [notebook] colab research google com github neuml txtai blob master examples _transform_tabular_data_with_composable_workflows ipynb
[D] Adversarial Loss understanding in Total Relighting: Learning to Relight Portraits for Background Replacement paper,2,qsaonk,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qsaonk/d_adversarial_loss_understanding_in_total/,0,"Hi  I hope to get some help understanding Google s [Total Relighting  Learning to Relight Portraits for Background Replacement] https  augmentedperception github io total_relighting total_relighting_paper pdf  paper  I m stuck on the adversarial loss  In   Paper says 

 we add an adversarial loss on the face region to help the network learn to plausibly remove high frequency shading effects from the input image while maintaining image detail  We use a least squares discriminator \[Mao et al  \] disc^ alb  to add a loss between a crop of the face from the ground truth albedo 𝐴crop\_gt and a matching crop of the face from the predicted albedo 𝐴crop

Predicted albedo 𝐴crop is the face crop of the output of U Net like network  No other details provided and I m struggling to understand the setup here  Since in the paper they re using both ground truth and predicted albedo as the inputs for the loss  I can imagine two scenarios 

  Use the loss from the original LSGAN paper and train the discriminator during the model training  which seems counterintuitive to me
  Use L or L distance between pretrained discriminator s output for ground truth and prediction  but that is not really adversarial loss I guess 

I have no experience with such discriminator usage and therefore can t choose between these two or come up with something else reasonable  

Is there a common way to use GAN discriminator for the loss calculation of  non GAN  networks  Because from the paper it sounds like something that doesn t require deeper explanation",1636722057.0,2021-11-12 14:00:57,hi hope get help understanding google [total relighting learning relight portraits background replacement] augmentedperception github io total_relighting total_relighting_paper pdf paper stuck adversarial loss paper says add adversarial loss face region help network learn plausibly remove high frequency shading effects input image maintaining image detail use least squares discriminator \[mao et al \] disc^ alb add loss crop face ground truth albedo 𝐴crop\_gt matching crop face predicted albedo 𝐴crop predicted albedo 𝐴crop face crop output u net like network details provided struggling understand setup since paper using ground truth predicted albedo inputs loss imagine two scenarios use loss original lsgan paper train discriminator model training seems counterintuitive use l l distance pretrained discriminator output ground truth prediction really adversarial loss guess experience discriminator usage therefore choose two come something else reasonable common way use gan discriminator loss calculation non gan networks paper sounds like something require deeper explanation
[R] Palette: Image-to-Image Diffusion Models,24,qry8i4,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qry8i4/r_palette_imagetoimage_diffusion_models/,1,"website  [https  iterative refinement github io palette ] https  iterative refinement github io palette 

paper  [https  arxiv org abs  ] https  arxiv org abs   

Samples 

 xB 

https  preview redd it ncseotyz png width= format=png auto=webp s=faaaceeaaedecbc

 xB 

https  preview redd it glyaejuryz png width= format=png auto=webp s=fbccddbfbbedeaff",1636674082.0,2021-11-12 00:41:22,website [ iterative refinement github io palette ] iterative refinement github io palette paper [ arxiv org abs ] arxiv org abs samples xb preview redd ncseotyz png width= format=png auto=webp s=faaaceeaaedecbc xb preview redd glyaejuryz png width= format=png auto=webp s=fbccddbfbbedeaff
"[R] Invitation to participate in study ""The Labour of Ethical AI""",0,qsdnc2,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qsdnc2/r_invitation_to_participate_in_study_the_labour/,0,"This is an invitation to participate in a study titled “The Labour of Ethical AI”  My name is James Steinhoff and I am a postdoctoral fellow at the Institute of Communication  Culture  Information and Technology at the University of Toronto Mississauga  The aim of this research is to understand the labour that goes into ethical AI research  research intended to promote ethical  responsible  democratic  human centered  non profit or socially beneficial AI   the organizations in which such work is conducted  the problems facing workers in this field  and how the field is connected to industry  academia and government  The goal of the study is to interview people who work  study or intern in ethical AI in order to gain empirical insight into the working conditions in this sector  

If you have experience working in the sector  you could provide great insight into some of the challenges and opportunities facing workers  If you are willing  your participation would involve meeting me for an online interview that will last approximately one hour  where I will ask you questions about your working conditions  why you chose the ethical AI sector and the promises and problems facing that sector  Participation in this project will be confidential  I will supply potential participants with an informed consent form that outlines in greater detail the project and the parameters of participation 

I am happy to answer any questions or concerns you might have 

Thank you for your time and consideration 

Sincerely 

James Steinhoff  PhD   j steinhoff utoronto ca

Postdoctoral Fellow

Institute of Communication  Culture  Information and Technology

CCT Building 

 Mississauga Rd 

Mississauga  ON

LL C",1636731023.0,2021-11-12 16:30:23,invitation participate study titled “the labour ethical ai” name james steinhoff postdoctoral fellow institute communication culture information technology university toronto mississauga aim research understand labour goes ethical ai research research intended promote ethical responsible democratic human centered non profit socially beneficial ai organizations work conducted problems facing workers field field connected industry academia government goal study interview people work study intern ethical ai order gain empirical insight working conditions sector experience working sector could provide great insight challenges opportunities facing workers willing participation would involve meeting online interview last approximately one hour ask questions working conditions chose ethical ai sector promises problems facing sector participation project confidential supply potential participants informed consent form outlines greater detail project parameters participation happy answer questions concerns might thank time consideration sincerely james steinhoff phd j steinhoff utoronto ca postdoctoral fellow institute communication culture information technology cct building mississauga rd mississauga c
[p] Alphafold 2.1.1 (Without Docker),15,qryaz4,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qryaz4/p_alphafold_211_without_docker/,4,"Want to fold monomers AND multimers using Alphafold without the need for Docker  Look no further than my fork of DeepMind s Alphafold repository that removes all Docker dependencies  Enjoy   AlphaFold  docker 

https  github com amorehead alphafold\_non\_docker",1636674314.0,2021-11-12 00:45:14,want fold monomers multimers using alphafold without need docker look fork deepmind alphafold repository removes docker dependencies enjoy alphafold docker github com amorehead alphafold\_non\_docker
"[D] Can data scientists still make a better predictive model using their talents, by the evidence? Will script kiddies increasingly take over because of great software and AI that does more and more of it very well?",0,qsc6y3,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qsc6y3/d_can_data_scientists_still_make_a_better/,7,"Having studied and applied data science and machine learning since   I am startled by the high quality free tools that are coming online in the past week  month  and year or two  that I have tried  They are that good   I am talking about automatic model selection  automatic tuning  and NLP that is just breaking away from the old limitations and old ways of doing projects 

See  on youtube lately I am watching all these  data scientists  posting videos where all they do is run the software that Microsoft and Google made  and call it a day   They don t even change the default settings and they add nothing of their own talents  if they have them    

And the results are pretty good  by golly  That s the thing  

 Automatic feature engineering is one of the hallmarks of the top deep neural networks  People have spent careers in linguistics and computer vision  hand making parts and hand curating mathematical techniques that are now often completely bypassed by today s ML techniques baked into free software  and that s just one example of many 

It s not a trivial question  

There s more to data science than making a predictive model on a canned  fixed public dataset like Iris  Titanic  Jewellery  or even Higgs Boson  There s also MLOps  exploratory data analysis  study design  visualization  data wrangling  data quality assurance  data life cycle  and many other areas  Data science has wide scope and many specialties   Predictive model making is only about  percent of the whole ML and DS job now  according to a presentation I saw yesterday 

But modeling might be about to be fully automated soon  Can you as a ML engineer or data scientist  bring your wide and deep talents to bear  to actually show that you can make a model better than a script kiddie on predictive performance on any   any at all   well known open dataset 

Is making predictive models a fully automated task now ",1636726869.0,2021-11-12 15:21:09,studied applied data science machine learning since startled high quality free tools coming online past week month year two tried good talking automatic model selection automatic tuning nlp breaking away old limitations old ways projects see youtube lately watching data scientists posting videos run software microsoft google made call day even change default settings add nothing talents results pretty good golly thing automatic feature engineering one hallmarks top deep neural networks people spent careers linguistics computer vision hand making parts hand curating mathematical techniques often completely bypassed today ml techniques baked free software one example many trivial question data science making predictive model canned fixed public dataset like iris titanic jewellery even higgs boson also mlops exploratory data analysis study design visualization data wrangling data quality assurance data life cycle many areas data science wide scope many specialties predictive model making percent whole ml ds job according presentation saw yesterday modeling might fully automated soon ml engineer data scientist bring wide deep talents bear actually show make model better script kiddie predictive performance well known open dataset making predictive models fully automated task
[D] Calling out the authors of 'Trajformer' paper for claiming they published code but never doing it,529,qrbkc7,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrbkc7/d_calling_out_the_authors_of_trajformer_paper_for/,94,"I read a paper from NeurIPS  titled  Trajformer  Trajectory Prediction with Local Self Attentive Contexts for Autonomous Driving   I found it interesting and the authors claim multiple times in the paper that  we release our code at  [https  github com Manojbhat Trajformer] https  github com Manojbhat Trajformer   Turns out they never did  fine  I thought perhaps they will in the future and starred the repo to check it out later 

Many others raised issues asking for update on code release and they never replied  Finally  it April they update the readme to say that they will release the code and that s been the last update 

I know this is a common trend in ML papers now  but what sucks is that I emailed the authors  both the grad student and the PI  multiple times asking for an update an they never replied  Their paper is literally based on empirical improvements and without working code to replicate the results it is their word against mine 

I strongly think things have to change  and I believe they only will if we call them out  I waited long enough  and made significant effort to contact the authors with no response  I mean I don t mind them not releasing their code  but at least don t claim that you did in the paper review phase and then disappear  An undergrad in my lab asked why she should take time to clean up the code and document it before release while others just move on to the next interesting project and I don t have an answer  ",1636600691.0,2021-11-11 04:18:11,read paper neurips titled trajformer trajectory prediction local self attentive contexts autonomous driving found interesting authors claim multiple times paper release code [ github com manojbhat trajformer] github com manojbhat trajformer turns never fine thought perhaps future starred repo check later many others raised issues asking update code release never replied finally april update readme say release code last update know common trend ml papers sucks emailed authors grad student pi multiple times asking update never replied paper literally based empirical improvements without working code replicate results word mine strongly think things change believe call waited long enough made significant effort contact authors response mean mind releasing code least claim paper review phase disappear undergrad lab asked take time clean code document release others move next interesting project answer
[D] What must every PhD doing ML know before graduating,8,qrwuvy,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrwuvy/d_what_must_every_phd_doing_ml_know_before/,4,I am a rd year PhD who finally finished all program requirements  classes etc   and am fully focused on research  My question is  what are some things that your average ML PhD should be good at at this point  I know its subjective and depends on their research field but what are some common things that I should be well versed in ,1636669986.0,2021-11-11 23:33:06,rd year phd finally finished program requirements classes etc fully focused research question things average ml phd good point know subjective depends research field common things well versed
[R] A Survey on Green Deep Learning,1,qs5f6h,MachineLearning,https://arxiv.org/abs/2111.05193,4,nan,1636699358.0,2021-11-12 07:42:38,nan
[D] Why do we have to discretize the data before we use mask prediction for representation learning?,6,qrte64,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrte64/d_why_do_we_have_to_discretize_the_data_before_we/,0,"In fields as vision or speech  a lot of recent papers learn representations by masking parts of the input and predicting the original  To do this  they often refer to masked language modeling  MLM  in BERT style pre training and say that we must first discretize the continuous input into discrete tokens   e g  using a VQ VAE  before making predictions  i e  classification task over possible tokens in the dictionary  

One of the argument is that the predicting discrete tokens allows the model to learn high level concepts whereas making prediction in the original input space  e g  raw pixels  will force the model to learn high frequency low level details that are not useful for representation learning compression  and is also computationally prohibitive  since the original input space is likely very high dimensional   However my question is why can t we do regression in a continuous latent space for the masked positions  e g  predicting the latent representation of a learned continuous VAE for the masked positions  instead of classification in a discrete latent space  e g  predicting the discrete tokens of a learned VQ VAE for the masked positions   Is there any theoretical advantage to using discrete tokens instead of continuous latents ",1636660160.0,2021-11-11 20:49:20,fields vision speech lot recent papers learn representations masking parts input predicting original often refer masked language modeling mlm bert style pre training say must first discretize continuous input discrete tokens e g using vq vae making predictions e classification task possible tokens dictionary one argument predicting discrete tokens allows model learn high level concepts whereas making prediction original input space e g raw pixels force model learn high frequency low level details useful representation learning compression also computationally prohibitive since original input space likely high dimensional however question regression continuous latent space masked positions e g predicting latent representation learned continuous vae masked positions instead classification discrete latent space e g predicting discrete tokens learned vq vae masked positions theoretical advantage using discrete tokens instead continuous latents
[R] Generalization in Dexterous Manipulation via Geometry-Aware Multi-Task Learning,1,qrynlx,MachineLearning,https://arxiv.org/abs/2111.03062,3,nan,1636675554.0,2021-11-12 01:05:54,nan
[P] Integrate your ML model with your favorite apps from a single Python file,7,qrpx36,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrpx36/p_integrate_your_ml_model_with_your_favorite_apps/,4,"Hi all 

Many here can build a simple Machine Learning model to predict whether a customer will *churn* if they get a nice `pandas DataFrame` with customer data  However  it gets really complicated if you want this model **deployed** and **integrated** in production  say a procedure as such 

  Pull data from a new customer from **Shopify**
  Predict for this customer whether they will *churn*
  If we predict `CHURN == True`
  Send a discount code to this customer with **Mailchimp**

Suddenly we have to code data integrations  ETL pipelines  deploy our original Machine Learning solution  spin up an HTTP server etc  a huge pain indeed 

We are building [a framework] https  flow magicsheets io  that takes care of exactly all the boring stuff described above  We really believe this will **bridge the gap between research and real world ML** 

Super excited to share this with all of you  please let me know if you have comments or feedback ",1636650680.0,2021-11-11 18:11:20,hi many build simple machine learning model predict whether customer *churn* get nice `pandas dataframe` customer data however gets really complicated want model **deployed** **integrated** production say procedure pull data new customer **shopify** predict customer whether *churn* predict `churn == true` send discount code customer **mailchimp** suddenly code data integrations etl pipelines deploy original machine learning solution spin http server etc huge pain indeed building [a framework] flow magicsheets io takes care exactly boring stuff described really believe **bridge gap research real world ml** super excited share please let know comments feedback
[R] Gradients are Not All You Need,47,qre2vo,MachineLearning,https://arxiv.org/abs/2111.05803,13,nan,1636609206.0,2021-11-11 06:40:06,nan
[P] Fine-tuning and running GPT-J made easy,1,qs0eup,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qs0eup/p_finetuning_and_running_gptj_made_easy/,1,"Have you guys seen Eleuther’s GPT J for NLP yet  I feel like it’s on par with OpenAI’s Curie  It s pretty good overall for generic language generation  but you still need to fine tune it for custom tasks  so I ended up putting together this project to simplify fine tuning and deployment to production after 

Both can be done through a web interface  Also  I added a default pre trained GPT J to use through an interface or API too  Please  check it out and give me feedback if you can  Thanks 

Project  https  www tensorbox ai",1636681321.0,2021-11-12 02:42:01,guys seen eleuther’s gpt j nlp yet feel like it’s par openai’s curie pretty good overall generic language generation still need fine tune custom tasks ended putting together project simplify fine tuning deployment production done web interface also added default pre trained gpt j use interface api please check give feedback thanks project www tensorbox ai
[D] How much VRAM and RAM do I need for NLP transformer models?,5,qrnozu,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrnozu/d_how_much_vram_and_ram_do_i_need_for_nlp/,10,"I m a PhD student looking for a new desktop because my current  personal  PC has an AMD GPU 

When I train a pre trained BERT model using my CPU  which takes forever  I assume that it is using RAM  Online  I often read about transformer models using VRAM 

So my question  does training transformer models exclusively use VRAM  or do I also need sufficient RAM  And how much would be needed  minimum  to work with such models 

 I am aware of Google Collab but it is not suitable for my work ",1636644422.0,2021-11-11 16:27:02,phd student looking new desktop current personal pc amd gpu train pre trained bert model using cpu takes forever assume using ram online often read transformer models using vram question training transformer models exclusively use vram also need sufficient ram much would needed minimum work models aware google collab suitable work
[D] Article: An introduction to Language Models in NLP,0,qrw5pb,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrw5pb/d_article_an_introduction_to_language_models_in/,0,"Hey  We re working on an intro series to language models  Would love any feedback on this first installment   

[https  www surgehq ai blog an introduction to language models in nlp part  intuition] https  www surgehq ai blog an introduction to language models in nlp part  intuition ",1636667997.0,2021-11-11 22:59:57,hey working intro series language models would love feedback first installment [ www surgehq ai blog introduction language models nlp part intuition] www surgehq ai blog introduction language models nlp part intuition
"[P] Cedille, the largest French language model (6b), released in open source",167,qqzuh0,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqzuh0/p_cedille_the_largest_french_language_model_6b/,25,"  [📝 DEMO] https  app cedille ai    [📘 REPO] https  github com coteries cedille ai 

We have spent the last  months of our lives  teraFLOPs of compute and gone through gb of text to bring you Cedille 

  **Ce que j aime quand je mange une baguette  c est** quand celle ci est craquante 
Je ne saurais définir le terme  craquant  mais je sais que lorsque c est le cas  je peux être sûre que la baguette est bonne 

The entirety of French spirit captured in measly B parameters  🇫🇷🥖

More seriously  we are super excited to share Cedille  the so far largest French language model  https  en cedille ai

You can play with it right now on our playground  as long as servers hold 😅    https  app cedille ai

We are proponents of “open AI” and as such have released a checkpoint for the world to use  MIT license    https  github com coteries cedille ai

Another aspect we had fun with is dataset filtering  We have run the [whole C French dataset] https  github com allenai allennlp discussions   through the Detoxify classifier to clean it up 🤬

Some acknowledgements  

* Cedille is based on GPT J  the b model developed by the wizards at EleutherAI  https  arankomatsuzaki wordpress com    gpt j 
* Cedille was also generously supported by the Google TFRC program  https  sites research google trc about ",1636566313.0,2021-11-10 18:45:13,[📝 demo] app cedille ai [📘 repo] github com coteries cedille ai spent last months lives teraflops compute gone gb text bring cedille **ce que j aime quand je mange une baguette c est** quand celle ci est craquante je ne saurais définir le terme craquant mais je sais que lorsque c est le cas je peux être sûre que la baguette est bonne entirety french spirit captured measly b parameters 🇫🇷🥖 seriously super excited share cedille far largest french language model en cedille ai play right playground long servers hold 😅 app cedille ai proponents “open ai” released checkpoint world use mit license github com coteries cedille ai another aspect fun dataset filtering run [whole c french dataset] github com allenai allennlp discussions detoxify classifier clean 🤬 acknowledgements * cedille based gpt j b model developed wizards eleutherai arankomatsuzaki wordpress com gpt j * cedille also generously supported google tfrc program sites research google trc
[D] Handling bound constraints in CMA-ES,1,qrvasc,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrvasc/d_handling_bound_constraints_in_cmaes/,1,"Hi all 

Apologies if the question is a bit naive as I only have passing familiarity with ML  full stack developer by trade   I m really just looking for more insight into the implementation and consequences of bound constraints with CMA ES optimization  I did come across  [Biedrzycki  ] https  staff elka pw edu pl ~rbiedrzy publ RBSWEVO_CMA_BCHMs pdf   which discusses this topic  and a lot of the paper s findings at least come across as fairly natural  but I m not finding a lot out there and I m just wondering whether anybody else has any recommendations for further reading or personal stories of potential pitfalls  In the problem I m dealing with specifically  each of the coordinates in my mutant search point vectors need to be bound within the unit interval and I wanted to make sure I wasn t walking into any easily preventable mistakes 

**Background Context**   I m currently working on a personal project  inspired by this popular [Geijtenbeek  et al  ] https  www goatstream com research papers SA index html  paper  where I ve implemented a Hill type muscle model and rigged up a humanoid model with some    muscles  approximating human skeletal muscle function  Now I m beginning muscle control experiments with inspiration from this paper  [Wochner  et al  ] https  www frontiersin org articles   fncom   full   except I m plugging in CMA ES instead of the Bayesian optimization used in the paper while utilizing the objective function insights ",1636665512.0,2021-11-11 22:18:32,hi apologies question bit naive passing familiarity ml full stack developer trade really looking insight implementation consequences bound constraints cma es optimization come across [biedrzycki ] staff elka pw edu pl ~rbiedrzy publ rbswevo_cma_bchms pdf discusses topic lot paper findings least come across fairly natural finding lot wondering whether anybody else recommendations reading personal stories potential pitfalls problem dealing specifically coordinates mutant search point vectors need bound within unit interval wanted make sure walking easily preventable mistakes **background context** currently working personal project inspired popular [geijtenbeek et al ] www goatstream com research papers sa index html paper implemented hill type muscle model rigged humanoid model muscles approximating human skeletal muscle function beginning muscle control experiments inspiration paper [wochner et al ] www frontiersin org articles fncom full except plugging cma es instead bayesian optimization used paper utilizing objective function insights
[D] replacement for SoftMax when you want to activate multiple samples equally?,3,qrn5qb,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrn5qb/d_replacement_for_softmax_when_you_want_to/,4,"Softmax has been used for activating normalizing a representation in way that the most important sample will have the largest value between \[ \] and the rest will be close to  

I am wondering what if we want to activate multiple samples equally in the representation  Lets say I have a vector of \`x\` dimension  and I know  of them are equally important and I want them to have same weight after activation  so softmax wont work here  one option is to use sigmoid  and then use softmax on the top of it to make sure the sum is one  but I am not sure if this is the smartest approach  So I was wondering if people here have any other suggestion  ",1636642863.0,2021-11-11 16:01:03,softmax used activating normalizing representation way important sample largest value \[ \] rest close wondering want activate multiple samples equally representation lets say vector \`x\` dimension know equally important want weight activation softmax wont work one option use sigmoid use softmax top make sure sum one sure smartest approach wondering people suggestion
[D] Fast CC Taylor Transform for Computer Vision: Potentially train NeRF in matter of minutes,59,qr3b6w,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qr3b6w/d_fast_cc_taylor_transform_for_computer_vision/,14,"Hello 

We just released our latest paper on arXiv  We explore the idea of fast summation algorithms  equivalent of the Fast Fourier Transform but with Taylor series instead  in the context of Computer Vision  In terms of computational complexity  our approach disentangles the number of model parameters  N  and model evaluations  M   i e  our approach is in O N M  instead of O NM   This allows us to reduce the number of FLOPs required for training and inference  x depending on the problem 

Unfortunately  our current implementation is very FLOP inefficient and a FLOP efficient implementation requires a good bit of custom CUDA code  We are working on it   

Video abstract  [https  www youtube com watch v=egXoMAte] https  www youtube com watch v=egXoMAte 

Paper link  [https  arxiv org abs  ] https  arxiv org abs   

Let me know if you have any questions ",1636575898.0,2021-11-10 21:24:58,hello released latest paper arxiv explore idea fast summation algorithms equivalent fast fourier transform taylor series instead context computer vision terms computational complexity approach disentangles number model parameters n model evaluations e approach n instead nm allows us reduce number flops required training inference x depending problem unfortunately current implementation flop inefficient flop efficient implementation requires good bit custom cuda code working video abstract [ www youtube com watch v=egxomate] www youtube com watch v=egxomate paper link [ arxiv org abs ] arxiv org abs let know questions
[Research] .wav dataset for morse code,1,qrpwsn,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrpwsn/research_wav_dataset_for_morse_code/,6,Does anyone know where to obtain a dataset containing morse code  wav files   I checked Kaggle  there was a competition once  but the data is no longer available  \[Research\],1636650660.0,2021-11-11 18:11:00,anyone know obtain dataset containing morse code wav files checked kaggle competition data longer available \[research\]
[D] Landmark annotations in Blender,3,qriz01,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qriz01/d_landmark_annotations_in_blender/,5,"I am building a synthetic dataset of images for a landmark prediction task and I m using Blender 

Having looked through the main data generation libraries available for Blender on Github  vision\_blender  BlenderProc  zpy   I can t find any that support landmarks  Before I go and implement this myself  does anyone have any pointers that I m missing 

Thanks

*Update*

The following script will write out the coordinates of vertices in a rendered image

```
import bpy
scene = bpy data scenes[ Scene ]
camera = bpy data objects[ Camera ]
obj = bpy data objects[ Cube ]

matrix = camera matrix_world normalized  inverted 
  Create a new mesh data block  using the inverse transform matrix to undo any transformations   
mesh = obj to_mesh preserve_all_data_layers=True 
mesh transform obj matrix_world 
mesh transform matrix 

  Get the world coordinates for the camera frame bounding box  before any transformations   
frame = [ v for v in camera data view_frame scene=scene [ ]]

lx = []
ly = []

for v in mesh vertices 
    co_local = v co
    z =  co_local z

    if z  =   
          Vertex is behind the camera  ignore it   
        continue
    else 
          Perspective division  
        frame = [ v    v z   z  for v in frame]

    min_x  max_x = frame[] x  frame[] x
    min_y  max_y = frame[] y  frame[] y

    x =  co_local x   min_x     max_x   min_x 
    y =  co_local y   min_y     max_y   min_y 

    lx append x 
    ly append y 
    
coords = [f {x}  {y} \n  for x  y in list zip lx  ly ]
with open log txt    w  as f 
    f writelines coords 

```",1636628976.0,2021-11-11 12:09:36,building synthetic dataset images landmark prediction task using blender looked main data generation libraries available blender github vision\_blender blenderproc zpy find support landmarks go implement anyone pointers missing thanks *update* following script write coordinates vertices rendered image ``` import bpy scene = bpy data scenes[ scene ] camera = bpy data objects[ camera ] obj = bpy data objects[ cube ] matrix = camera matrix_world normalized inverted create new mesh data block using inverse transform matrix undo transformations mesh = obj to_mesh preserve_all_data_layers=true mesh transform obj matrix_world mesh transform matrix get world coordinates camera frame bounding box transformations frame = [ v v camera data view_frame scene=scene [ ]] lx = [] ly = [] v mesh vertices co_local = v co z = co_local z z = vertex behind camera ignore continue else perspective division frame = [ v v z z v frame] min_x max_x = frame[] x frame[] x min_y max_y = frame[] frame[] x = co_local x min_x max_x min_x = co_local min_y max_y min_y lx append x ly append coords = [f {x} {y} \n x list zip lx ly ] open log txt w f f writelines coords ```
[D] What is the current state of deep learning theory?,23,qr6hhb,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qr6hhb/d_what_is_the_current_state_of_deep_learning/,9,"As I assume many people on this subreddit are  I m interested in understanding why currently state of the art models work  and if we can use the understanding of the underlying princples of these models to create better more powerful ones  I mean this in the sense that physics has theory or classical computer science has one  in that both develop a mathematical framework to understand and predict their subject  there is of course a difference between them as one assumes laws from experiences and onr is more firmly rooted in math  but both still use a mathematical approach   

I sometimes see some papers on this subreddit that look very relevant to this vmatter  and while I try to read some of them  given my current math education  which is somewhere between a practioneer and a BA student I do not believe I can truly understand and therefore judge them 

So for those of you who are experts in this  do we have an accepted predictive theory for deep learning models  If not  what do we have ",1636584690.0,2021-11-10 23:51:30,assume many people subreddit interested understanding currently state art models work use understanding underlying princples models create better powerful ones mean sense physics theory classical computer science one develop mathematical framework understand predict subject course difference one assumes laws experiences onr firmly rooted math still use mathematical approach sometimes see papers subreddit look relevant vmatter try read given current math education somewhere practioneer ba student believe truly understand therefore judge experts accepted predictive theory deep learning models
[D] Paper Explained - Autoregressive Diffusion Models (Full Video Walkthrough),20,qr5per,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qr5per/d_paper_explained_autoregressive_diffusion_models/,1,"[https  youtu be htRsQzipQ] https  youtu be htRsQzipQ 

Diffusion models have made large advances in recent months as a new type of generative models  This paper introduces Autoregressive Diffusion Models  ARDMs   which are a mix between autoregressive generative models and diffusion models  ARDMs are trained to be agnostic to the order of autoregressive decoding and give the user a dynamic tradeoff between speed and performance at decoding time  This paper applies ARDMs to both text and image data  and as an extension  the models can also be used to perform lossless compression 

 xB 

OUTLINE 

    Intro   Overview

    Decoding Order in Autoregressive Models

    Autoregressive Diffusion Models

    Dependent and Independent Sampling

    Application to Character Level Language Models

    How Sampling   Training Works

    Extension   Parallel Sampling

    Extension   Depth Upscaling

    Conclusion   Comments

 xB 

Paper  [https  arxiv org abs  ] https  arxiv org abs   ",1636582575.0,2021-11-10 23:16:15,[ youtu htrsqzipq] youtu htrsqzipq diffusion models made large advances recent months new type generative models paper introduces autoregressive diffusion models ardms mix autoregressive generative models diffusion models ardms trained agnostic order autoregressive decoding give user dynamic tradeoff speed performance decoding time paper applies ardms text image data extension models also used perform lossless compression xb outline intro overview decoding order autoregressive models autoregressive diffusion models dependent independent sampling application character level language models sampling training works extension parallel sampling extension depth upscaling conclusion comments xb paper [ arxiv org abs ] arxiv org abs
[D] What are your favourite annotation platforms?,0,qrknk3,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrknk3/d_what_are_your_favourite_annotation_platforms/,4,"I have tried a few annotations platforms 

 Redbrick AI

 V Labs

 Supervise ly


None of them quite hit the sweet spot for my use case  I mainly care about uploading pre annotations of a semantic segmentation network to be double checked by human annotators 

The aforementioned platforms do offer this service but it involves fiddling with their respective SDKs and doesn t always work that well 

Can anyone share their favourite annotation platforms and why ",1636635174.0,2021-11-11 13:52:54,tried annotations platforms redbrick ai v labs supervise ly none quite hit sweet spot use case mainly care uploading pre annotations semantic segmentation network double checked human annotators aforementioned platforms offer service involves fiddling respective sdks always work well anyone share favourite annotation platforms
"[R] Microsoft India Proposes Varuna: Scalable, Low-Cost Training of Massive Deep Learning Models",45,qqxcgt,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqxcgt/r_microsoft_india_proposes_varuna_scalable/,8,"A Microsoft Research India team presents Varuna  a system for training massive deep learning models on commodity networking that eliminates the need for specialized hyperclusters and alleviates the cost  scale  and resource utilization challenges of deep learning model training  

Here is a quick read  [Microsoft India Proposes Varuna  Scalable  Low Cost Training of Massive Deep Learning Models ] https  syncedreview com    deepmind podracer tpu based rl frameworks deliver exceptional performance at low cost  

The Varuna code has been open sourced and is available on the project’s [Github] https  github com microsoft varuna   The paper *Varuna  Scalable  Low cost Training of Massive Deep Learning Models* is on [arXiv] https  arxiv org abs    ",1636559400.0,2021-11-10 16:50:00,microsoft research india team presents varuna system training massive deep learning models commodity networking eliminates need specialized hyperclusters alleviates cost scale resource utilization challenges deep learning model training quick read [microsoft india proposes varuna scalable low cost training massive deep learning models ] syncedreview com deepmind podracer tpu based rl frameworks deliver exceptional performance low cost varuna code open sourced available project’s [github] github com microsoft varuna paper *varuna scalable low cost training massive deep learning models* [arxiv] arxiv org abs
[D] Is anyone working on code generation other than OpenAI?,4,qravhd,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qravhd/d_is_anyone_working_on_code_generation_other_than/,7,Are there any other works in code generation models like Codex from OpenAI  I have seen some open source version of Codex but no variants of that model ,1636598475.0,2021-11-11 03:41:15,works code generation models like codex openai seen open source version codex variants model
[P] List of ICLR 2022 Papers with Review Scores,20,qqzpf6,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqzpf6/p_list_of_iclr_2022_papers_with_review_scores/,2,"ICLR  reviews are publicly available now  We have compiled a list of papers sorted by the review scores  weighted by confidence  

**Link ** [https  papers labml ai papers iclr\_ sort\_by=conference\_score dsc=] https  papers labml ai papers iclr_ sort_by=conference_score dsc= 

**Mean review score is   **

https  preview redd it zhrojxsy png width= format=png auto=webp s=fdebdbdbba",1636565909.0,2021-11-10 18:38:29,iclr reviews publicly available compiled list papers sorted review scores weighted confidence **link ** [ papers labml ai papers iclr\_ sort\_by=conference\_score dsc=] papers labml ai papers iclr_ sort_by=conference_score dsc= **mean review score ** preview redd zhrojxsy png width= format=png auto=webp s=fdebdbdbba
[D] What are the advances on encryption?,0,qrhh89,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrhh89/d_what_are_the_advances_on_encryption/,7,"I m new to the topic  I understand that it s easy for AI find a valid result from a predictable algorithm like a basic letter letter encryption with a small dataset  I saw it as an example in a lecture 

But how about more complicated encryption algorithms  bits  bits  bits  TLS  How big would the dataset get to be able to easily predict a correct unencrypted text from encrypted text 

Can AI be used to test if an algorithm is actually safe by solving it without really knowing how exactly the data was encrypted  For example if an encryption standard that looks very safe to experts but has an unknown mathematical shortcut AI could find it  is that possible  

Where can I find more about this topic ",1636622767.0,2021-11-11 10:26:07,new topic understand easy ai find valid result predictable algorithm like basic letter letter encryption small dataset saw example lecture complicated encryption algorithms bits bits bits tls big would dataset get able easily predict correct unencrypted text encrypted text ai used test algorithm actually safe solving without really knowing exactly data encrypted example encryption standard looks safe experts unknown mathematical shortcut ai could find possible find topic
[D] What are simple projects I can do with OpenAI?,0,qrm0o7,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrm0o7/d_what_are_simple_projects_i_can_do_with_openai/,2,I m thinking about making a SQL query generator from English language  Any other ideas are welcome  Thanks ,1636639462.0,2021-11-11 15:04:22,thinking making sql query generator english language ideas welcome thanks
[D] Advice on buying PC,44,qqqzmu,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqqzmu/d_advice_on_buying_pc/,76,"I m doing a PhD in ML and have decided I need a better set up computationally  Currently I m running most scripts either locally on my Dell XPS   no GPU  or on Colab  but evidently both are insufficient  As there s a budget in my PhD studentship for equipment  I m thinking of buying a desktop that I can SSH into from my laptop  Does anyone have experience with this  and if so  recommendations  e g  for GPU CPU specs  

Info 

* Area of research not yet set  though I will definitely be working with generative models  though not huge models  only tabular and time series data   Work often involves data preprocessing steps that might mainly require CPU power 
* No fixed budget  though I ll need to justify my purchase

It seems buying a moderately good gaming PC with Nvidia GPU is financially and computationally interesting 

Any help is appreciated 

 xB 

EDIT  Budget wise I was thinking below £ to be able to justify it to my funding body  but if this is too limiting please let me know ",1636538365.0,2021-11-10 10:59:25,phd ml decided need better set computationally currently running scripts either locally dell xps gpu colab evidently insufficient budget phd studentship equipment thinking buying desktop ssh laptop anyone experience recommendations e g gpu cpu specs info * area research yet set though definitely working generative models though huge models tabular time series data work often involves data preprocessing steps might mainly require cpu power * fixed budget though need justify purchase seems buying moderately good gaming pc nvidia gpu financially computationally interesting help appreciated xb edit budget wise thinking £ able justify funding body limiting please let know
[D] How to avoid CPU bottlenecking in PyTorch - training slowed by augmentations and data loading?,9,qr0rck,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qr0rck/d_how_to_avoid_cpu_bottlenecking_in_pytorch/,20,"Hello

My colleague and I are training models on a few workstations and we are noticing some bottlenecks that are not leveraging all our GPUs and stopping us from reaching full performance  We are curious what techniques folks use in Python   PyTorch to fully make use of the available CPU cores to keep the GPUs saturated  data loading or data formatting tricks  etc 

Firstly our systems 

*  AMD  Ryzen   GB Ram x  FE   M SSDs for Data sets
*  Intel i k   GB Ram  x  FE   M SSDs for Data Sets

We notice that both of our systems take the same amount of time per epoch   ie   we get no gains with  GPUs vs  GPUs  which is frustrating 

Some things we are observing 
* CPUs on both systems spike to   CPU on occasion but aren t always utilized
* Disk throughput via IOTOP shows around     MB s max read  which is way below SSD speeds  Surprisingly low 
* GPU usage is very spikey  

[Here s an image of NVTop and HTop for both systems] https  imgur com a MhFuISB 

Some things we are doing  

* We are using PyTorch  
* Pillow Simd and the latest Nvidia NGC containers  We also use PyTorch Lighting for training  
* [We follow most of the best practices here] https  tigress web princeton edu ~jdh PyTorchPerformanceTuningGuide_GTC pdf 
* We are setting to gradient to none instead of zero grad for performance small improvements
* We are setting cu DNN auto benchmark to true
* We are using the Distributed Data Parallel accelerator 
* We are using Pinned Memory 
* We are using num_workers = 
* We see this behavior of low GPU usage   without augmentations 
* We ve reduced batch size as an experiment to see where the issues lie  We are at  rd max possible batch size   and we see maybe a    difference in performance 

Some things we have observed 

* We get intermittent crashing if we increase num workers above    
* We ve noticed GPU  on our  GPU system is sometimes idle  which would explain performance differences   However its unclear to us why that may be  [Similar to this issue] https  github com PyTorchLightning pytorch lightning discussions  

Our guess is image loading and pre processing appear to be the issue  We aren t entirely sure if we are diagnosing this correctly 

How are folks getting around issues like these  Should we be pre processing our data set somehow and storing it in a more optimal format  We are relying on Pillow Simd for image reading  decoding and copying to tensors 

Are there any good pragmatic guides to optimizing training 

Thank you ",1636568821.0,2021-11-10 19:27:01,hello colleague training models workstations noticing bottlenecks leveraging gpus stopping us reaching full performance curious techniques folks use python pytorch fully make use available cpu cores keep gpus saturated data loading data formatting tricks etc firstly systems * amd ryzen gb ram x fe ssds data sets * intel k gb ram x fe ssds data sets notice systems take amount time per epoch ie get gains gpus vs gpus frustrating things observing * cpus systems spike cpu occasion always utilized * disk throughput via iotop shows around mb max read way ssd speeds surprisingly low * gpu usage spikey [here image nvtop htop systems] imgur com mhfuisb things * using pytorch * pillow simd latest nvidia ngc containers also use pytorch lighting training * [we follow best practices here] tigress web princeton edu ~jdh pytorchperformancetuningguide_gtc pdf * setting gradient none instead zero grad performance small improvements * setting cu dnn auto benchmark true * using distributed data parallel accelerator * using pinned memory * using num_workers = * see behavior low gpu usage without augmentations * reduced batch size experiment see issues lie rd max possible batch size see maybe difference performance things observed * get intermittent crashing increase num workers * noticed gpu gpu system sometimes idle would explain performance differences however unclear us may [similar issue] github com pytorchlightning pytorch lightning discussions guess image loading pre processing appear issue entirely sure diagnosing correctly folks getting around issues like pre processing data set somehow storing optimal format relying pillow simd image reading decoding copying tensors good pragmatic guides optimizing training thank
[R] MIT AI Researchers Introduce ‘PARP’: A Method To Improve The Efficiency And Performance Of A Neural Network,94,qqmhiz,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqmhiz/r_mit_ai_researchers_introduce_parp_a_method_to/,9,"Recent developments in machine learning have enabled automated speech recognition technologies  such as Siri  to learn the world’s uncommon languages  which lack the enormous volume of transcribed speech required to train algorithms  However  these methods are frequently too complicated and costly to be broadly used 

Researchers from MIT  National Taiwan University  and the University of California  Santa Barbara  have developed a simple technique that minimizes the complexity of a sophisticated speech learning model  allowing it to run more efficiently and achieve higher performance 

Their method entails deleting unneeded components from a standard but complex speech recognition model and then making slight tweaks to recognize a given language  Teaching this model an unusual language is a low cost and time efficient process because only minor adjustments are required once the larger model is trimmed down to size 

  [Read The] https  arxiv org pdf   pdf  [Paper] https  arxiv org pdf   pdf    [Checkout The] https  people csail mit edu clai parp  [Project] https  people csail mit edu clai parp    [ Min Read] https  www marktechpost com    mit ai researchers introduce parp a method to improve the efficiency and performance of a neural network    [MIT Blog] https  news mit edu  speech recognition uncommon languages  

 xB 

https  preview redd it rigqmwvdpy png width= format=png auto=webp s=cfafadbcbaadcffefcaeb",1636519924.0,2021-11-10 05:52:04,recent developments machine learning enabled automated speech recognition technologies siri learn world’s uncommon languages lack enormous volume transcribed speech required train algorithms however methods frequently complicated costly broadly used researchers mit national taiwan university university california santa barbara developed simple technique minimizes complexity sophisticated speech learning model allowing run efficiently achieve higher performance method entails deleting unneeded components standard complex speech recognition model making slight tweaks recognize given language teaching model unusual language low cost time efficient process minor adjustments required larger model trimmed size [read the] arxiv org pdf pdf [paper] arxiv org pdf pdf [checkout the] people csail mit edu clai parp [project] people csail mit edu clai parp [ min read] www marktechpost com mit ai researchers introduce parp method improve efficiency performance neural network [mit blog] news mit edu speech recognition uncommon languages xb preview redd rigqmwvdpy png width= format=png auto=webp s=cfafadbcbaadcffefcaeb
"[D] Mel Spectrum, is it useful for non-speech recognition classification tasks?",4,qr1pex,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qr1pex/d_mel_spectrum_is_it_useful_for_nonspeech/,7,"I am working on a long term project involving the development of a process monitoring program for an additive manufacturing process  I have read a good deal on different techniques used in audio classification  speech recognition  etc  From what I understand  most machine learning models involving audio either use features extracted from the time and frequency representations of the signal or spectrograms as inputs 

When looking into audio feature extraction  I have noticed it is very common to extract frequency domain features from the mel spectrum  Obviously  the mel spectrum is very useful in speech recognition tasks since it is intended to align with how a human ear perceives sound  My question  is the mel spectrum useful for audio classification or anomaly detection tasks that do not involve human speech  If so  what is the reason it would be more useful than say the standard frequency scale spectrum for a non speech recognition task 

 literature references would be helpful ",1636571402.0,2021-11-10 20:10:02,working long term project involving development process monitoring program additive manufacturing process read good deal different techniques used audio classification speech recognition etc understand machine learning models involving audio either use features extracted time frequency representations signal spectrograms inputs looking audio feature extraction noticed common extract frequency domain features mel spectrum obviously mel spectrum useful speech recognition tasks since intended align human ear perceives sound question mel spectrum useful audio classification anomaly detection tasks involve human speech reason would useful say standard frequency scale spectrum non speech recognition task literature references would helpful
[D] Can a trained discriminator in Gan be used for multi class classification ?,3,qr44bi,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qr44bi/d_can_a_trained_discriminator_in_gan_be_used_for/,1,I have been thinking of using discriminator as a classifier after it is trained once with gan  But I have no idea on how to make discriminator into classifier because discriminator only says if it is fake or real   Any leads please,1636578184.0,2021-11-10 22:03:04,thinking using discriminator classifier trained gan idea make discriminator classifier discriminator says fake real leads please
"[R] Rebooting ACGAN: A new GAN that achieves SOTA results and harmonizes with various architectures, adversarial losses, and even differentiable augmentations (Neurips 2021).",12,qqu6xh,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqu6xh/r_rebooting_acgan_a_new_gan_that_achieves_sota/,1,"A research team from Pohang University of Science and Technology introduces a new type of ACGAN  the Rebooted Auxiliary Classifier GAN  ReACGAN  to overcome unstable training and poor generation performance of ACGAN  

Here is a quick summary of the paper

* Gradient exploding in the classifier of ACGAN can cause an undesirable training collapse 
* Simply normalizing feature embeddings can resolve the problem 
* Using the normalization technique  we propose the Rebooted Auxiliary Classifier GAN  ReACGAN  
* ReACGAN achieves state of the arts generation results on benchmark datasets  
* ReACGAN harmonizes with various GAN architectures  DCGAN  ResNet  Big ResNet  StyleGAN   adversarial losses  and differentiable augmentations  ADA  DiffAugment    


arXiv  [https  arxiv org abs  ] https  arxiv org abs     


https  preview redd it szmabijry png width= format=png auto=webp s=dfbedccaafed

https  preview redd it mzoszjry png width= format=png auto=webp s=bbddecacebabeaacbfb",1636550132.0,2021-11-10 14:15:32,research team pohang university science technology introduces new type acgan rebooted auxiliary classifier gan reacgan overcome unstable training poor generation performance acgan quick summary paper * gradient exploding classifier acgan cause undesirable training collapse * simply normalizing feature embeddings resolve problem * using normalization technique propose rebooted auxiliary classifier gan reacgan * reacgan achieves state arts generation results benchmark datasets * reacgan harmonizes various gan architectures dcgan resnet big resnet stylegan adversarial losses differentiable augmentations ada diffaugment arxiv [ arxiv org abs ] arxiv org abs preview redd szmabijry png width= format=png auto=webp s=dfbedccaafed preview redd mzoszjry png width= format=png auto=webp s=bbddecacebabeaacbfb
[discussion] Plaforms/Frameworks for Backtesting and Regression Testing Lots of Models?,2,qr6bu6,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qr6bu6/discussion_plaformsframeworks_for_backtesting_and/,2,"I have a need to test lots of models submitted by different teams     There will be baseline  curated  datasets but there will also be updates as new data comes in from the field    Models may have different preprocessing requirements  

  We ll need to retrain models on the updated data  evaluate the models  and archive the reports and models  configuration management      Most of this will probably need to be queued up  something like slurm  along with the need to asynchronously monitor performance of multiple DGX servers 

Are there commercial tools to manage testing of a fleet of models and the data      must work offline ",1636584255.0,2021-11-10 23:44:15,need test lots models submitted different teams baseline curated datasets also updates new data comes field models may different preprocessing requirements need retrain models updated data evaluate models archive reports models configuration management probably need queued something like slurm along need asynchronously monitor performance multiple dgx servers commercial tools manage testing fleet models data must work offline
[P] Machine Learning tutorial in R,0,qr9ndf,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qr9ndf/p_machine_learning_tutorial_in_r/,0,"Does anybody know of a good tutorial that would help me do this in R 

* A model to predict the probability of a home run for a given ball in play
* An explanation of your chosen model features
* A visualization of your model outputs
* Identify the home run that was least likely to be a home run  and the non home run that was most likely to be a home run  Describe why you think your model classified these plays less accurately than others 

Advice for how to get started model to choose would be helpful as well but I do not want answers here I want to learn  So far I have a model that has an overall prediction accuracy of    compared to   for completely random   and a prediction accuracy of   when at least one of the predicted or actual result is home run    accuracy for random model   The models I have been using are KNN and random forest",1636594461.0,2021-11-11 02:34:21,anybody know good tutorial would help r * model predict probability home run given ball play * explanation chosen model features * visualization model outputs * identify home run least likely home run non home run likely home run describe think model classified plays less accurately others advice get started model choose would helpful well want answers want learn far model overall prediction accuracy compared completely random prediction accuracy least one predicted actual result home run accuracy random model models using knn random forest
"[N] Modernizing Gov Finance Data & Creating Infrastructure for Fed AI Adoption with Justin Marsico, BFS CDO Thursday, November 18, 2021 at 11:30 AM ET",6,qqvsu0,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqvsu0/n_modernizing_gov_finance_data_creating/,1,"Hi r MachineLearning 

I wanted to share an upcoming webinar with you  Below are the details from the website 

**Featured Guest Speaker  Justin Marsico  Chief Data Officer  and Deputy Assistant Commissioner at the Bureau of the Fiscal Service **

During this presentation learn how the US Treasury s Bureau of the  Fiscal Service is building a better public understanding of federal  finance as they continue to modernize their management of data and data  sharing  Justin Marsico  Chief Data Officer for the Fiscal Service  shares unique opportunities around data at the federal level  what it  entails to create the infrastructure for AI adoption  how they are  recruiting data scientists  as well as the challenges and opportunities  in data governance  security  ownership and related data areas  By  offering clear  accessible information  citizens can see how their  taxpayer dollars are spent and learn about the federal budget  Justin  will give a live demonstration of the latest online resources so  attendees can learn how the Fiscal Service is enhancing data education  and building and enhancing public trust through greater transparency 

Come join us for this great presentation around areas related to 

* Enhancing AI Skills in the Government Workforce Recruiting Data Scientists
* Finding the most appropriate use cases
* Creating the Infrastructure for AI Adoption

and stick around for Q A with Justin at the end 

**Agenda **

   pm  Featured Presentation

   pm  Your Q A and interaction

Link to website  [https  events cognilytica com CLNDMzMXwxOA] https  events cognilytica com CLNDMzMXwxOA ",1636554957.0,2021-11-10 15:35:57,hi r machinelearning wanted share upcoming webinar details website **featured guest speaker justin marsico chief data officer deputy assistant commissioner bureau fiscal service ** presentation learn us treasury bureau fiscal service building better public understanding federal finance continue modernize management data data sharing justin marsico chief data officer fiscal service shares unique opportunities around data federal level entails create infrastructure ai adoption recruiting data scientists well challenges opportunities data governance security ownership related data areas offering clear accessible information citizens see taxpayer dollars spent learn federal budget justin give live demonstration latest online resources attendees learn fiscal service enhancing data education building enhancing public trust greater transparency come join us great presentation around areas related * enhancing ai skills government workforce recruiting data scientists * finding appropriate use cases * creating infrastructure ai adoption stick around q justin end **agenda ** pm featured presentation pm q interaction link website [ events cognilytica com clndmzmxwxoa] events cognilytica com clndmzmxwxoa
[P] New open-source vector search solution,7,qqssgq,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqssgq/p_new_opensource_vector_search_solution/,2,"Meet our new open source vector search solution   Vektonn 

We offer an opportunity for product teams and data scientists to solve the problem of reliable vector data storage  scalability  and undisturbed availability 

We store embeddings and their attributes  which are more interesting to users since they can use real world objects  For example  they can identify objects using their real identification 

We support changing indexes as new data arrives  delete  change  or add data to the index  parallel with search queries 

You can expand multiple indexes over a single data source  vectors and attributes  and seamlessly transition to new versions of indexes  You can expand different indexes with different parameters of the same data 

You can work with vectors of any type  For example  you can use bag of words to solve word processing problems and load appropriate sparse vectors into Vektonn 

We d appreciate any feedback or suggestions for the project and welcome GitHub stars to join in if  of course  you find it interesting  🙂

Learn more   [https  vektonn io ] https  vektonn io 

See what we have done   [https  github com vektonn] https  github com vektonn ",1636545435.0,2021-11-10 12:57:15,meet new open source vector search solution vektonn offer opportunity product teams data scientists solve problem reliable vector data storage scalability undisturbed availability store embeddings attributes interesting users since use real world objects example identify objects using real identification support changing indexes new data arrives delete change add data index parallel search queries expand multiple indexes single data source vectors attributes seamlessly transition new versions indexes expand different indexes different parameters data work vectors type example use bag words solve word processing problems load appropriate sparse vectors vektonn appreciate feedback suggestions project welcome github stars join course find interesting 🙂 learn [ vektonn io ] vektonn io see done [ github com vektonn] github com vektonn
[R] Partition and Code: learning how to compress graphs,14,qqpfye,MachineLearning,https://arxiv.org/abs/2107.01952,2,nan,1636531492.0,2021-11-10 09:04:52,nan
[N] NVIDIA GTC 2021,0,qr5t70,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qr5t70/n_nvidia_gtc_2021/,0,Check out OmniSci’s session at the NVIDIA GTC  for FREE  Learn how BIDMC Dept of Endocrinology is leveraging OmniSci’s GPU accelerated analytics platform to explore massive amounts of transcriptomic data and how that has advanced their research processes  Register here  https  reg rainfocus com flow nvidia nvidiagtc ap page sessioncatalog search= A  ncid=ref spo ,1636582860.0,2021-11-10 23:21:00,check omnisci’s session nvidia gtc free learn bidmc dept endocrinology leveraging omnisci’s gpu accelerated analytics platform explore massive amounts transcriptomic data advanced research processes register reg rainfocus com flow nvidia nvidiagtc ap page sessioncatalog search= ncid=ref spo
[D] Why does AMD do so much less work in AI than NVIDIA?,358,qq21s6,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qq21s6/d_why_does_amd_do_so_much_less_work_in_ai_than/,98,"Or is the assumption in the title false 

Does AMD just not care  or did they get left behind somehow and can t catch up 

 xB 

I know this question is very vague  maybe still somebody can point to a fitting interview or something else",1636458809.0,2021-11-09 12:53:29,assumption title false amd care get left behind somehow catch xb know question vague maybe still somebody point fitting interview something else
[D] How to train GANs really fast - Projected GANs Converge Faster explained (5-minute summary by Casual GAN Papers),25,qqfe6y,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqfe6y/d_how_to_train_gans_really_fast_projected_gans/,9,"Despite significant progress in the field training GANs from scratch is still no easy task  especially for smaller datasets  Luckily Axel Sauer and the team at the University of Tübingen came up with a Projected GAN  that achieves SOTA level FID in hours instead of days and works on even the tiniest datasets  The new training method works by utilizing a  pretrained network to obtain embeddings for real and fake images that the discriminator processes  Additionally  feature pyramids provide multi scale feedback from multiple discriminators and random projections better utilize deeper layers of the pretrained network 

Full summary  [https  t me casual\_gan ] https  t me casual_gan  

Blog post  [https  www casualganpapers com data efficient fast gan training small datasets ProjectedGAN explained html] https  www casualganpapers com data efficient fast gan training small datasets ProjectedGAN explained html 

[ProjectedGAN] https  preview redd it vqyvadpeny png width= format=png auto=webp s=ffbebdcdacdffad 

UPD  I originally included the wrong links  
[arxiv] https  studios disneyresearch com app uploads   Adaptive Convolutions for Structure Aware Style Transfer pdf    [code] https  github com RElbers ada conv pytorch 

Subscribe to [Casual GAN Papers] https  t me casual_gan  and follow me on [Twitter] https  twitter com KirillDemochkin  for weekly AI paper summaries ",1636497858.0,2021-11-09 23:44:18,despite significant progress field training gans scratch still easy task especially smaller datasets luckily axel sauer team university tübingen came projected gan achieves sota level fid hours instead days works even tiniest datasets new training method works utilizing pretrained network obtain embeddings real fake images discriminator processes additionally feature pyramids provide multi scale feedback multiple discriminators random projections better utilize deeper layers pretrained network full summary [ casual\_gan ] casual_gan blog post [ www casualganpapers com data efficient fast gan training small datasets projectedgan explained html] www casualganpapers com data efficient fast gan training small datasets projectedgan explained html [projectedgan] preview redd vqyvadpeny png width= format=png auto=webp s=ffbebdcdacdffad upd originally included wrong links [arxiv] studios disneyresearch com app uploads adaptive convolutions structure aware style transfer pdf [code] github com relbers ada conv pytorch subscribe [casual gan papers] casual_gan follow [twitter] twitter com kirilldemochkin weekly ai paper summaries
[D] Good Advertising Papers Using RL or DL,3,qquog8,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qquog8/d_good_advertising_papers_using_rl_or_dl/,2,"Hey  everybody

I m currently working in AdTech and I m searching for innovative products in advertising  CTR prediction  Digital Inventory Pricing   Online learning for costumer segmentation or other stuff  

If you can help me  please suggest me a good paper in the comments or where I d be able to find good papers in those subjects  I ve looked into fb research and arxiv mainly  ",1636551616.0,2021-11-10 14:40:16,hey everybody currently working adtech searching innovative products advertising ctr prediction digital inventory pricing online learning costumer segmentation stuff help please suggest good paper comments able find good papers subjects looked fb research arxiv mainly
[D] How do you choose an Optimizer? And why are there so many?,52,qq75zu,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qq75zu/d_how_do_you_choose_an_optimizer_and_why_are/,29,"Choosing an optimizer for the training of ANNS is one of the most critical design choices  Because ANNS are black boxes  the theoretical guidelines on the overall design are very limited  They are mostly anecdotally and strongly depend on the developers experience 

 xB 

When it comes to optimizer  there are hunderts available  from SGD to Adam to very specific ones  It feels like there is a custom tailored optimizer for every problem and every architecture 

 xB 

Why is it so hard to come up with something more general  Why are there this many optimizer  And  how do you choose an optimizer for your project ",1636474726.0,2021-11-09 17:18:46,choosing optimizer training anns one critical design choices anns black boxes theoretical guidelines overall design limited mostly anecdotally strongly depend developers experience xb comes optimizer hunderts available sgd adam specific ones feels like custom tailored optimizer every problem every architecture xb hard come something general many optimizer choose optimizer project
[Discussion] ICLR 2022 submission statistics,8,qqkwas,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqkwas/discussion_iclr_2022_submission_statistics/,0,"The statistics of ICLR submission can be found here 

[https  guoqiangwei xyz htmls iclr\_stats html] https  guoqiangwei xyz htmls iclr_stats html 

https  preview redd it spfsgsoy png width= format=png auto=webp s=daceaffbafedc

** **  **number of reviewers for each submission **

[**https  guoqiangwei xyz htmls iclr\_stats\_number\_of\_reviewers html**] https  guoqiangwei xyz htmls iclr_stats_number_of_reviewers html 

* **Submissions with maximum   reviewers **  item in total  each with  reviewers

https  preview redd it fknneqvqlqy png width= format=png auto=webp s=fabaceedcacade

* **Submissions with minimum   reviewers **   item in total  each with  reviewers

https  preview redd it uehgnwulqy png width= format=png auto=webp s=fbebdcfecbda

**  Rating variance**

[**https  guoqiangwei xyz htmls iclr\_stats\_variance html**] https  guoqiangwei xyz htmls iclr_stats_variance html 

* **Submissions with highest rating variance**

https  preview redd it ckbkhzbmqy png width= format=png auto=webp s=bdfaeeecabdeddbb

* **Submissions with highest rating gap  max   min **

https  preview redd it mggqsuemqy png width= format=png auto=webp s=eeabebbdcbabbe

* **Submissions with lowest rating variance**

https  preview redd it adxlgehmqy png width= format=png auto=webp s=bcbbebcbeddfffbedcbebfe

 xB 

 xB 

More data can be found here 

[https  github com weigq iclr\_stats] https  github com weigq iclr_stats ",1636514526.0,2021-11-10 04:22:06,statistics iclr submission found [ guoqiangwei xyz htmls iclr\_stats html] guoqiangwei xyz htmls iclr_stats html preview redd spfsgsoy png width= format=png auto=webp s=daceaffbafedc ** ** **number reviewers submission ** [** guoqiangwei xyz htmls iclr\_stats\_number\_of\_reviewers html**] guoqiangwei xyz htmls iclr_stats_number_of_reviewers html * **submissions maximum reviewers ** item total reviewers preview redd fknneqvqlqy png width= format=png auto=webp s=fabaceedcacade * **submissions minimum reviewers ** item total reviewers preview redd uehgnwulqy png width= format=png auto=webp s=fbebdcfecbda ** rating variance** [** guoqiangwei xyz htmls iclr\_stats\_variance html**] guoqiangwei xyz htmls iclr_stats_variance html * **submissions highest rating variance** preview redd ckbkhzbmqy png width= format=png auto=webp s=bdfaeeecabdeddbb * **submissions highest rating gap max min ** preview redd mggqsuemqy png width= format=png auto=webp s=eeabebbdcbabbe * **submissions lowest rating variance** preview redd adxlgehmqy png width= format=png auto=webp s=bcbbebcbeddfffbedcbebfe xb xb data found [ github com weigq iclr\_stats] github com weigq iclr_stats
[N] Message from r/MLOps: Announcing Our first AMA!,16,qqckfw,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqckfw/n_message_from_rmlops_announcing_our_first_ama/,2,"Hi  mod of r MLOPs here  We finally managed to book our first AMA  which should interest some of the M members here 

The following is the content of the sticky over at our sub 
Would you please help us grow our community of MLOps enthusiasts by not burying this post 八 ＾□＾* 
 

I don t know if you remember  but we were going to have AMAs here to celebrate the fact that there are so many of us  
Naturally  since this is a niche subreddit  it wasn t as if top tier mlops superheroes were lining up to post an AMA here 


But then  a miracle happened  



I am delighted to announce that [Alessya Visnjic] https  twitter com zalessya status  s=  will be doing an AMA  Here  this Thursday  So spread the word  and let s make this AMA be the first of many successful ones 


And just in case you are not as immersed in the MLOps ecosphere as I am  here is her bio 


 Alessya Visnjic is the CEO and co founder of WhyLabs  the AI Observability company on a mission to build the interface between AI and human operators  Before WhyLabs  Alessya was a CTO in residence at the Allen Institute for AI  AI   evaluating the commercial potential for the latest advancements in AI research  Earlier in her career  Alessya spent  years at Amazon leading Machine Learning adoption and tooling efforts  She was a founding member of Amazon s first ML research center in Berlin  Germany  Alessya is also the founder of Rsqrd AI  a global community of    AI practitioners committed to making AI technology Robust   Responsible  

Of course  there s always an ulterior motive  Alessya will be focusing on the recent announcements by WhyLabs   their round of funding and their new SaaS solution called AI Observatory  

Personally  I think their corner of the MLOps tooling space is super exciting  and WhyLabs are doing some hard opensource groundwork  Additionally  their marketing is not spammy  so it s an honor to host them on the sub ",1636489878.0,2021-11-09 21:31:18,hi mod r mlops finally managed book first ama interest members following content sticky sub would please help us grow community mlops enthusiasts burying post 八 ＾□＾* know remember going amas celebrate fact many us naturally since niche subreddit top tier mlops superheroes lining post ama miracle happened delighted announce [alessya visnjic] twitter com zalessya status s= ama thursday spread word let make ama first many successful ones case immersed mlops ecosphere bio alessya visnjic ceo co founder whylabs ai observability company mission build interface ai human operators whylabs alessya cto residence allen institute ai ai evaluating commercial potential latest advancements ai research earlier career alessya spent years amazon leading machine learning adoption tooling efforts founding member amazon first ml research center berlin germany alessya also founder rsqrd ai global community ai practitioners committed making ai technology robust responsible course always ulterior motive alessya focusing recent announcements whylabs round funding new saas solution called ai observatory personally think corner mlops tooling space super exciting whylabs hard opensource groundwork additionally marketing spammy honor host sub
[D] Motivation for a 3D bounding-box in ADAS,0,qqwa9u,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqwa9u/d_motivation_for_a_3d_boundingbox_in_adas/,0,One of the common test sets for  car recognition  is called KITTI  Until  they had only a D bounding box test set  but around that year they moved to a D bounding box test sets  Can anyone explain what is the motivation behind a D bounding box for ADAS and its features ,1636556399.0,2021-11-10 15:59:59,one common test sets car recognition called kitti bounding box test set around year moved bounding box test sets anyone explain motivation behind bounding box adas features
"""[D]"" Interesting bit of info from HTC Keynote - Nvidia Selene 500 node superpod trained GPT3 in 11 days",8,qqhct3,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqhct3/d_interesting_bit_of_info_from_htc_keynote_nvidia/,1,I m not sure the cost to run such a machine  but I d imagine that represents a big cost and time reduction vs  year ago ,1636503654.0,2021-11-10 01:20:54,sure cost run machine imagine represents big cost time reduction vs year ago
[D] Virtual MLOps Round Table,35,qq6jgo,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qq6jgo/d_virtual_mlops_round_table/,1,"I m putting together an MLOps roundtable focused on peer learning  ML and MLOps practitioners can share how they utilize MLOps to automate and scale their ML processes and learn about how other teams structure theirs 

It works like this  we break people up into small groups of   people based on their team size and what they are working on  

There is absolutely no selling or pitching  The focus is pure peer learning 

You can sign up here if you re interested  [https  www eventbrite com e ] https  www eventbrite com e  

Let me know if you have any ideas  thoughts  or feedback ",1636472979.0,2021-11-09 16:49:39,putting together mlops roundtable focused peer learning ml mlops practitioners share utilize mlops automate scale ml processes learn teams structure works like break people small groups people based team size working absolutely selling pitching focus pure peer learning sign interested [ www eventbrite com e ] www eventbrite com e let know ideas thoughts feedback
[P] GPT-3 in the style of Shel Silverstein,12,qqdkeo,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqdkeo/p_gpt3_in_the_style_of_shel_silverstein/,0,"I ve recently been playing with the OpenAI API beta for GPT   and used their fine tuning API to create a model that has been fine tuned on a corpus of all of Shel Silverstein s poetry  The resulting model generates whimsical poems based on a prompted title 

Here s one example I liked   Walking On a Whale 

  I am walking on a whale 

  I feel it move and swell 

  I feel the mist come in and float 

  I feel the rain and the cold 

  But I don t mind at all 

  It s just like walking on the ground 

While the model is quite good at understanding semantics  and even has a flair for the vaguely metaphorical  it didn t pick up any notion of rhyming  It seems like this would be a hard thing for the model to learn given the training data it s seen  since only very rarely does the rhyme scheme affect the conditional likelihood of a word 

Question for any commenters  if you had a rhyming dictionary where you could simply look up whether two words rhyme instead of trying to infer this as some latent attribute  how could you update a large NLP model like this to take into account such declarative knowledge 

More examples and discussion here  http  dean dog shel silverstein gpt ",1636492686.0,2021-11-09 22:18:06,recently playing openai api beta gpt used fine tuning api create model fine tuned corpus shel silverstein poetry resulting model generates whimsical poems based prompted title one example liked walking whale walking whale feel move swell feel mist come float feel rain cold mind like walking ground model quite good understanding semantics even flair vaguely metaphorical pick notion rhyming seems like would hard thing model learn given training data seen since rarely rhyme scheme affect conditional likelihood word question commenters rhyming dictionary could simply look whether two words rhyme instead trying infer latent attribute could update large nlp model like take account declarative knowledge examples discussion http dean dog shel silverstein gpt
[D] ICLR2022 review stats,33,qq5c51,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qq5c51/d_iclr2022_review_stats/,0,"I crawled the ICLR preliminary reviews with some help of another repo and uploaded the crawled raw data  crawled today around PM UTC    You can also find some quick stats like

* distribution of mean scores etc 
* best paper by mean median score
* most controversial paper by std of scores

in the following notebook 

[https  github com VietTralala ICLR OpenReviewData blob master analyze\_reviews ipynb] https  github com VietTralala ICLR OpenReviewData blob master analyze_reviews ipynb 

Feel free to play around with it ✌  

  Excerpt of the data

  best  paper by median score 
  paper_id      title                                                                              link                                          keywords                                                                                                                                                                    mean     max     min        std     median     num  
                     
  LdlwbBPmlq   Minibatch vs Local SGD with Shuffling  Tight Convergence Bounds and Beyond         https  openreview net forum id=LdlwbBPmlq   Local SGD  Minibatch SGD  Shuffling  Without replacement  Convex Optimization  Stochastic Optimization  Federated Learning  Large Scale Learning  Distributed Learning                                                    
  iMSjopcOnp   MT  Multi Task Multitrack Music Transcription                                     https  openreview net forum id=iMSjopcOnp   music transcription  transformer  multi task learning  low resource learning  music understanding  music information retrieval                                                                                            
  BrPdXbDZkQ   DemoDICE  Offline Imitation Learning with Supplementary Imperfect Demonstrations   https  openreview net forum id=BrPdXbDZkQ   imitation learning  offline imitation learning  imperfect demonstration  non expert demonstration                                                                                                              
  sOK zSWHB    Responsible Disclosure of Generative Models Using Scalable Fingerprinting          https  openreview net forum id=sOK zSWHB    Generative models  fingerprinting  responsible disclosure  deep fake detection and attribution                                                                                                                      
  bVvMOtLMiw    DIVA  Dataset Derivative of a Learning Task                                        https  openreview net forum id=bVvMOtLMiw    Leave one out cross validation  AutoML  dataset optimization                                                                                                                                                         
  lrocYB ST   Approximation and Learning with Deep Convolutional Models  a Kernel Perspective    https  openreview net forum id=lrocYB ST   kernel methods  deep learning theory  convolution  approximation  generalization                                                                                                                                   
  siCtxZnVe   What Happens after SGD Reaches Zero Loss   A Mathematical Framework               https  openreview net forum id=siCtxZnVe   SGD  implicit bias  generalization  deep learning  implicit regularization  manifold                                                                                                                                
  DLwqQLmqV    NAS Bench Suite  NAS Evaluation is  Now  Surprisingly Easy                         https  openreview net forum id=DLwqQLmqV    neural architecture search  AutoML                                                                                                                                                                                 
  KE_FgFDgA   The MultiBERTs  BERT Reproductions for Robustness Analysis                         https  openreview net forum id=KE_FgFDgA   Pre trained models  BERT  bootstrapping  hypothesis testing  robustness                                                                                                                                        
  fKv__asZk   Learning Similarity Metrics for Volumetric Simulations with Multiscale CNNs        https  openreview net forum id=fKv__asZk   metric learning  PDEs  numerical simulation  physical modeling                                                                                                                                                  

 

   most controversial papers by std of scores

  paper_id      title                                                                               link                                          keywords                                                                                                                                                                                        mean     max     min       std     median     num  
                     
  prCmDEN_     Visual hyperacuity with moving sensor and recurrent neural computations             https  openreview net forum id=prCmDEN_     visual system  convolutional neural networks  recurrent neural networks  active vision  active sensing  ocular drift                                                                                                                 
  FPGslUeq   Palette  Image to Image Diffusion Models                                            https  openreview net forum id=FPGslUeq   machine learning  artificial intelligence  computer vision                                                                                                                                                                           
  SCJbEviuD   White Paper Assistance  A Step Forward Beyond the Shortcut Learning                 https  openreview net forum id=SCJbEviuD   Shortcut Learning  Bias  Classification  Imbalanced Classification  Robustness                                                                                                                                                        
  IWGzQgZD   Constructing a Good Behavior Basis for Transfer using Generalized Policy Updates    https  openreview net forum id=IWGzQgZD   reinforcement learning  lifelong learning  transfer learning  successor features                                                                                                                                                       
  JGOCvGS    Universal Approximation Under Constraints is Possible with Transformers             https  openreview net forum id=JGOCvGS    Constrained Universal Approximation  Probabilistic Attention  Transformer Networks  Geometric Deep Learning  Measurable Maximum Theorem  Non Affine Random Projections  Optimal Transport                                              
  ILxkQyElm   Learning Continuous Environment Fields via Implicit Functions                       https  openreview net forum id=ILxkQyElm   Continuous Scene Representation  Implicit Neural Networks                                                                                                                                                                               
  VMBgNBxE    Mask and Understand  Evaluating the Importance of Parameters                        https  openreview net forum id=VMBgNBxE    influence function  interpretability  model pruning  feature importance ranking                                                                                                                                                         
  TQMd FqQp   Efficient and Modular Implicit Differentiation                                      https  openreview net forum id=TQMd FqQp   implicit differentiation  bilevel optimization  autodiff  jax                                                                                                                                                                     
  MeMMmuWRXsy   Robust Robotic Control from Pixels using Contrastive Recurrent State Space Models   https  openreview net forum id=MeMMmuWRXsy   contrastive learning  model based RL  distractions  predictive coding                                                                                                                                                              
  kxARpzoqAk   Information Aware Time Series Meta Contrastive Learning                             https  openreview net forum id=kxARpzoqAk   Information Aware Time Series Meta Contrastive Learning                                                                                                                                                                           ",1636469545.0,2021-11-09 15:52:25,crawled iclr preliminary reviews help another repo uploaded crawled raw data crawled today around pm utc also find quick stats like * distribution mean scores etc * best paper mean median score * controversial paper std scores following notebook [ github com viettralala iclr openreviewdata blob master analyze\_reviews ipynb] github com viettralala iclr openreviewdata blob master analyze_reviews ipynb feel free play around ✌ excerpt data best paper median score paper_id title link keywords mean max min std median num ldlwbbpmlq minibatch vs local sgd shuffling tight convergence bounds beyond openreview net forum id=ldlwbbpmlq local sgd minibatch sgd shuffling without replacement convex optimization stochastic optimization federated learning large scale learning distributed learning imsjopconp mt multi task multitrack music transcription openreview net forum id=imsjopconp music transcription transformer multi task learning low resource learning music understanding music information retrieval brpdxbdzkq demodice offline imitation learning supplementary imperfect demonstrations openreview net forum id=brpdxbdzkq imitation learning offline imitation learning imperfect demonstration non expert demonstration sok zswhb responsible disclosure generative models using scalable fingerprinting openreview net forum id=sok zswhb generative models fingerprinting responsible disclosure deep fake detection attribution bvvmotlmiw diva dataset derivative learning task openreview net forum id=bvvmotlmiw leave one cross validation automl dataset optimization lrocyb st approximation learning deep convolutional models kernel perspective openreview net forum id=lrocyb st kernel methods deep learning theory convolution approximation generalization sictxznve happens sgd reaches zero loss mathematical framework openreview net forum id=sictxznve sgd implicit bias generalization deep learning implicit regularization manifold dlwqqlmqv nas bench suite nas evaluation surprisingly easy openreview net forum id=dlwqqlmqv neural architecture search automl ke_fgfdga multiberts bert reproductions robustness analysis openreview net forum id=ke_fgfdga pre trained models bert bootstrapping hypothesis testing robustness fkv__aszk learning similarity metrics volumetric simulations multiscale cnns openreview net forum id=fkv__aszk metric learning pdes numerical simulation physical modeling controversial papers std scores paper_id title link keywords mean max min std median num prcmden_ visual hyperacuity moving sensor recurrent neural computations openreview net forum id=prcmden_ visual system convolutional neural networks recurrent neural networks active vision active sensing ocular drift fpgslueq palette image image diffusion models openreview net forum id=fpgslueq machine learning artificial intelligence computer vision scjbeviud white paper assistance step forward beyond shortcut learning openreview net forum id=scjbeviud shortcut learning bias classification imbalanced classification robustness iwgzqgzd constructing good behavior basis transfer using generalized policy updates openreview net forum id=iwgzqgzd reinforcement learning lifelong learning transfer learning successor features jgocvgs universal approximation constraints possible transformers openreview net forum id=jgocvgs constrained universal approximation probabilistic attention transformer networks geometric deep learning measurable maximum theorem non affine random projections optimal transport ilxkqyelm learning continuous environment fields via implicit functions openreview net forum id=ilxkqyelm continuous scene representation implicit neural networks vmbgnbxe mask understand evaluating importance parameters openreview net forum id=vmbgnbxe influence function interpretability model pruning feature importance ranking tqmd fqqp efficient modular implicit differentiation openreview net forum id=tqmd fqqp implicit differentiation bilevel optimization autodiff jax memmmuwrxsy robust robotic control pixels using contrastive recurrent state space models openreview net forum id=memmmuwrxsy contrastive learning model based rl distractions predictive coding kxarpzoqak information aware time series meta contrastive learning openreview net forum id=kxarpzoqak information aware time series meta contrastive learning
[P] Community sourced Open Audio Datasets – Hacktoberfest 2021,21,qq5hrq,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qq5hrq/p_community_sourced_open_audio_datasets/,2,"Hey r ML  A month ago  [I posted here] https  www reddit com r MachineLearning comments qt d_supporting_hacktoberfest_for_ml_datasets  about [DagsHub] https  DagsHub com  supporting Hacktoberfest for ML Datasets  We wanted to do something that was geared towards the ML community  and we decided to create an open source catalog of 🔊  audio datasets 

The response has been truly amazing  We received  dataset contributions  which are now publicly available  and viewable on DagsHub  They cover various tasks  languages  and sizes  and you can use them all for your projects 

If you want to check out the list of datasets  [https  dagshub com blog hacktoberfest  open source audio datasets ] https  dagshub com blog hacktoberfest  open source audio datasets   I can t wait to see what everyone builds with these 

A huge **THANK YOU** to everyone who participated  You are what made this possible  The fact that Hacktoberfest is over doesn t mean you can t continue contributing  We d love to see more datasets  both in the audio domain and others ",1636469988.0,2021-11-09 15:59:48,hey r ml month ago [i posted here] www reddit com r machinelearning comments qt d_supporting_hacktoberfest_for_ml_datasets [dagshub] dagshub com supporting hacktoberfest ml datasets wanted something geared towards ml community decided create open source catalog 🔊 audio datasets response truly amazing received dataset contributions publicly available viewable dagshub cover various tasks languages sizes use projects want check list datasets [ dagshub com blog hacktoberfest open source audio datasets ] dagshub com blog hacktoberfest open source audio datasets wait see everyone builds huge **thank you** everyone participated made possible fact hacktoberfest mean continue contributing love see datasets audio domain others
"Alibaba DAMO Academy Creates World’s Largest AI Pre-Training Model, With Parameters Far Exceeding Google and Microsoft (10T parameters) [N]",164,qpuax4,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qpuax4/alibaba_damo_academy_creates_worlds_largest_ai/,35,"  [According to the company  the M has achieved the ultimate low carbon and high efficiency in the industry  using  GPUs to train a usable  trillion model within  days ] https  pandaily com alibaba damo academy creates worlds largest ai pre training model with parameters far exceeding google and microsoft  Compared to the GPT   a large model released last year  M achieves the same parameter scale and consumes only   of its energy 

Thoughts  The pace of foundational models is starting to get scary  seems like a bigger and bigger model is pushed out every week ",1636427107.0,2021-11-09 04:05:07,[according company achieved ultimate low carbon high efficiency industry using gpus train usable trillion model within days ] pandaily com alibaba damo academy creates worlds largest ai pre training model parameters far exceeding google microsoft compared gpt large model released last year achieves parameter scale consumes energy thoughts pace foundational models starting get scary seems like bigger bigger model pushed every week
[D] ICLR 2022 reviews,23,qq2rbm,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qq2rbm/d_iclr_2022_reviews/,46,Share your rants ,1636461419.0,2021-11-09 13:36:59,share rants
[D] Google AutoML's prices,0,qqopdj,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqopdj/d_google_automls_prices/,0,"I m trying to understand Google AutoML s pricing and I have three questions 

  What is  price for forecasting  here  [https  cloud google com vertex ai pricing tabular data] https  cloud google com vertex ai pricing tabular data   
  How much will I pay for having an endpoint available   to which I can post data and execute previously trained model  assuming simple numerical data with classification  
  Can I upload my own model and have it ready for predictions 

Thanks",1636528458.0,2021-11-10 08:14:18,trying understand google automl pricing three questions price forecasting [ cloud google com vertex ai pricing tabular data] cloud google com vertex ai pricing tabular data much pay endpoint available post data execute previously trained model assuming simple numerical data classification upload model ready predictions thanks
[N] AMD launches MI200 AI accelerators (2.5x Nvidia A100 FP32 performance),230,qphg92,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qphg92/n_amd_launches_mi200_ai_accelerators_25x_nvidia/,69,"Source  https  twitter com IanCutress status 

More Info  https  www anandtech com show  amd announces instinct mi accelerator family cdna exacale servers

  For today’s announcement  AMD is revealing  MI series accelerators  These are the top end MIX  it’s smaller sibling the MI  and finally an MI PCIe card  the MI  The two MI parts are the focus of today’s announcement  and for now AMD has not announced the full specifications of the MI ",1636389639.0,2021-11-08 17:40:39,source twitter com iancutress status info www anandtech com show amd announces instinct mi accelerator family cdna exacale servers today’s announcement amd revealing mi series accelerators top end mix it’s smaller sibling mi finally mi pcie card mi two mi parts focus today’s announcement amd announced full specifications mi
[D] improving segmentation masks,8,qq18tc,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qq18tc/d_improving_segmentation_masks/,2,I have a dataset with some segmentation masks for objects   or better polygons around the objects   I am interested in  but the quality is not very good  The polygons around the objects are correct but very rough  a low number of edges with huge chunks of background in there  Is there some algorithmic way to improve those  I tried GrapCut but the performance is not very good  huge chunks of background are still included and stuff like hair is done very poorly ,1636455502.0,2021-11-09 11:58:22,dataset segmentation masks objects better polygons around objects interested quality good polygons around objects correct rough low number edges huge chunks background algorithmic way improve tried grapcut performance good huge chunks background still included stuff like hair done poorly
[R] Intel Optimized Facebook DLRM with 8x speedup (Deep Learning Recommendation Model),0,qqcrbh,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqcrbh/r_intel_optimized_facebook_dlrm_with_8x_speedup/,1,Intel leveraged SigOpt s Hyper Parameter Optimization platform to achieve a software speedup for DLRM  Additionally  Intel leveraged vertical split embedding  LAMB optimization  and parallelizable data loaders ,1636490443.0,2021-11-09 21:40:43,intel leveraged sigopt hyper parameter optimization platform achieve software speedup dlrm additionally intel leveraged vertical split embedding lamb optimization parallelizable data loaders
Reward Prediction for Representation Learning and Reward Shaping,5,qq1ae2,MachineLearning,https://www.scitepress.org/Papers/2021/106402/,1,nan,1636455663.0,2021-11-09 12:01:03,nan
[R] The How and Why of Bayesian Nonparametric Causal Inference,14,qpuqtk,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qpuqtk/r_the_how_and_why_of_bayesian_nonparametric/,0,"A nice summary paper  at the cutting edge of Bayesian causal inference  

Link  https  arxiv org abs  

Abstract   Spurred on by recent successes in causal inference competitions  Bayesian nonparametric  and high dimensional  methods have recently seen increased attention in the causal inference literature  In this paper  we present a comprehensive overview of Bayesian nonparametric applications to causal inference  Our aims are to  i  introduce the fundamental Bayesian nonparametric toolkit   ii  discuss how to determine which tool is most appropriate for a given problem  and  iii  show how to avoid common pitfalls in applying Bayesian nonparametric methods in high dimensional settings  Unlike standard fixed dimensional parametric problems  where outcome modeling alone can sometimes be effective  we argue that most of the time it is necessary to model both the selection and outcome processes  ",1636428613.0,2021-11-09 04:30:13,nice summary paper cutting edge bayesian causal inference link arxiv org abs abstract spurred recent successes causal inference competitions bayesian nonparametric high dimensional methods recently seen increased attention causal inference literature paper present comprehensive overview bayesian nonparametric applications causal inference aims introduce fundamental bayesian nonparametric toolkit ii discuss determine tool appropriate given problem iii show avoid common pitfalls applying bayesian nonparametric methods high dimensional settings unlike standard fixed dimensional parametric problems outcome modeling alone sometimes effective argue time necessary model selection outcome processes
Landing AI gets $57 million series A to build a data centric MLOps platform. [News],70,qpk1tt,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qpk1tt/landing_ai_gets_57_million_series_a_to_build_a/,13,"Are data centric MLOps tools about to take off 

[CNBC] https  www cnbc com    google brain founder andrew ng raises  million for landing ai html 

[TechCrunch] https  techcrunch com    landing ai machine learning operations tools ",1636396790.0,2021-11-08 19:39:50,data centric mlops tools take [cnbc] www cnbc com google brain founder andrew ng raises million landing ai html [techcrunch] techcrunch com landing ai machine learning operations tools
[D] What does having more than three reviews mean on ICLR 22?,11,qpwdps,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qpwdps/d_what_does_having_more_than_three_reviews_mean/,12,ICLR  reviews are in  Hope you guys got good reviews  I noticed that some papers got three reviews  while others got four or five reviews  Why do some papers get more reviews and what does it signify ,1636434371.0,2021-11-09 06:06:11,iclr reviews hope guys got good reviews noticed papers got three reviews others got four five reviews papers get reviews signify
[D] Evaluating the effectiveness of text generation,0,qq644r,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qq644r/d_evaluating_the_effectiveness_of_text_generation/,0,"I m using GPT to generate text based on a Q A dataset  the data is domain specific  based on data scrapped from various internal company sources   The challenge I am facing is that the quality of the output is somewhat subjective 

This makes it hard to improve the model output   I ve easily been able to move beyond outputting gibberish to something which works reasonably well  However  I am not finding it hard to evaluating the effectiveness of minor model changes  e g  temperature  prompt design  tweaks to the dataset  etc   

I m considering  crowd sourcing  input from my colleagues  giving them model output  with various tweaks  and asking them to score the results  However  this has obvious limitations 

So  I was wondering if there are techniques that people have developed that make it easier to fine tune models where the output has a subjective quality ",1636471763.0,2021-11-09 16:29:23,using gpt generate text based q dataset data domain specific based data scrapped various internal company sources challenge facing quality output somewhat subjective makes hard improve model output easily able move beyond outputting gibberish something works reasonably well however finding hard evaluating effectiveness minor model changes e g temperature prompt design tweaks dataset etc considering crowd sourcing input colleagues giving model output various tweaks asking score results however obvious limitations wondering techniques people developed make easier fine tune models output subjective quality
[R] Deep Shallow Fusion for RNN-T Personalization,1,qq4v0g,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qq4v0g/r_deep_shallow_fusion_for_rnnt_personalization/,0,"End to end deep learning models for Speech Recognition can produce highly accurate transcriptions  but they are a lot harder to personalize  This paper from Facebook s AI team walks through some methods that help increase the accuracy of proper nouns and rare words from end to end deep learning models which I found really interesting 

I made a summary of this paper that [you can read here] https  www assemblyai com blog deep shallow fusion for rnn t personalization   

And the link to the original paper from Facebook AI can be found here   [https  arxiv org abs  ] https  arxiv org abs   ",1636468155.0,2021-11-09 15:29:15,end end deep learning models speech recognition produce highly accurate transcriptions lot harder personalize paper facebook ai team walks methods help increase accuracy proper nouns rare words end end deep learning models found really interesting made summary paper [you read here] www assemblyai com blog deep shallow fusion rnn personalization link original paper facebook ai found [ arxiv org abs ] arxiv org abs
[Project] Google MoveNet (Real-Time Pose Estimation) Used To Control Nintendo Punch-Out!!,70,qpenkt,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qpenkt/project_google_movenet_realtime_pose_estimation/,8,"Hey AI fans  I hacked the original Nintendo Punch Out  so that you control it with actual punches  This is a boxing video game that now uses Google s MoveNet  real time pose estimation  to track your movements and detect punches  blocks  and other moves and then sends those commands to the game 

You can check out the full video here with plenty of sweet MoveNet footage  [https  www youtube com watch v=JibJJVNp] https  www youtube com watch v=JibJJVNp 

And play it yourself here  [https  reallifepunchout com] https  reallifepunchout com   


 xB 

https  reddit com link qpenkt video swwtjwsdy player",1636381349.0,2021-11-08 15:22:29,hey ai fans hacked original nintendo punch control actual punches boxing video game uses google movenet real time pose estimation track movements detect punches blocks moves sends commands game check full video plenty sweet movenet footage [ www youtube com watch v=jibjjvnp] www youtube com watch v=jibjjvnp play [ reallifepunchout com] reallifepunchout com xb reddit com link qpenkt video swwtjwsdy player
[R] M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining,6,qpu5m7,MachineLearning,https://arxiv.org/abs/2110.03888,3,nan,1636426619.0,2021-11-09 03:56:59,nan
[Project] JORLDY: OpenSource Reinforcement Learning Framework,108,qp9bra,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qp9bra/project_jorldy_opensource_reinforcement_learning/,14,"Hello WoRLd  We are Reinforcement Learning  RL  engineers at KakaoEnterprise in South Korea  We published an opensource RL framework and named it JORLDY  Join Our Reinforcement Learning framework for Developing Yours   JORLDY is opened for helping RL researchers and students who study RL  The features of JORLDY are as follows 

*   RL Algorithms  [Pytorch] https  pytorch org  and various RL environment are provided
* The algorithms and environments can be run with simple command
* Algorithms and environment can be easily added and customized
* Distributed RL algorithms are provided using [ray] https  github com ray project ray 
* Benchmark of the algorithms is conducted in many RL environment

JORLDY github link  [https  github com kakaoenterprise JORLDY] https  github com kakaoenterprise JORLDY 

As we mentioned  JORLDY is an  open source  RL framework  Accordingly  our team wants to work with many people to develop JORLDY into a better framework  We would be very grateful if you use it widely and give us a lot of comments about JORLDY 

Thank you ",1636360178.0,2021-11-08 09:29:38,hello world reinforcement learning rl engineers kakaoenterprise south korea published opensource rl framework named jorldy join reinforcement learning framework developing jorldy opened helping rl researchers students study rl features jorldy follows * rl algorithms [pytorch] pytorch org various rl environment provided * algorithms environments run simple command * algorithms environment easily added customized * distributed rl algorithms provided using [ray] github com ray project ray * benchmark algorithms conducted many rl environment jorldy github link [ github com kakaoenterprise jorldy] github com kakaoenterprise jorldy mentioned jorldy open source rl framework accordingly team wants work many people develop jorldy better framework would grateful use widely give us lot comments jorldy thank
[D] ML datasets for commercial use,0,qpzewi,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qpzewi/d_ml_datasets_for_commercial_use/,3,"Hi all 

there are a ton of datasets to ML researchers stemming from different areas  But when looking more closely  the vast majority of them have very restrictive licensing  only allowing to be used for research purposes  but not in a commercial environment 

I am now wondering what the strategies are for obtaining high quality datasets for commercial purposes  So let s say I want to build a car object detection model for my company  one of the most well known detection use cases 

I can neither use any of the public datasets  as they do not allow commercial usage  nor can I use any of the pre trained models for this task  as they have been trained on these datasets 

I could now 

* Collect my own data
* Pay crowd annotators to annotate the data
* Buy data

I would be specifically interested in the last point  is there a way to acquire these types of datasets  Is there a market for it 

How are other handling this in their companies ",1636447234.0,2021-11-09 09:40:34,hi ton datasets ml researchers stemming different areas looking closely vast majority restrictive licensing allowing used research purposes commercial environment wondering strategies obtaining high quality datasets commercial purposes let say want build car object detection model company one well known detection use cases neither use public datasets allow commercial usage use pre trained models task trained datasets could * collect data * pay crowd annotators annotate data * buy data would specifically interested last point way acquire types datasets market handling companies
[D] Recursive ML strategies?,3,qpw3br,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qpw3br/d_recursive_ml_strategies/,8,"I m looking for ideas on how to use recursive ML strategies  possibly utilizing multiple individual models where one model uses the output of another model to make more accurate predictions 

For example  I use two sklearn `RandomForestClassifier` models to provide a simple signal about the direction of the stock market  The first takes `n` inputs and outputs a prediction  The second takes the original `n` inputs plus the output of the first to make a new prediction  It doesn t provide earth shattering results  but it appears to be slightly better than only using the one model 

Random forests also provide the ability to use Out of Bag samples  which could also be used 

I m just curious if there any established methods  papers I should look at  etc  that discuss meta or recursive strategies to get the most out of ML models ",1636433341.0,2021-11-09 05:49:01,looking ideas use recursive ml strategies possibly utilizing multiple individual models one model uses output another model make accurate predictions example use two sklearn `randomforestclassifier` models provide simple signal direction stock market first takes `n` inputs outputs prediction second takes original `n` inputs plus output first make new prediction provide earth shattering results appears slightly better using one model random forests also provide ability use bag samples could also used curious established methods papers look etc discuss meta recursive strategies get ml models
[T] Procedural Generalization by Planning with Self-Supervised World Models,2,qpuh3x,MachineLearning,https://arxiv.org/abs/2111.01587,2,nan,1636427689.0,2021-11-09 04:14:49,nan
"[P] HuBERT: How to apply BERT to speech, visually explained",41,qpbci5,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qpbci5/p_hubert_how_to_apply_bert_to_speech_visually/,2,"Recently Facebook AI released HuBERT  a BERT like model for learning powerful speech representations  At first glance  this model looks similar to wavvec    but the training process objective is actually very different 

I made some detailed illustrations to visually explain the pre training process of HuBERT and how it compares to wavvec    Both of these models are already available in the HuggingFace Transformers library 

[https  jonathanbgn com    hubert visually explained html] https  jonathanbgn com    hubert visually explained html 

Hope this is helpful ",1636369620.0,2021-11-08 12:07:00,recently facebook ai released hubert bert like model learning powerful speech representations first glance model looks similar wavvec training process objective actually different made detailed illustrations visually explain pre training process hubert compares wavvec models already available huggingface transformers library [ jonathanbgn com hubert visually explained html] jonathanbgn com hubert visually explained html hope helpful
[P] Open-NSFW 2: TensorFlow 2 implementation of the Yahoo Open-NSFW model,70,qp8897,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qp8897/p_opennsfw_2_tensorflow_2_implementation_of_the/,13,"Detecting Not Suitable For Work  NSFW  images  in particular pornographic images  is a high demand task in computer vision  The Yahoo Open NSFW model originally developed with the Caffe framework has been a favourite choice  but the work is now discontinued and Caffe is also becoming less popular 

This Open NSFW  project provides a TensorFlow  implementation of the Yahoo model  with references to its previous third party TensorFlow  implementation 

Please take a look 

[https  github com bhky opennsfw] https  github com bhky opennsfw ",1636355084.0,2021-11-08 08:04:44,detecting suitable work nsfw images particular pornographic images high demand task computer vision yahoo open nsfw model originally developed caffe framework favourite choice work discontinued caffe also becoming less popular open nsfw project provides tensorflow implementation yahoo model references previous third party tensorflow implementation please take look [ github com bhky opennsfw] github com bhky opennsfw
[D] Commercial Distribution of OpenAI Jukebox Songs,4,qplwld,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qplwld/d_commercial_distribution_of_openai_jukebox_songs/,1,Hello  What is the copyright process for music created by an artificial intelligence  I have made a song using OpenAI s Jukebox and am wondering if I can commercially distribute it in streaming platforms such as Spotify ,1636401921.0,2021-11-08 21:05:21,hello copyright process music created artificial intelligence made song using openai jukebox wondering commercially distribute streaming platforms spotify
[R] Implicit Behavioral Cloning,1,qpljnm,MachineLearning,https://arxiv.org/abs/2109.00137,2,nan,1636400939.0,2021-11-08 20:48:59,nan
[R] Introducing MetaICL: A Language Model Meta-Training Framework for Few-Shot In-Context Learning,3,qpf5t0,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qpf5t0/r_introducing_metaicl_a_language_model/,1,"A research team from the University of Washington  FacebookAI Research and the Allen Institute for AI introduces Meta training for InContext Learning  MetaICL   a new meta training framework for few shot learning where an LM is meta trained to learn in context — conditioning on training examples to recover the task and make predictions 

Here is a quick read  Introducing MetaICL [A Language Model Meta Training Framework for Few Show In Context Learning ] https  syncedreview com    deepmind podracer tpu based rl frameworks deliver exceptional performance at low cost  

The MetaICL code and data will be made available on the project’s [GitHub] https  github com facebookresearch metaicl   The paper *MetaICL  Learning to Learn In Context* is on [arXiv] https  arxiv org abs    ",1636382856.0,2021-11-08 15:47:36,research team university washington facebookai research allen institute ai introduces meta training incontext learning metaicl new meta training framework shot learning lm meta trained learn context — conditioning training examples recover task make predictions quick read introducing metaicl [a language model meta training framework show context learning ] syncedreview com deepmind podracer tpu based rl frameworks deliver exceptional performance low cost metaicl code data made available project’s [github] github com facebookresearch metaicl paper *metaicl learning learn context* [arxiv] arxiv org abs
[D]What is something you took the time to learn that benefitted you the most?,248,qordhq,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qordhq/dwhat_is_something_you_took_the_time_to_learn/,112,Saw a thread in cscareer questiosn and I thought it was a great question that could help a lot of people in machine learning since there is so much to learn in this field and could use some direction ,1636300735.0,2021-11-07 16:58:55,saw thread cscareer questiosn thought great question could help lot people machine learning since much learn field could use direction
[RESEARCH] OpenAI's GPT-3: cases of misusage and failures,3,qpi381,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qpi381/research_openais_gpt3_cases_of_misusage_and/,5,"Hello everyone 
Name s Alex  yo from Italy and currently studying Marketing and A I  at IULM University here in Milan 

I grew quite an interest when finally GPT  came out  and while discussing with one of my professors  the topic of my BD thesis came up  Long story short  I m gonna talk about GPT  

One of the topics I d love to cover  is a collection of known applications  aka Use cases  
While I found quite a decent number of  more or less  succesful cases  I can t find anything 
I also tried on Google  since I only refer to Google Scholar to find reliable sources   but still never managed to find anything 
So here I am  asking if you guys know of any case where a company tried to use GPT  in some ways but the whole thing didn t end up quite as they expected 

Thank you all ",1636391456.0,2021-11-08 18:10:56,hello everyone name alex yo italy currently studying marketing iulm university milan grew quite interest finally gpt came discussing one professors topic bd thesis came long story short gonna talk gpt one topics love cover collection known applications aka use cases found quite decent number less succesful cases find anything also tried google since refer google scholar find reliable sources still never managed find anything asking guys know case company tried use gpt ways whole thing end quite expected thank
[N] Maritime Grand Challenge - Abu Dhabi,0,qplveb,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qplveb/n_maritime_grand_challenge_abu_dhabi/,0,"Came across this Maritime Grand Challenge that I thought others might find interesting  It combines drones  robotics and AI and there’s a  M first prize and  M overall in prize money  Open to universities  research institutions  companies and individual innovators  

Here’s a link  [https  www mbzirc com abudhabi\_aspire\_launches\_mbzirc\_challenge php] https  www mbzirc com abudhabi_aspire_launches_mbzirc_challenge php   

And a video  

[https  www youtube com watch v=AdBJCnzQ] https  www youtube com watch v=AdBJCnzQ 

The competition is to further development of real world solutions to illegal fishing  piracy  smuggling and coastline security   First deadline   initial phase  includes white papers and registration – is Dec     

I found out about it through this story  

[https  www robotics com article mbzirc\_maritime\_grand\_challenge\_m\_prize\_launches\_abu\_dhabi ] https  www robotics com article mbzirc_maritime_grand_challenge_m_prize_launches_abu_dhabi ",1636401821.0,2021-11-08 21:03:41,came across maritime grand challenge thought others might find interesting combines drones robotics ai there’s first prize overall prize money open universities research institutions companies individual innovators here’s link [ www mbzirc com abudhabi\_aspire\_launches\_mbzirc\_challenge php] www mbzirc com abudhabi_aspire_launches_mbzirc_challenge php video [ www youtube com watch v=adbjcnzq] www youtube com watch v=adbjcnzq competition development real world solutions illegal fishing piracy smuggling coastline security first deadline initial phase includes white papers registration – dec found story [ www robotics com article mbzirc\_maritime\_grand\_challenge\_m\_prize\_launches\_abu\_dhabi ] www robotics com article mbzirc_maritime_grand_challenge_m_prize_launches_abu_dhabi
"[R] A Unified View of Relational Deep Learning for Polypharmacy Side Effect, Combination Synergy, and Drug-Drug Interaction Prediction",6,qp9mnn,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qp9mnn/r_a_unified_view_of_relational_deep_learning_for/,1," xB 

https  preview redd it tfyuncy jpg width= format=pjpg auto=webp s=eafbeadaaceddab

**Git ** [https  github com AstraZeneca polypharmacy ddi synergy survey] https  github com AstraZeneca polypharmacy ddi synergy survey 

**Paper ** [https  arxiv org abs  ] https  arxiv org abs   

**Abstract **

In recent years  numerous machine learning models which attempt to solve polypharmacy side effect identification  drug drug interaction prediction  and combination therapy design tasks have been proposed  Here  we present a unified theoretical view of relational machine learning models which can address these tasks  We provide fundamental definitions  compare existing model architectures and discuss performance metrics  datasets  and evaluation protocols  In addition  we emphasize possible high impact applications and important future research directions in this domain 

**The paper provides **

\  A unified model of drug pair scoring models with a general architecture design recipe   
\  Model design comparisons based on architecture and input modalities   
\  Evaluation metrics used by the most important papers   
\  Public datasets that are relevant   
\  Evaluation regime designs for stratified splits

**The Github repo comes with **

\  Paper links with implementations   
\  Links to the datasets ",1636361620.0,2021-11-08 09:53:40,xb preview redd tfyuncy jpg width= format=pjpg auto=webp s=eafbeadaaceddab **git ** [ github com astrazeneca polypharmacy ddi synergy survey] github com astrazeneca polypharmacy ddi synergy survey **paper ** [ arxiv org abs ] arxiv org abs **abstract ** recent years numerous machine learning models attempt solve polypharmacy side effect identification drug drug interaction prediction combination therapy design tasks proposed present unified theoretical view relational machine learning models address tasks provide fundamental definitions compare existing model architectures discuss performance metrics datasets evaluation protocols addition emphasize possible high impact applications important future research directions domain **the paper provides ** \ unified model drug pair scoring models general architecture design recipe \ model design comparisons based architecture input modalities \ evaluation metrics used important papers \ public datasets relevant \ evaluation regime designs stratified splits **the github repo comes ** \ paper links implementations \ links datasets
[D] Intuition for meaning behind magnitude of covariance,0,qphreq,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qphreq/d_intuition_for_meaning_behind_magnitude_of/,4,"Covariance matrices are pretty essential to many ML algorithms and probabilistic models  When two variables have positive covariance  they are correlated  when they have negative covariance  they are inversely correlated and when the covariance is zero  they are not correlated  However  the degree of correlation cannot be read from the magnitude of the covariance value  

My question follows  well  what *can be read* from this magnitude  What does it mean if two variables have a very large covariance value opposed to a small one ",1636390548.0,2021-11-08 17:55:48,covariance matrices pretty essential many ml algorithms probabilistic models two variables positive covariance correlated negative covariance inversely correlated covariance zero correlated however degree correlation cannot read magnitude covariance value question follows well *can read* magnitude mean two variables large covariance value opposed small one
[R] Check the blog that introduces our recent #NeurIPS2021 work on nonconvex stochastic optimization! We propose stochastic Anderson mixing with theoretical guarantees and promising results in training neural networks.,14,qp432r,MachineLearning,https://thumtblog.github.io/2021/10/30/stochastic-anderson-mixing/,2,nan,1636338894.0,2021-11-08 03:34:54,nan
[R] StyleCLIPDraw: Coupling Content and Style in Text-to-Drawing Synthesis,10,qp5pk4,MachineLearning,https://arxiv.org/abs/2111.03133,3,nan,1636344674.0,2021-11-08 05:11:14,nan
[N] State of AI 2021,55,qotk5u,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qotk5u/n_state_of_ai_2021/,18,"Forth edition  [https  www stateof ai ] https  www stateof ai 

If you read it  or skimmed it   what was the most important information in your opinion ",1636306842.0,2021-11-07 18:40:42,forth edition [ www stateof ai ] www stateof ai read skimmed important information opinion
"[R][P] 2021: A Year Full of Amazing AI papers - A Review [work in progress...] A curated list of the latest breakthroughs in AI by release date with a clear video explanation, link to a more in-depth article, and code.",98,qoox33,MachineLearning,https://github.com/louisfb01/best_AI_papers_2021,6,nan,1636293026.0,2021-11-07 14:50:26,nan
[D] New in-depth AI interview episode out! Tristan Zajonc's AI startup just came out of stealth mode already doing incredible work.,4,qp7ooe,MachineLearning,https://youtu.be/i8gIHUbZmwY,0,nan,1636352694.0,2021-11-08 07:24:54,nan
[R] [P] AnimeGANv2 Face Portrait v2,1878,qo4kp8,MachineLearning,https://i.redd.it/k25gkmonb0y71.gif,104,nan,1636218407.0,2021-11-06 18:06:47,nan
[D] Measure the distance between two domains for transfer learning.,27,qoqpv2,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qoqpv2/d_measure_the_distance_between_two_domains_for/,14,"I know there are distances are defined to minimise for the domain adaption 

While I want to know does there exist any distance measurement that can measure the difficulty of performing the domain adaption from the source domain to different target domains？",1636298714.0,2021-11-07 16:25:14,know distances defined minimise domain adaption want know exist distance measurement measure difficulty performing domain adaption source domain different target domains？
[D] What happened to Compressive Transformers?,5,qoyyy7,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qoyyy7/d_what_happened_to_compressive_transformers/,4,They promised to solve one of the Transformer architecuter s greatest weaknesses  it s lack of long term memory  but I can t seem to find any bigger experiment using them  Did they not work out ,1636322519.0,2021-11-07 23:01:59,promised solve one transformer architecuter greatest weaknesses lack long term memory seem find bigger experiment using work
[D] According to google and AWS these are very NSFW... I want it on a shirt!,846,qo1l35,MachineLearning,https://medium.com/@tom_25234/synthetic-abstractions-8f0e8f69f390,72,nan,1636209527.0,2021-11-06 15:38:47,nan
[R] stl file data annotation,4,qoqqp2,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qoqqp2/r_stl_file_data_annotation/,2,"First time using Reddit  

Anyone know a software I can use to annotate a point cloud stl medical image file  I m aware of several softwares for DICOM files but none seem to work with stl  

Ideally free software but willing to pay if good ",1636298795.0,2021-11-07 16:26:35,first time using reddit anyone know software use annotate point cloud stl medical image file aware several softwares dicom files none seem work stl ideally free software willing pay good
[Discussion] MLops tool for image data management and exploration,5,qoqp4w,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qoqp4w/discussion_mlops_tool_for_image_data_management/,7,"Hi there 

 xB 

Large part of the current work of our developers is dedicated to manual work on data  This is done in several stages of the development 

  In the beginning when we get the annotated data we go over it  thousands of images   For segmentation tasks we observe the annotated images to validate the annotation correctness and images relevance for our tasks  Some images may be correctly annotated  but not relevant to our task or use case so we remove them 
  We check statistics of the data to validate that the data isn t biased  For example that we have enough samples from each class  and that the distribution of instances sizes  e g  number of pixels in the segmentation mask  within the classes is reasonable 
  When we have a trained model  we go over images for which the model gives poor results  and try to find similarities to understand weaknesses of the model and find root causes  This often leads to change of the data and repeating stages   and eventually  

These are very time consuming tasks  and we are looking for MLops tools that will make this work more efficient  Optimally  the tool will also take care of other MLops aspects  like experiments orchestration  experiment tracking and data versioning  We can also combine several tools  but using few  one   tools is preferred  Open source or proprietary paid product are both possible 

Any recommendations 

Thanks ",1636298647.0,2021-11-07 16:24:07,hi xb large part current work developers dedicated manual work data done several stages development beginning get annotated data go thousands images segmentation tasks observe annotated images validate annotation correctness images relevance tasks images may correctly annotated relevant task use case remove check statistics data validate data biased example enough samples class distribution instances sizes e g number pixels segmentation mask within classes reasonable trained model go images model gives poor results try find similarities understand weaknesses model find root causes often leads change data repeating stages eventually time consuming tasks looking mlops tools make work efficient optimally tool also take care mlops aspects like experiments orchestration experiment tracking data versioning also combine several tools using one tools preferred open source proprietary paid product possible recommendations thanks
[R] A (rare) real example of a true time series anomaly discovered by an algorithm.,34,qoa1tv,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qoa1tv/r_a_rare_real_example_of_a_true_time_series/,3," In spite of all the academic work on anomaly detection in time series  it is almost impossible to find a ***real*** example of a true anomaly captured in the wild  Here I present such an example 

A group from Texas A M USC has released a very nice large dataset relating to electric grids  Most of the data is *measured*  temp  voltage etc    but some  Solar Zenith Angle etc   is *computed* 

As a sanity check upon downloading the data  I ran the Matrix Profile \[a\]  to look for any anomalies in the data  It found the highly significant anomaly shown in the attached figure 

Can you guess what it is…  spoiler below 

It took me a few seconds  but I guessed it   might be a Leap Year bug in the data generator   and indeed  after I reported it  I found that this was the case 

Moral of the story  Check your data  and  the Matrix Profile is a very useful tool 

More examples of time series anomalies at \[a\] and \[b\]

\[a\] [www cs ucr edu \~eamonn MatrixProfile html] https  www cs ucr edu ~eamonn MatrixProfile html 

\[b\] [https  www cs ucr edu \~eamonn MERLIN\_Long\_version\_for\_website pdf] https  www cs ucr edu ~eamonn MERLIN_Long_version_for_website pdf 

\[c\] [https  github com tamu engineering research Open source power dataset] https  github com tamu engineering research Open source power dataset ",1636235133.0,2021-11-06 22:45:33,spite academic work anomaly detection time series almost impossible find ***real*** example true anomaly captured wild present example group texas usc released nice large dataset relating electric grids data *measured* temp voltage etc solar zenith angle etc *computed* sanity check upon downloading data ran matrix profile \[a\] look anomalies data found highly significant anomaly shown attached figure guess is… spoiler took seconds guessed might leap year bug data generator indeed reported found case moral story check data matrix profile useful tool examples time series anomalies \[a\] \[b\] \[a\] [www cs ucr edu \~eamonn matrixprofile html] www cs ucr edu ~eamonn matrixprofile html \[b\] [ www cs ucr edu \~eamonn merlin\_long\_version\_for\_website pdf] www cs ucr edu ~eamonn merlin_long_version_for_website pdf \[c\] [ github com tamu engineering research open source power dataset] github com tamu engineering research open source power dataset
FLOPS Calculation [D],1,qoptlo,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qoptlo/flops_calculation_d/,6,"Please consider the following table 

 xB 

https  preview redd it zwdlhoy png width= format=png auto=webp s=ebbecbffbbbe

which is from FaceNet publication  What I am trying is to learn FLOPS calculations with the help of this model so I can calculate for the below ones  If I am not wrong  the first row should have  \*\* \* \*\*     FLOPS which is a bit lesser than given value  Likewise the second convolution row conva  \*\* \*  =    which is again near but not exactly the same 

[https \ \ sefiks com\ \ \ \ face recognition with deepid in keras\ ] https  preview redd it cxfoy png width= format=png auto=webp s=bcefffcdffbdfdefacadd 

 xB 

Am I on the right track  What am I doing wrong 

 Also [https  www thinkautonomous ai blog  p=deep learning optimization] https  www thinkautonomous ai blog  p=deep learning optimization  mentions multiplying by  after all those operations which further confuses me  ",1636295924.0,2021-11-07 15:38:44,please consider following table xb preview redd zwdlhoy png width= format=png auto=webp s=ebbecbffbbbe facenet publication trying learn flops calculations help model calculate ones wrong first row \*\* \* \*\* flops bit lesser given value likewise second convolution row conva \*\* \* = near exactly [ \ \ sefiks com\ \ \ \ face recognition deepid keras\ ] preview redd cxfoy png width= format=png auto=webp s=bcefffcdffbdfdefacadd xb right track wrong also [ www thinkautonomous ai blog p=deep learning optimization] www thinkautonomous ai blog p=deep learning optimization mentions multiplying operations confuses
"[Research] Looking for interesting machine learning papers to read over the weekend? Here is a curated list I made for 2020. (with video explanation, short read, paper, and code) - Stay tuned for 2021 at the end of December!",52,qo0f1y,MachineLearning,https://github.com/louisfb01/Best_AI_paper_2020,5,nan,1636205864.0,2021-11-06 14:37:44,nan
[D] GPT-3 is No Longer the Only Game in Town,22,qo5in1,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qo5in1/d_gpt3_is_no_longer_the_only_game_in_town/,9,"Hey there  I just put out a little article that you might find interesting   [GPT  is No Longer the Only Game in Town] https  lastweekin ai p gpt  is no longer the only game   It catalogues the appearance of models akin to GPT  over the course of   like  [HyperCLOVA] https  venturebeat com    naver trained a gpt  like korean language model  and such  Hope you enjoy it 

*TLDR  Organizations face significant challenges in creating a model similar to OpenAI’s GPT   but nevertheless a half dozen or so models as big or bigger than GPT  have been announced over the course  of  *",1636221258.0,2021-11-06 18:54:18,hey put little article might find interesting [gpt longer game town] lastweekin ai p gpt longer game catalogues appearance models akin gpt course like [hyperclova] venturebeat com naver trained gpt like korean language model hope enjoy *tldr organizations face significant challenges creating model similar openai’s gpt nevertheless half dozen models big bigger gpt announced course *
Comparing deep models with different complexity and different accuracies [D],0,qojz5g,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qojz5g/comparing_deep_models_with_different_complexity/,3," I am working with a deep learning based system where complexity has to be reduced to a minimal  However  this is having some impact on the accuracy  I am having to ask the question in somewhat hypothetical scenario and hope that I am clear in my statement 

Say one system uses  billion FLOPS network to achieve   accuracy yet another uses   billion FLOPS network to give    On equal accuracy we could say that the system with lower flops is better  likewise  on equal FLOPS we would have been able to say that the first system is better  Is there a direct way like a standard KPI to compare these two systems in the given scenario 

In short  can we compare two systems with different accuracies and different FLOPS ",1636271893.0,2021-11-07 08:58:13,working deep learning based system complexity reduced minimal however impact accuracy ask question somewhat hypothetical scenario hope clear statement say one system uses billion flops network achieve accuracy yet another uses billion flops network give equal accuracy could say system lower flops better likewise equal flops would able say first system better direct way like standard kpi compare two systems given scenario short compare two systems different accuracies different flops
[R] Unsupervised Learning of Compositional Energy Concepts,11,qo4set,MachineLearning,https://arxiv.org/abs/2111.03042,3,nan,1636219041.0,2021-11-06 18:17:21,nan
"[P] League of Legends Patch 11.21 Game Playing AI (Reinforcement Learning, Supervised Learning) Dataset",182,qnktqk,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qnktqk/p_league_of_legends_patch_1121_game_playing_ai/,36,"This dataset is meant for anyone who would like to try to create a deep learning agent either using supervised or  offline  reinforcement learning to play League of Legends  The dataset contains  games from patch    last patch  where the game ended in an early surrender  These games were chosen as the game lengths were guaranteed to be low which kept the dataset from being too large 
To download the dataset  go to [this] https  github com MiscellaneousStuff tlol  GitHub link and click on the `Google Drive Link`  The dataset is stored as an SQLite database file and the schema should be relatively self explanatory  Happy to answer any questions 

This is just a preliminary dataset which demonstrates that this is possible  Within the next few days the dataset will contain s of replays which means  s of champions worth of data  for each time a player plays a champion  

Edit  Database now contains all  early surrender games  games ending at or before   minutes  in the dataset 
This table shows the top  champion occurrences within the dataset 

  Champion       No   
         
  Nami             
  Miss Fortune     
  Lucian            
  Khazix            
  Viego             
  Lux               
  Jhin              
  Yone              
  Camille           
  Graves            

Edit   Larger dataset containing  games targeting Miss Fortune in the early game  up to first  minutes  with the same schema and format as the first dataset  Also contains all game objects recorded  times a second  The games were chosen by getting the games where the MF player lived the longest  This gave a dataset where the players overall had a    win rate in roughly EUW Diamond II 

Edit   A further  games also targeting Miss Fortune in the early game  up to first  minutes  with the same schema and format as the first and second dataset  This brings the total number of games for the MF Longevity datasets to   or
`  games *   minutes *  seconds *  frames second   =    frames in total `
This should now be enough to at least create a deep learning agent which can play Miss Fortune for the first five minutes of a game at least to a basic level 

Edit   Another day another dataset  A further  games from the `MFLongevity` dataset have been uploaded  I have now also included a Jupyter Notebook to analyse the data from the ` EarlyFF` dataset which works completely standalone from Google Colab  Feel free to also run it locally if you wish to 

[GitHub Link] https  github com MiscellaneousStuff tlol 

[ [Open Notebook In Colab] https  colab research google com assets colab badge svg ] https  colab research google com github MiscellaneousStuff tlol blob main League_of_Legends_Patch___ Reinforcement_Learning  ipynb ",1636146868.0,2021-11-05 22:14:28,dataset meant anyone would like try create deep learning agent either using supervised offline reinforcement learning play league legends dataset contains games patch last patch game ended early surrender games chosen game lengths guaranteed low kept dataset large download dataset go [this] github com miscellaneousstuff tlol github link click `google drive link` dataset stored sqlite database file schema relatively self explanatory happy answer questions preliminary dataset demonstrates possible within next days dataset contain replays means champions worth data time player plays champion edit database contains early surrender games games ending minutes dataset table shows top champion occurrences within dataset champion nami miss fortune lucian khazix viego lux jhin yone camille graves edit larger dataset containing games targeting miss fortune early game first minutes schema format first dataset also contains game objects recorded times second games chosen getting games mf player lived longest gave dataset players overall win rate roughly euw diamond ii edit games also targeting miss fortune early game first minutes schema format first second dataset brings total number games mf longevity datasets ` games * minutes * seconds * frames second = frames total ` enough least create deep learning agent play miss fortune first five minutes game least basic level edit another day another dataset games `mflongevity` dataset uploaded also included jupyter notebook analyse data ` earlyff` dataset works completely standalone google colab feel free also run locally wish [github link] github com miscellaneousstuff tlol [ [open notebook colab] colab research google com assets colab badge svg ] colab research google com github miscellaneousstuff tlol blob main league_of_legends_patch___ reinforcement_learning ipynb
[P] Is it necessary to manually label desired objects in each image for object detection?,3,qo8wvt,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qo8wvt/p_is_it_necessary_to_manually_label_desired/,5,"For example i have a dataset of thousands of images of  different objects and each object images already stored in different folders and i train my model on it  

Now i give an image as input which contains all  of the objects and i want my model to detect all  of these objects in the image and draw bounding boxes around them 

To be able to do this  when preparing my dataset do i have to label desired object in each image by drawing boxes around them and then train my model on it 

Is there any better way to do this than manually labelling data of thousands of images ",1636231637.0,2021-11-06 21:47:17,example dataset thousands images different objects object images already stored different folders train model give image input contains objects want model detect objects image draw bounding boxes around able preparing dataset label desired object image drawing boxes around train model better way manually labelling data thousands images
[D] Google MUM details?,8,qo03fo,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qo03fo/d_google_mum_details/,5,"Google has [anounced] https  www google com amp s blog google products search introducing mum amp    MUM several months ago  In the blog post it says that it uses T framework and is multimodal but that is about it  
The name Multitask Unified Model does not help a lot 

Does anybody know how it is trained and how it combines text and images  The only thing i can think of is to train it to generate image captions and then finetune it on multiple tasks  something like bigscience Tpp ",1636204803.0,2021-11-06 14:20:03,google [anounced] www google com amp blog google products search introducing mum amp mum several months ago blog post says uses framework multimodal name multitask unified model help lot anybody know trained combines text images thing think train generate image captions finetune multiple tasks something like bigscience tpp
"[D] ""Real-World Challenges for AGI"" by DeepMind",23,qnu4bg,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qnu4bg/d_realworld_challenges_for_agi_by_deepmind/,4,Either  I did not understand the article  [https  deepmind com blog article real world challenges for agi] https  deepmind com blog article real world challenges for agi  or they just inserted the letters A  G  and I randomly in between their current progress with weather prediction and plasma control for fusion ,1636178538.0,2021-11-06 07:02:18,either understand article [ deepmind com blog article real world challenges agi] deepmind com blog article real world challenges agi inserted letters g randomly current progress weather prediction plasma control fusion
[R] Is CVPR really only for computer vision?,7,qnxu7h,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qnxu7h/r_is_cvpr_really_only_for_computer_vision/,9,"I know [CVPR] https  cvpr thecvf com  literally means  Computer Vision and Pattern Recognition   but does it really means that any submission in the ML field with no direct link with Computer vision will be discarded   
Thanks ",1636195879.0,2021-11-06 11:51:19,know [cvpr] cvpr thecvf com literally means computer vision pattern recognition really means submission ml field direct link computer vision discarded thanks
GPTSD - Transfer of trauma from human to machine back to human via machine learning models [P],3,qo1sdh,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qo1sdh/gptsd_transfer_of_trauma_from_human_to_machine/,1," xB 

https  preview redd it tfftlnzx jpg width= format=pjpg auto=webp s=cabcfcea

GPTSD is a series of images and text created with GPT which explores the transfer of trauma from human to machine back to human via machine learning models  By converting human portraits to text  GPT is able to recreate new text base portraits and dream recollections based off the dream diary of a Vietnam war veteren  [https  www hicetnunc xyz gptsd] https  www hicetnunc xyz gptsd ",1636210107.0,2021-11-06 15:48:27,xb preview redd tfftlnzx jpg width= format=pjpg auto=webp s=cabcfcea gptsd series images text created gpt explores transfer trauma human machine back human via machine learning models converting human portraits text gpt able recreate new text base portraits dream recollections based dream diary vietnam war veteren [ www hicetnunc xyz gptsd] www hicetnunc xyz gptsd
[D] Help: Choosing right data/file format for Storing Text Data,4,qnznd4,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qnznd4/d_help_choosing_right_datafile_format_for_storing/,5,"Hello  I m working on a NLP project which requires me to collect the data  As of now I have collected data and stored in to individual text file one article into a single text file  

So for future analysis I have cleaned the data and converted into dict  article id and article text  and now I want to store this data into some specific file formate such as Json  hd etc  

So I want your recommendation on this  which is the best format to store this kind of data  

Please provide suggestions by keeping in mind the latency in loading data from drive  or faster the better  ",1636203320.0,2021-11-06 13:55:20,hello working nlp project requires collect data collected data stored individual text file one article single text file future analysis cleaned data converted dict article id article text want store data specific file formate json hd etc want recommendation best format store kind data please provide suggestions keeping mind latency loading data drive faster better
[P] Problems with a Neural Networks's Output's Order of Magnitude,1,qo5fhz,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qo5fhz/p_problems_with_a_neural_networkss_outputs_order/,12," 

I m working on a project where I m trying to train an airplane to control itself in a D setting 

The neural network that acts as the airplane s pilot has  outputs that are related to the control of the airplane s altitude  angle of attack and throttle  for those that are familiar  

Unfortunately  my network is outputing numbers that have a completely wrong order of magnitude  I tried writing my own activation function to limit their values  but I m simply getting the max or min allowable value  since my actual output is off the scale  

Is there any way to control the order of magnitude of the outputs  I ve tried a ton of things  from intitializing weights to be extremely small  to normalization of the input ",1636220978.0,2021-11-06 18:49:38,working project trying train airplane control setting neural network acts airplane pilot outputs related control airplane altitude angle attack throttle familiar unfortunately network outputing numbers completely wrong order magnitude tried writing activation function limit values simply getting max min allowable value since actual output scale way control order magnitude outputs tried ton things intitializing weights extremely small normalization input
"[N] TF/Keras in Hugging Face, Datasets Edition",64,qnhf5f,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qnhf5f/n_tfkeras_in_hugging_face_datasets_edition/,8,"Hi all  Tensorflow maintainer at Hugging Face here  I [posted here a few months ago] https  www reddit com r MachineLearning comments okv n_tf_keras_and_transformers  about the big change we were making to the library to make everything Keras native  and people seemed to like it  so I thought I d give another update on what s changed since then  We ve made a couple of big changes that reduce the amount of duplicate  boilerplate code in common scripts massively  and we d love to get people using the new approaches and get feedback 

**What happened in last week s episode of Hugging Face **

The story up until now is that all our models are now Keras models  You can still write your own training loop or use the models as a layer in a larger model  everything like that remains unchanged  but it s incredibly convenient to just load a model  then just immediately `compile ` and `fit ` it  I gave some examples in the post I linked above 

** Last week  usually refers to times less than four months ago **

You aren t telling me anything that isn t already in my performance reviews  don t worry 

**At least you delivered eventually  What s new **

So the first big new change is a really nice integration with 🤗 Datasets  If you re unfamiliar  Datasets is the data equivalent to Hugging Face s model hub   you just load any uploaded dataset in one line of code the same way you load a pretrained model  with the `load_dataset ` function  

It s not just for NLP   people are using Transformers for audio and vision and everything else these days  so there s [all kinds of data] https  huggingface co datasets  in there  You should check it out 

To see an example of `load_dataset ` in action  a standard workflow with Datasets and Transformers goes something like this 

    from transformers import AutoTokenizer  TFAutoModelForSequenceClassification
    from datasets import load_dataset
    
      Load a pretrained model and its tokenizer
    model_name =  bert base cased 
    tokenizer = AutoTokenizer from_pretrained model_name 
    model = TFAutoModelForSequenceClassification from_pretrained model_name 
    
      Load a dataset   we ll use the COLA dataset from the GLUE benchmark
    data = load_dataset glue    cola 
    
      Define a function to tokenize the data  then apply it to the dataset 
      The tokenizer returns a dict  and map  will add keys from that dict
      to the dataset as columns
    def tokenize_function dataset  
        return tokenizer dataset[ sentence ] 
    
    tokenized_dataset = data map tokenize_function 

So far  so good  but this is the point where problems start to arise  because it s really hard to get the tokenized data into your model  The data is often   quite large and   jagged  because different samples will tokenize to arrays of different lengths  As a result  if you want to load the data as a single dict of `np ndarray` or `tf Tensor`  you end up having to do huge amounts of padding  which bloats memory usage and massively slows down the model  The way to get good performance is to load random batches of samples and only pad that batch  not the entire dataset  but doing that basically required you to write a custom training loop  or at the very least a Python generator  before it would work with Keras  

If anyone was using Transformers with TF before now I d love to hear how you were solving this  because it was a huge recurring pain for me 

**So is there a solution now **

There is   The solution is that we added the method `to_tf_dataset ` to all our datasets  This basically wraps the dataset in a `tf data Dataset`  which will do the just in time data padding you want  We ve also updated our `DataCollator` classes to work with this  so you can generate your dataset like so 

    from transformers import DataCollatorWithPadding
    
    data_collator = DataCollatorWithPadding tokenizer=tokenizer  return_tensors= tf 
    
    tf_dataset = tokenized_dataset[ train ] to_tf_dataset 
        columns=[ input_ids    attention_mask    labels ] 
        batch_size= 
        shuffle=True 
        collate_fn=data_collator
     

Note how the data collator needs your model s `tokenizer` \  that s because every **god damned** research group in every **god damned** university in every **god damned** country handles their data in a slightly different way  and so there s no universal approach to padding that works for all of the hundreds of different models out there   We do guarantee  though  that the `tokenizer` that comes with a given `model` will have a `pad ` method that works for that model  and that s what the `data_collator` will use  You have no idea how much pain you re being saved with that method 

**Okay calm down  what do I do with this tf\_dataset **

That bit s easy  A lot of people aren t that familiar with `tf data`  but it s actually really cool  Once you have a `tf data Dataset`  you can pass it straight to `model fit ` or just iterate over it in a for loop to get batches 

**Won t I need to compile this model before I can fit  it  What loss should I use **

That s a great question  And that brings me to the second big change we ve made  Our models now **automatically compute losses that are suitable for their task in a way that s accessible to Keras ** In other words  if you use `TFAutoModelForSequenceClassification`  that model will now compute a loss appropriate for sequence classification tasks  i e  crossentropy  for you  Don t know what loss you need to train GPT  with  No problem   `TFAutoModelForCausalLM from_pretrained gpt ` will do it for you 

**Wait  stop  I m an advanced user and I want my loss  not your loss **

Don t panic  You can still use whatever loss you want  and all old code will work exactly as it did before  The only change is that if you `compile ` your model without a loss  it ll interpret that as you wanting the default internal loss  If you specify a loss argument to `compile `  then it ll use that and not the internal loss  In addition  this only applies when using the Keras API  like `fit `  If you re writing manual training loops or using the model as a layer in a larger model  none of this is relevant to you  This is just a convenience  and it s easy to disable 

**So I just  skip the loss argument **

Exactly  If we continue on the code samples from above  all you need to do is 

    from tensorflow keras optimizers import Adam
    optimizer = Adam e      Transformers work much better with lower LRs
    
    model compile optimizer=optimizer     No loss argument 
    
    model fit tf_dataset 

And that s it  Dataset loaded  tokenized and trained on  With changes to a few lines almost any NLP task from translation to token classification or summarization can be handled in a similar way 

If you want to see more  we have [a bunch of example notebooks in both TensorFlow and PyTorch] https  huggingface co transformers notebooks html   and all the TF examples should be up to date with these new methods ",1636136930.0,2021-11-05 19:28:50,hi tensorflow maintainer hugging face [posted months ago] www reddit com r machinelearning comments okv n_tf_keras_and_transformers big change making library make everything keras native people seemed like thought give another update changed since made couple big changes reduce amount duplicate boilerplate code common scripts massively love get people using new approaches get feedback **what happened last week episode hugging face ** story models keras models still write training loop use models layer larger model everything like remains unchanged incredibly convenient load model immediately `compile ` `fit ` gave examples post linked ** last week usually refers times less four months ago ** telling anything already performance reviews worry **at least delivered eventually new ** first big new change really nice integration 🤗 datasets unfamiliar datasets data equivalent hugging face model hub load uploaded dataset one line code way load pretrained model `load_dataset ` function nlp people using transformers audio vision everything else days [all kinds data] huggingface co datasets check see example `load_dataset ` action standard workflow datasets transformers goes something like transformers import autotokenizer tfautomodelforsequenceclassification datasets import load_dataset load pretrained model tokenizer model_name = bert base cased tokenizer = autotokenizer from_pretrained model_name model = tfautomodelforsequenceclassification from_pretrained model_name load dataset use cola dataset glue benchmark data = load_dataset glue cola define function tokenize data apply dataset tokenizer returns dict map add keys dict dataset columns def tokenize_function dataset return tokenizer dataset[ sentence ] tokenized_dataset = data map tokenize_function far good point problems start arise really hard get tokenized data model data often quite large jagged different samples tokenize arrays different lengths result want load data single dict `np ndarray` `tf tensor` end huge amounts padding bloats memory usage massively slows model way get good performance load random batches samples pad batch entire dataset basically required write custom training loop least python generator would work keras anyone using transformers tf love hear solving huge recurring pain **so solution ** solution added method `to_tf_dataset ` datasets basically wraps dataset `tf data dataset` time data padding want also updated `datacollator` classes work generate dataset like transformers import datacollatorwithpadding data_collator = datacollatorwithpadding tokenizer=tokenizer return_tensors= tf tf_dataset = tokenized_dataset[ train ] to_tf_dataset columns=[ input_ids attention_mask labels ] batch_size= shuffle=true collate_fn=data_collator note data collator needs model `tokenizer` \ every **god damned** research group every **god damned** university every **god damned** country handles data slightly different way universal approach padding works hundreds different models guarantee though `tokenizer` comes given `model` `pad ` method works model `data_collator` use idea much pain saved method **okay calm tf\_dataset ** bit easy lot people familiar `tf data` actually really cool `tf data dataset` pass straight `model fit ` iterate loop get batches **won need compile model fit loss use ** great question brings second big change made models **automatically compute losses suitable task way accessible keras ** words use `tfautomodelforsequenceclassification` model compute loss appropriate sequence classification tasks e crossentropy know loss need train gpt problem `tfautomodelforcausallm from_pretrained gpt ` **wait stop advanced user want loss loss ** panic still use whatever loss want old code work exactly change `compile ` model without loss interpret wanting default internal loss specify loss argument `compile ` use internal loss addition applies using keras api like `fit ` writing manual training loops using model layer larger model none relevant convenience easy disable **so skip loss argument ** exactly continue code samples need tensorflow keras optimizers import adam optimizer = adam e transformers work much better lower lrs model compile optimizer=optimizer loss argument model fit tf_dataset dataset loaded tokenized trained changes lines almost nlp task translation token classification summarization handled similar way want see [a bunch example notebooks tensorflow pytorch] huggingface co transformers notebooks html tf examples date new methods
[R] Arch-Net: A Family Of Neural Networks Built With Operators To Bridge The Gap Between Computer Architecture of ASIC Chips And Neural Network Model Architectures,9,qntbjh,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qntbjh/r_archnet_a_family_of_neural_networks_built_with/,1,"**Key Takeaways**

* As it turns out  the Arch Net is actually building a bridge that translates between computer architectures of ASIC chips and neural network model architectures by changing existing floating point DNNs into hardware friendly quantized Arch Net 
* The structure of ArchNet is made up of five operators  × Convolutions  Batch Normalization  Concatenation  × Max pooling  and Fully Connected layers 
* The conversion to Arch Net is much simpler without labeled data as researchers employ Blockwise Model Distillation on feature maps 
* Researchers did extensive experiments on image classification and machine translation tasks to confirm that Arch Net is both effective  efficient and fast 

  [Paper] https  arxiv org pdf  v pdf    [Github] https  github com megvii research Arch Net 

https  preview redd it eqbzvmrwx png width= format=png auto=webp s=cddbabfafaca",1636175091.0,2021-11-06 06:04:51,**key takeaways** * turns arch net actually building bridge translates computer architectures asic chips neural network model architectures changing existing floating point dnns hardware friendly quantized arch net * structure archnet made five operators × convolutions batch normalization concatenation × max pooling fully connected layers * conversion arch net much simpler without labeled data researchers employ blockwise model distillation feature maps * researchers extensive experiments image classification machine translation tasks confirm arch net effective efficient fast [paper] arxiv org pdf v pdf [github] github com megvii research arch net preview redd eqbzvmrwx png width= format=png auto=webp s=cddbabfafaca
[D] Professors and research groups in Neural program synthesis,2,qo3704,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qo3704/d_professors_and_research_groups_in_neural/,5,"I want to collect a list of professors and research groups that work in neural program synthesis or program induction 
All you can find with a simple search is groups at Microsoft  Google Brain  ETH Zurich and MIT 
Does any one know other groups that work in this topic especially outside USA 

PS  I couldn t find any groups or professors in Germany
If you have any helpful tips how to do PhD in this topic would be nice",1636214319.0,2021-11-06 16:58:39,want collect list professors research groups work neural program synthesis program induction find simple search groups microsoft google brain eth zurich mit one know groups work topic especially outside usa ps find groups professors germany helpful tips phd topic would nice
[R] Adversarial Intrinsic Motivation for Reinforcement Learning,3,qnwqed,MachineLearning,https://arxiv.org/abs/2105.13345,2,nan,1636190576.0,2021-11-06 10:22:56,nan
[P] optimization of Hugging Face Transformer models to get Inference < 1 Millisecond Latency + deployment on production ready inference server,184,qn8com,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qn8com/p_optimization_of_hugging_face_transformer_models/,32,"Hi 

I just released a project showing how to optimize big NLP models and deploy them on Nvidia Triton inference server 

source code  [https  github com ELS RD triton\_transformers] https  github com ELS RD triton_transformers 

project description   [https  towardsdatascience com hugging face transformer inference under  millisecond latency ebeac source=friends\_link sk=cdeccfbb] https  towardsdatascience com hugging face transformer inference under  millisecond latency ebeac source=friends_link sk=cdeccfbb 

Please note that it is for **real life large scale NLP model deployment**  It s only based on open source softwares  It s using tools not very often discussed in usual NLP tutorial 

Performance have been benchmarked and compared with recent Hugging Face Infinity inference server  commercial product   K  for a single model deployed on a single machine  

Our open source inference server with carefully optimized models get better latency times that the commercial product in both scenarios they have shown during the demo  GPU based  

Don t hesitate if you have any question ",1636108867.0,2021-11-05 11:41:07,hi released project showing optimize big nlp models deploy nvidia triton inference server source code [ github com els rd triton\_transformers] github com els rd triton_transformers project description [ towardsdatascience com hugging face transformer inference millisecond latency ebeac source=friends\_link sk=cdeccfbb] towardsdatascience com hugging face transformer inference millisecond latency ebeac source=friends_link sk=cdeccfbb please note **real life large scale nlp model deployment** based open source softwares using tools often discussed usual nlp tutorial performance benchmarked compared recent hugging face infinity inference server commercial product k single model deployed single machine open source inference server carefully optimized models get better latency times commercial product scenarios shown demo gpu based hesitate question
"[P] Open-source project to collect data from multiple databases, apps, SaaS tools and prepare for ML tasks",4,qnq3m4,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qnq3m4/p_opensource_project_to_collect_data_from/,2,"We have lots of business data scattered across different databases and apps  [Rudderstack] https  github com rudderlabs rudder server  can integrate data from various sources and then activate this data in your warehouse or business tools for ML operations  This unlocks the potential applications of the data which was hard to do before Rudderstack e g  UX personalization  business analytics  etc 

I m seeking feedback  what can I improve and how would you want to use it  AMA",1636163429.0,2021-11-06 02:50:29,lots business data scattered across different databases apps [rudderstack] github com rudderlabs rudder server integrate data various sources activate data warehouse business tools ml operations unlocks potential applications data hard rudderstack e g ux personalization business analytics etc seeking feedback improve would want use ama
[R] RLiable: Better Evaluation for Reinforcement Learning—A Visual Explanation,13,qnevvp,MachineLearning,https://araffin.github.io/post/rliable/,0,nan,1636129838.0,2021-11-05 17:30:38,nan
"[R] Google & UC Berkeley’s Data-Driven Offline Optimization Approach Significantly Boosts Hardware Accelerator Performance, Reduces Simulation Time by More Than 90%",14,qnchpo,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qnchpo/r_google_uc_berkeleys_datadriven_offline/,0,"A research team from Google Research and UC Berkeley proposes PRIME  an offline data driven approach that can architect hardware accelerators without any form of simulations  Compared to state of the art simulation driven methods  PRIME achieves impressive performance improvements of up to  × while reducing the total required simulation time by up to  percent  

Here is a quick read  [Google   UC Berkeley’s Data Driven Offline Optimization Approach Significantly Boosts Hardware Accelerator Performance  Reduces Simulation Time by More Than   ] https  syncedreview com    deepmind podracer tpu based rl frameworks deliver exceptional performance at low cost  

The paper *Data Driven Offline Optimization for Architecting Hardware Accelerators* is on [arXiv] https  arxiv org abs    ",1636122952.0,2021-11-05 15:35:52,research team google research uc berkeley proposes prime offline data driven approach architect hardware accelerators without form simulations compared state art simulation driven methods prime achieves impressive performance improvements × reducing total required simulation time percent quick read [google uc berkeley’s data driven offline optimization approach significantly boosts hardware accelerator performance reduces simulation time ] syncedreview com deepmind podracer tpu based rl frameworks deliver exceptional performance low cost paper *data driven offline optimization architecting hardware accelerators* [arxiv] arxiv org abs
[D] How do you actually serve your models in production?,8,qnfjdc,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qnfjdc/d_how_do_you_actually_serve_your_models_in/,14,"Do you use some sort of a built in tool with the technology stack that you are using  Amazon or Azure for example  

Do you use other tools to create a Docker image for inference like mlflow or azureml 

Do you DIY it and make your own image and your own service microservice ",1636131653.0,2021-11-05 18:00:53,use sort built tool technology stack using amazon azure example use tools create docker image inference like mlflow azureml diy make image service microservice
ruDALL-E model is open-source [P],75,qmzy8a,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmzy8a/rudalle_model_is_opensource_p/,4,"Sberbank submitted an open source ruDALL E model  inspired by OpenAI s DALL·E  for Russian 

The model with   billion parameters is available under Apache   license  The pipeline includes image generation  ranging results with ruCLIP and super resolution 

The large model   billion parameters  will be available in the cloud  ruDALL E is the biggest neural network project in the history of Russia  taking more than   GPU days of Nvidia V to train it 

Github  [https  github com sberbank ai ru dalle] https  github com sberbank ai ru dalle 

Model  [https  huggingface co sberbank ai rudalle Malevich] https  huggingface co sberbank ai rudalle Malevich 

Demo  Russian   [https  rudalle ru ] https  rudalle ru 

 xB 

Here are some pictures generated with it   cherry pick by authors 

[Avocado in the style of Malevich] https  preview redd it uzwimcshox jpg width= format=pjpg auto=webp s=ccfbabbdcefdad 

[Cat looks at food] https  preview redd it cshox jpg width= format=pjpg auto=webp s=efcfeaefebedd 

[Anime chan] https  preview redd it wngfcshox jpg width= format=pjpg auto=webp s=cacaecfecaeceec 

[Trump hides the pain] https  preview redd it zivbitcshox jpg width= format=pjpg auto=webp s=dbabbdceefdf 

[Mystery forest] https  preview redd it nlmcshox jpg width= format=pjpg auto=webp s=caaafaedafeefcfee 

[Salvador Dali picture] https  preview redd it paagcshox jpg width= format=pjpg auto=webp s=cefedacfeebba 

[Beautiful chan] https  preview redd it olmhncshox jpg width= format=pjpg auto=webp s=bcbaabddfdfddaf 

[Pepe frog] https  preview redd it gnlqicshox jpg width= format=pjpg auto=webp s=ebacbfefcedafbf 

[Grand Canyon] https  preview redd it sgqnncshox jpg width= format=pjpg auto=webp s=eddddfaddddadafeeeda ",1636075281.0,2021-11-05 02:21:21,sberbank submitted open source rudall e model inspired openai dall·e russian model billion parameters available apache license pipeline includes image generation ranging results ruclip super resolution large model billion parameters available cloud rudall e biggest neural network project history russia taking gpu days nvidia v train github [ github com sberbank ai ru dalle] github com sberbank ai ru dalle model [ huggingface co sberbank ai rudalle malevich] huggingface co sberbank ai rudalle malevich demo russian [ rudalle ru ] rudalle ru xb pictures generated cherry pick authors [avocado style malevich] preview redd uzwimcshox jpg width= format=pjpg auto=webp s=ccfbabbdcefdad [cat looks food] preview redd cshox jpg width= format=pjpg auto=webp s=efcfeaefebedd [anime chan] preview redd wngfcshox jpg width= format=pjpg auto=webp s=cacaecfecaeceec [trump hides pain] preview redd zivbitcshox jpg width= format=pjpg auto=webp s=dbabbdceefdf [mystery forest] preview redd nlmcshox jpg width= format=pjpg auto=webp s=caaafaedafeefcfee [salvador dali picture] preview redd paagcshox jpg width= format=pjpg auto=webp s=cefedacfeebba [beautiful chan] preview redd olmhncshox jpg width= format=pjpg auto=webp s=bcbaabddfdfddaf [pepe frog] preview redd gnlqicshox jpg width= format=pjpg auto=webp s=ebacbfefcedafbf [grand canyon] preview redd sgqnncshox jpg width= format=pjpg auto=webp s=eddddfaddddadafeeeda
"[N] Isomorphic Labs just unveiled today, a new Alphabet company led by DeepMind's Demis Hassabis. Plans to tackle drug discovery using AI.",234,qmrglg,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmrglg/n_isomorphic_labs_just_unveiled_today_a_new/,57,"Even as an insider  I found the idea of a DeepMind offshoot pretty surprising   curious what you folks think about it  What are the odds it ll succeed  Will Alphafold  even be useful for drug discovery   


Tweet unveiling the company  [https  twitter com demishassabis status  s=] https  twitter com demishassabis status  s=   


Website  [https  www isomorphiclabs com blog] https  www isomorphiclabs com blog ",1636051415.0,2021-11-04 19:43:35,even insider found idea deepmind offshoot pretty surprising curious folks think odds succeed alphafold even useful drug discovery tweet unveiling company [ twitter com demishassabis status s=] twitter com demishassabis status s= website [ www isomorphiclabs com blog] www isomorphiclabs com blog
[D] CVPR: Policy for posting on arXiv,2,qnhn4s,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qnhn4s/d_cvpr_policy_for_posting_on_arxiv/,1,"I am planning on submitting a paper to CVPR   and I have some questions regarding the process  I am finding the information on the site a bit confusing 

What time frame are we allowed to post preprint versions of our papers to arXiv  Are there certain considerations we need to know about  Thanks in advance ",1636137542.0,2021-11-05 19:39:02,planning submitting paper cvpr questions regarding process finding information site bit confusing time frame allowed post preprint versions papers arxiv certain considerations need know thanks advance
[D] How do you structure your CV?,1,qniktz,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qniktz/d_how_do_you_structure_your_cv/,7,"After I made my first CV I ve been working on the same company for about two years and worked in multiple projects  Now I m interested in looking out for a bit and reworking my CV and don t really know in what way should I display my projects and skills 

Any Senior AI related Engineers want to share your CVs or suggestions ",1636140262.0,2021-11-05 20:24:22,made first cv working company two years worked multiple projects interested looking bit reworking cv really know way display projects skills senior ai related engineers want share cvs suggestions
[R] No One Representation to Rule Them All: Overlapping Features of Training Methods,17,qn2qbx,MachineLearning,https://arxiv.org/abs/2110.12899,4,nan,1636084592.0,2021-11-05 04:56:32,nan
[R] Hierarchical Transformers Are More Efficient Language Models,102,qmm9z7,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmm9z7/r_hierarchical_transformers_are_more_efficient/,9,"[https  arxiv org abs  ] https  arxiv org abs   

A team from Google  OpenAI  and University of Warsaw proposes a new Efficient Transformer architecture for language modeling  setting a new state of the art on the imagenet for autoregressive models ",1636037469.0,2021-11-04 15:51:09,[ arxiv org abs ] arxiv org abs team google openai university warsaw proposes new efficient transformer architecture language modeling setting new state art imagenet autoregressive models
[D] Why do we need the random noise z in conditional GANs?,16,qmy3ir,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmy3ir/d_why_do_we_need_the_random_noise_z_in/,10,Obviously  we need some kind of input for the neural net  But in the case of conditional GANs  we have another kind of input  Does the random noise z then only serve to introduce variety for a given condition  e g  many different faces all with blonde hair   If I didn’t care about this variety  could I just do without the random noise  Or is there some other justification for why we need the random noise z  makes training easier  some theoretical reason  …  ,1636069609.0,2021-11-05 00:46:49,obviously need kind input neural net case conditional gans another kind input random noise z serve introduce variety given condition e g many different faces blonde hair didn’t care variety could without random noise justification need random noise z makes training easier theoretical reason …
[D] How does ACL rolling review work?,0,qnbrji,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qnbrji/d_how_does_acl_rolling_review_work/,1,"Hi folks  I am going through the ACL rolling review process and have some doubts 

So  after submitting the paper at the open review website  

  if accepted the reviews  comments  and pdf on open review will be there 

  Same for rejected papers  if rejected  can I withdraw my article from open review  or it is going to be there along with rejected reviews on open review website 

My main concern is if rejected and submitted to another conference  there will be plagiarism because the paper will be already on the open review website along with reviews  then it will affect the other submission ",1636120818.0,2021-11-05 15:00:18,hi folks going acl rolling review process doubts submitting paper open review website accepted reviews comments pdf open review rejected papers rejected withdraw article open review going along rejected reviews open review website main concern rejected submitted another conference plagiarism paper already open review website along reviews affect submission
[P] A place to create your online data science portfolio and browse the community data science projects,0,qnhtxj,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qnhtxj/p_a_place_to_create_your_online_data_science/,0,"Hey all  I m a data scientist who has shifted career from the biomedical field   now working at a tech company  It was hard to learn data science skills  showcase them to my first employers and stand out  That s why I created [datascienceportfol io] https  www datascienceportfol io  You can create your own online portfolio  showcasing your projects and skills in an effective way  Also  you can get inspired by browsing projects created by the community 

Still early days and I m now working on a section to browse projects of other people and get inspired 

Please  let me know what you think  any feedback or improvement ideas are very welcome   D Thanks so much  Pasquale",1636138088.0,2021-11-05 19:48:08,hey data scientist shifted career biomedical field working tech company hard learn data science skills showcase first employers stand created [datascienceportfol io] www datascienceportfol io create online portfolio showcasing projects skills effective way also get inspired browsing projects created community still early days working section browse projects people get inspired please let know think feedback improvement ideas welcome thanks much pasquale
"[D] In theory, could we make a code that bridges different music genres?",7,qn1erq,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qn1erq/d_in_theory_could_we_make_a_code_that_bridges/,5,"I know little to no about coding  but I have been playing with VQGAN recently and this idea has popped into my head  In theory  could we write an AI code that gets feed stems  that is  each instrument track as a separate audio file  for two different songs and rebuilds the first song in the style of the second one    
The code would look for relevant information from the first song such as melodies  rhythms  structure and maybe even allow the user to feed in the lyrics to the vocals  Then  it would study how these elements are employed in the second song and rebuild the first song using the second song s style  This code could rebuild a rock song as a rap song  for instance ",1636080051.0,2021-11-05 03:40:51,know little coding playing vqgan recently idea popped head theory could write ai code gets feed stems instrument track separate audio file two different songs rebuilds first song style second one code would look relevant information first song melodies rhythms structure maybe even allow user feed lyrics vocals would study elements employed second song rebuild first song using second song style code could rebuild rock song rap song instance
[D] Ethical concerns for ML to predict race & gender,41,qmm6uh,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmm6uh/d_ethical_concerns_for_ml_to_predict_race_gender/,86,"I’m working on a data product that primarily uses image and name classifiers to identify race and gender  This means that someone who buys this product is now able to see race and gender data associated with people and or companies in their database  The use case behind this is to report on and make decisions to improve diversity  i e  an investment firm seeking to invest more in underrepresented groups  an HR company reporting on industry trends  

I’m looking for feedback on ethical design quality concerns in regards to some of the following factors 

 	We are primarily leveraging publicly available training data sets  models  and classifiers 
 	Gender classification includes only male female options 
 	We use a publicly available photo for classifying each person 
 	None of the data we provide is self reported  nor does the product communicate that to the customer  
 	We do not yet provide a confidence score or any feedback or correction feature 
 	Race and gender sex are legally protected classes in some cases  While we are not using these to make any decisions in our product  we are the ones generating this data  which our customers will use at their discretion 

I’m worried we are not doing enough due diligence for the intentional choices we’re making and the unintended impact they may have  Any resources for designing fair systems  especially ones that attempt to generate rather than consume this type of data  would be appreciated ",1636037227.0,2021-11-04 15:47:07,i’m working data product primarily uses image name classifiers identify race gender means someone buys product able see race gender data associated people companies database use case behind report make decisions improve diversity e investment firm seeking invest underrepresented groups hr company reporting industry trends i’m looking feedback ethical design quality concerns regards following factors primarily leveraging publicly available training data sets models classifiers gender classification includes male female options use publicly available photo classifying person none data provide self reported product communicate customer yet provide confidence score feedback correction feature race gender sex legally protected classes cases using make decisions product ones generating data customers use discretion i’m worried enough due diligence intentional choices we’re making unintended impact may resources designing fair systems especially ones attempt generate rather consume type data would appreciated
[D] Buying a PC for training - Does it make sense in 2021?,0,qngw0u,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qngw0u/d_buying_a_pc_for_training_does_it_make_sense_in/,7,"I work with text data  I d love to put \~  down to have a machine that can fine tune the largest GPT  instance  the largest two are M  and  B parameters   Does it still make sense to buy a computer to do it  I ve spent a few hundred dollars on AWS credits  but knowing that I m being throttled by the cost limits the experiments I can run 

I would love to hear from those who have bought machines or those who have decided against it  I would probably look for a used one on eBay  though I know very little about purchasing a PC ",1636135439.0,2021-11-05 19:03:59,work text data love put \~ machine fine tune largest gpt instance largest two b parameters still make sense buy computer spent hundred dollars aws credits knowing throttled cost limits experiments run would love hear bought machines decided would probably look used one ebay though know little purchasing pc
[D] Why Jupyter notebook doesnt store requirements (require packages) in ipynb file?,0,qnh7w4,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qnh7w4/d_why_jupyter_notebook_doesnt_store_requirements/,9,The ipynb file is a JSON file  List with required packages can be easily added there  Why there is a separate file for this ,1636136360.0,2021-11-05 19:19:20,ipynb file json file list required packages easily added separate file
[D] What's the difference between top-tier papers and others?,53,qmhthd,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmhthd/d_whats_the_difference_between_toptier_papers_and/,14,"Hello  guys 

Although  I read a various of papers that in top conferences such as ICCV  ECCV  ICML 

I don t know what s difference in the first  second and other tier papers 

From the top paper  All I can know it s very obvious in writing skills and technologies that I don t understand yet 

 xB 

To be specific  What s different in the way that distinguish them ",1636022465.0,2021-11-04 11:41:05,hello guys although read various papers top conferences iccv eccv icml know difference first second tier papers top paper know obvious writing skills technologies understand yet xb specific different way distinguish
[D] What is the most effective way to mix scalar value(s) into CNN feature maps?,3,qn2jg5,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qn2jg5/d_what_is_the_most_effective_way_to_mix_scalar/,5,"It feels like an easy task but I can t seem to recall or find much info on this 

What I m trying to do is use a scalar value and try to mix it into the intermediate feature maps of a CNN 

I know typically you might just concatenate these kind of scalars after flattening the feature map and before a FC layer  but I want this value to be combined into the intermediate feature maps between convolutions  and not at the end of the whole CNN encoder 

If it were categorical information  I know I could use learnable embeddings and add concat  but in my case this is a continuous scalar 

I ve seen suggestions to treat this scalar like a bias term and simply add  but this doesn t look strong and I m not quite convinced  I ve also thought about copying this value to a h x w x  array to concat before the next convolution  but I m not sure about this either 

What is the most effective method to do this ",1636083886.0,2021-11-05 04:44:46,feels like easy task seem recall find much info trying use scalar value try mix intermediate feature maps cnn know typically might concatenate kind scalars flattening feature map fc layer want value combined intermediate feature maps convolutions end whole cnn encoder categorical information know could use learnable embeddings add concat case continuous scalar seen suggestions treat scalar like bias term simply add look strong quite convinced also thought copying value h x w x array concat next convolution sure either effective method
[P] StyleGAN3 + Wav2Lip,11,qmqhgg,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmqhgg/p_stylegan3_wav2lip/,7," xB 

[due to the limit of my compute the quality suffers a bit ] https  reddit com link qmqhgg video vzcfbmx player ",1636048748.0,2021-11-04 18:59:08,xb [due limit compute quality suffers bit ] reddit com link qmqhgg video vzcfbmx player
[P] Survey study examining practices in NLG evaluation,3,qmw9lr,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmw9lr/p_survey_study_examining_practices_in_nlg/,1,"**Do you work or do research on natural language generation  NLG  ** If yes  we are interested in your participation in a ** minute survey** about practices when evaluating NLG systems or models 

The participants should have experience with working with or on  any type of  natural language generation  NLG  systems and tasks  The purpose of this research is to uncover unnamed practices and assumptions made during the evaluation of NLG systems  applications  and tasks  We hope that by understanding such practices and assumptions we will be able to better unpack the ways they could lead to **unintended consequences related to fairness and inclusion**   

**If you are interested  please fill out this form here**   [https  forms office com r RBLszeei] https  forms office com r RBLszeei   Thank you so much for your consideration and help ",1636064370.0,2021-11-04 23:19:30,**do work research natural language generation nlg ** yes interested participation ** minute survey** practices evaluating nlg systems models participants experience working type natural language generation nlg systems tasks purpose research uncover unnamed practices assumptions made evaluation nlg systems applications tasks hope understanding practices assumptions able better unpack ways could lead **unintended consequences related fairness inclusion** **if interested please fill form here** [ forms office com r rblszeei] forms office com r rblszeei thank much consideration help
[Discussion] Applied machine learning implementation debate. Is OOP approach towards data preprocessing in python an overkill?,201,qm6ieq,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qm6ieq/discussion_applied_machine_learning/,86,"***TL DR*** 

* I am trying to find ways to standardise the way we solve things in my Data Science team  setting common workflows and conventions
* To illustrate the case I expose a *probably over engineered* OOP solution for Preprocessing data 
* The OOP proposal is neither relevant nor important and I will be happy to do things differently  I actually apply a functional approach myself when working alone   The main interest here is to **trigger conversations towards** **proper project and software architecture  patterns and best practices among the Data Science community **

  Context

I am working as a Data Scientist in a big company and I am trying as hard as I can to set some best practices and protocols to standardise the way we do things within my team  ergo  changing the extensively spread and overused Jupyter Notebook practices and start building a proper workflow and reusable set of tools 

In particular  the idea is to define a common way of doing things  workflow protocol  over s of projects implementations  so anyone can jump in and understand whats going on  as the way of doing so has been enforced by process definition  As of today  every Data Scientist in the team follows a procedural approach of its own taste  making it sometimes cumbersome and non obvious to understand what is going on  Also  often times it is not easily executable and hardly replicable 

I have seen among the community that this is a recurrent problem  eg 

* [https  www reddit com r MachineLearning comments qey d\_tired\_of\_writing\_mundane\_data\_wrangling\_code ] https  www reddit com r MachineLearning comments qey d_tired_of_writing_mundane_data_wrangling_code 

In my own opinion  many Data Scientist  are really in the crossroad between Data Engineering  Machine Learning Engineering  Analytics and Software Development  knowing about all  but not necessarily mastering any   Unless you have a CS background  I don t   we may understand very well ML concepts and algorithms  know inside out **Scikit Learn** and **PyTorch ** but there is no doubt that we sometimes lack software development basics that really help when building something bigger 

I have been searching general applied machine learning best practices for a while now  and even if there are tons of resources for general architectures and design patterns in many other areas  I have not found a clear agreement for the case  The closest thing you can find is cookiecutters that just define a general project structure  not detailed implementation and intention 

  Example  Proposed solution for Preprocessing

For the sake of example  I would like to share a potential structured solution for **Processing**  as I believe it may well be   of the job  This case is for the general **Dask** or **Pandas** processing routine  not other huge big data pipes that may require other sort of solutions 

\*\** if by any chance this ends up being something people are willing to debate and we can together find a common framework  I would be more than happy to share more examples for different processes *

 xB 

 *Keep in mind that the proposal below could be perfectly solved with a functional approach as well  The idea here is to force a team to use the same* ***blueprint*** *over and over again and follow the same* ***structure and protocol***  even if by so the solution may be a bit over engineered  The blocks are meant to be replicated many times and set a common agreement to always proceed the same way  forced by the abstract class    
   
 IMO the final abstraction seems to be clear and it makes easy to understand whats happening  in which order things are being processed  etc  The transformation itself  `main_pipe`  is also clear and shows the steps explicitly 

In a typical routine  there are  well defined steps 

* Read parse data
* Transform data
* Export processed data

Basically  an ETL process  This could be solved in a functional way  You can even go the extra mile by following `pipes` chained methods  as brilliantly explained here [https  tomaugspurger github io method chaining] https  tomaugspurger github io method chaining 

It is clear the `pipes` approach follows the same *parse→transform→export* structure  This level of cohesion shows a common pattern that could be defined into an `abstract class`  This `class` defines the bare minimum requirements of a **pipe**  being of course always possible to extend the functionality of any instance if needed 

By defining the `Base class` as such  we explicitly force a cohesive way of defining `DataProcessPipe`  *pipe* naming convention may be substituted by *block* to avoid later confusion with **Scikit learn** `Pipelines`   This base class contains `parse_data`  `export_data`  `main_pipe` and `process` methods

In short  **it defines a formal interface that describes what any process block pipe implementation should do **

A specific implementation of the former will then follow 

    
    from processing base import DataProcessPipeBase
    
    class Pipe DataProcessPipeBase  
    
        name =  Clean raw files  
    
        def __init__ self  import_path  export_path  params  
            self import_path = import_path
            self export_path = export_path
            self params = params
    
        def parse_data self    pd DataFrame 
            df = pd read_csv self import_path 
            return df
    
        def export_data self  df  pd DataFrame    None 
            df to_csv os path join self export_path  index=False 
            return None
    
        def main_pipe self  df  pd DataFrame    pd DataFrame 
            return  df
                      dropnan 
                      reset_index drop=True 
                      pipe extract_name  self params[ extract ] 
                      pipe time_to_datetime  self params[ dt ] 
                      groupby foo  sum 
                      reset_index drop=True 
    
        def process self    None 
            df = self parse_data 
            df = self main_pipe df 
            self export_data df 
            return None

With this approach 

* The ins and outs are clear  this could be one or many in both cases and specify imports  exports  even middle exports in the `main_pipe` method 
* The interface allows to use indistinctly **Pandas**  **Dask** or any other library of choice 
* If needed  further functionality beyond the `abstractmethods` defined can be implemented 

Note how parameters can be just passed from a **yaml** or **json** file 

For complete processing pipelines  it will be needed to implement as many DataProcessPipes required  This is also convenient  as they can easily be then executed as follows 

    from processing pipes import Pipe  Pipe  Pipe
    
    class DataProcessPipeExecutor 
        def __init__ self  sorted_pipes_dict  
            self pipes = sorted_pipes_dict
    
        def execute self  
            for _  pipe in pipes items  
                pipe process 
    
    if __name__ ==  __main__  
        PARAMS = json loads parameters json 
        pipes_dict = {
             pipe   Pipe input csv    output csv   PARAMS[ pipe ] 
             pipe   Pipe output csv    output csv   PARAMS[ pipe ] 
             pipe   Pipe [ input csv    output csv ]   clean csv   PARAMS[ pipe ] 
        }
        executor = DataProcessPipeExecutor pipes_dict 
        executor execute 

  Conclusion

Even if this approach works for me  I would like this to be just an example that opens conversations towards proper project and software architecture  patterns and best practices among the Data Science community  I will be more than happy to flush this idea away if a better way can be proposed and its highly standardised and replicable 

If any  the main questions here would be 

* Does all this makes any sense whatsoever for this particular example approach 
* Is there any place  resource  etc  where I can have some guidance or where people are discussing this 

Thanks a lot in advance

\ 

PS  this first post was published on StackOverflow  but was erased cause  as you can see  it does not define a clear question based on facts  at least until the end  I would still love to see if anyone is interested and can share its views ",1635979901.0,2021-11-03 23:51:41,***tl dr*** * trying find ways standardise way solve things data science team setting common workflows conventions * illustrate case expose *probably engineered* oop solution preprocessing data * oop proposal neither relevant important happy things differently actually apply functional approach working alone main interest **trigger conversations towards** **proper project software architecture patterns best practices among data science community ** context working data scientist big company trying hard set best practices protocols standardise way things within team ergo changing extensively spread overused jupyter notebook practices start building proper workflow reusable set tools particular idea define common way things workflow protocol projects implementations anyone jump understand whats going way enforced process definition today every data scientist team follows procedural approach taste making sometimes cumbersome non obvious understand going also often times easily executable hardly replicable seen among community recurrent problem eg * [ www reddit com r machinelearning comments qey d\_tired\_of\_writing\_mundane\_data\_wrangling\_code ] www reddit com r machinelearning comments qey d_tired_of_writing_mundane_data_wrangling_code opinion many data scientist really crossroad data engineering machine learning engineering analytics software development knowing necessarily mastering unless cs background may understand well ml concepts algorithms know inside **scikit learn** **pytorch ** doubt sometimes lack software development basics really help building something bigger searching general applied machine learning best practices even tons resources general architectures design patterns many areas found clear agreement case closest thing find cookiecutters define general project structure detailed implementation intention example proposed solution preprocessing sake example would like share potential structured solution **processing** believe may well job case general **dask** **pandas** processing routine huge big data pipes may require sort solutions \*\** chance ends something people willing debate together find common framework would happy share examples different processes * xb *keep mind proposal could perfectly solved functional approach well idea force team use same* ***blueprint*** *over follow same* ***structure protocol*** even solution may bit engineered blocks meant replicated many times set common agreement always proceed way forced abstract class imo final abstraction seems clear makes easy understand whats happening order things processed etc transformation `main_pipe` also clear shows steps explicitly typical routine well defined steps * read parse data * transform data * export processed data basically etl process could solved functional way even go extra mile following `pipes` chained methods brilliantly explained [ tomaugspurger github io method chaining] tomaugspurger github io method chaining clear `pipes` approach follows *parse→transform→export* structure level cohesion shows common pattern could defined `abstract class` `class` defines bare minimum requirements **pipe** course always possible extend functionality instance needed defining `base class` explicitly force cohesive way defining `dataprocesspipe` *pipe* naming convention may substituted *block* avoid later confusion **scikit learn** `pipelines` base class contains `parse_data` `export_data` `main_pipe` `process` methods short **it defines formal interface describes process block pipe implementation ** specific implementation former follow processing base import dataprocesspipebase class pipe dataprocesspipebase name = clean raw files def __init__ self import_path export_path params self import_path = import_path self export_path = export_path self params = params def parse_data self pd dataframe df = pd read_csv self import_path return df def export_data self df pd dataframe none df to_csv os path join self export_path index=false return none def main_pipe self df pd dataframe pd dataframe return df dropnan reset_index drop=true pipe extract_name self params[ extract ] pipe time_to_datetime self params[ dt ] groupby foo sum reset_index drop=true def process self none df = self parse_data df = self main_pipe df self export_data df return none approach * ins outs clear could one many cases specify imports exports even middle exports `main_pipe` method * interface allows use indistinctly **pandas** **dask** library choice * needed functionality beyond `abstractmethods` defined implemented note parameters passed **yaml** **json** file complete processing pipelines needed implement many dataprocesspipes required also convenient easily executed follows processing pipes import pipe pipe pipe class dataprocesspipeexecutor def __init__ self sorted_pipes_dict self pipes = sorted_pipes_dict def execute self _ pipe pipes items pipe process __name__ == __main__ params = json loads parameters json pipes_dict = { pipe pipe input csv output csv params[ pipe ] pipe pipe output csv output csv params[ pipe ] pipe pipe [ input csv output csv ] clean csv params[ pipe ] } executor = dataprocesspipeexecutor pipes_dict executor execute conclusion even approach works would like example opens conversations towards proper project software architecture patterns best practices among data science community happy flush idea away better way proposed highly standardised replicable main questions would * makes sense whatsoever particular example approach * place resource etc guidance people discussing thanks lot advance \ ps first post published stackoverflow erased cause see define clear question based facts least end would still love see anyone interested share views
[D] FUZZY C-MEANS Clustering on line graph data,1,qn0ynq,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qn0ynq/d_fuzzy_cmeans_clustering_on_line_graph_data/,1,"Hi I ve been trying to apply fuzzy c means on data that can be represented as line graphs  hourly electrical load profiles   I understand that I must cluster the points on each hour  What I don t get is how do I relate the clustered points on each hour to the adjacent hour   So that I can obtain the result which is a clustered line graph  

Here is an example of an input and the desired output 

[Input  Electrical Load Profiles] https  preview redd it icxkmqox png width= format=png auto=webp s=dfecfebfadffbf 

 xB 

[Output  Clustered Results] https  preview redd it vyoqox png width= format=png auto=webp s=adfbfddfbeadca ",1636078590.0,2021-11-05 03:16:30,hi trying apply fuzzy c means data represented line graphs hourly electrical load profiles understand must cluster points hour get relate clustered points hour adjacent hour obtain result clustered line graph example input desired output [input electrical load profiles] preview redd icxkmqox png width= format=png auto=webp s=dfecfebfadffbf xb [output clustered results] preview redd vyoqox png width= format=png auto=webp s=adfbfddfbeadca
[D] Is there any way for GAN to generate arbitrary length of time series signal?,3,qmq7hb,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmq7hb/d_is_there_any_way_for_gan_to_generate_arbitrary/,4,"Hello  I m working on using GAN to generate some signals  As I have viewed some related works  I found that most of them merely sample a latent vector from some distribution with fixed size * e g a latent vector of dim  *  and after some upsampling operation  they will get signals in a fixed window size * e g a signal of s \* Hz =  points * 

I want to get rid of the annoying limit of  fixed window size   and be able to generate continuous signal with arbitrary length  

I tried to code by myself  I have designed a GAN framework   in which the generator takes an input of an arbitrary length of vector  and outputs a signal in same length as the input  The upsampling process is thrown away here  so the generator merely do some modification on the input signal instead of upsampling on it  As for discriminator  I use global average pooling in replace of the linear layers  However  my code failed to work  So I think maybe I need some new ideas 

I come to you guys for help  Do you know any paper that might be helpful for me  Or do you have any good idea 

Thanks ",1636047990.0,2021-11-04 18:46:30,hello working using gan generate signals viewed related works found merely sample latent vector distribution fixed size * e g latent vector dim * upsampling operation get signals fixed window size * e g signal \* hz = points * want get rid annoying limit fixed window size able generate continuous signal arbitrary length tried code designed gan framework generator takes input arbitrary length vector outputs signal length input upsampling process thrown away generator merely modification input signal instead upsampling discriminator use global average pooling replace linear layers however code failed work think maybe need new ideas come guys help know paper might helpful good idea thanks
[D] SageMaker Linear Learner,0,qmxns9,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmxns9/d_sagemaker_linear_learner/,2,"Hi Everyone 

I was just wondering  does anyone know where there is a corresponding library in cran or scikit learn for the linear learner in AWS sage maker 

I do not have access to AWS so I can t tell whether it is just a interface to different regressions  or something more sophisticated 

Enjoy your day    
Fella",1636068276.0,2021-11-05 00:24:36,hi everyone wondering anyone know corresponding library cran scikit learn linear learner aws sage maker access aws tell whether interface different regressions something sophisticated enjoy day fella
[R] Washington U & Google Study Reveals How Attention Matrices Are Formed in Encoder-Decoder Architectures,4,qmln6l,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmln6l/r_washington_u_google_study_reveals_how_attention/,0,"In the new paper Understanding How Encoder Decoder Architectures Attend  researchers from the University of Washington  Google Blueshift Team and Google Brain Team propose a method for decomposing hidden states over a sequence into temporal  and input driven components  revealing how attention matrices are formed in encoder decoder networks  

Here is a quick read  [Washington U   Google Study Reveals How Attention Matrices Are Formed in Encoder Decoder Architectures ] https  syncedreview com    deepmind podracer tpu based rl frameworks deliver exceptional performance at low cost  

The paper *Understanding How Encoder Decoder Architectures Attend* is on [arXiv] https  arxiv org abs    ",1636035633.0,2021-11-04 15:20:33,new paper understanding encoder decoder architectures attend researchers university washington google blueshift team google brain team propose method decomposing hidden states sequence temporal input driven components revealing attention matrices formed encoder decoder networks quick read [washington u google study reveals attention matrices formed encoder decoder architectures ] syncedreview com deepmind podracer tpu based rl frameworks deliver exceptional performance low cost paper *understanding encoder decoder architectures attend* [arxiv] arxiv org abs
[R] Koopman Q-learning: Offline Reinforcement Learning via Symmetries of Dynamics,17,qmc9mb,MachineLearning,https://arxiv.org/abs/2111.01365,1,nan,1635998223.0,2021-11-04 04:57:03,nan
[R] Pruning for Self-Supervised Speech Recognition,1,qmsyf8,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmsyf8/r_pruning_for_selfsupervised_speech_recognition/,0,"MIT News  [https  news mit edu  speech recognition uncommon languages  fbclid=IwARXbTIBITLpqKbevvDhgBdoSPEcTfIzwOCR IgybNvdtMZblo] https  news mit edu  speech recognition uncommon languages  fbclid=IwARXbTIBITLpqKbevvDhgBdoSPEcTfIzwOCR IgybNvdtMZblo 

Paper  NeurIPS    [https  arxiv org abs  ] https  arxiv org abs   ",1636055444.0,2021-11-04 20:50:44,mit news [ news mit edu speech recognition uncommon languages fbclid=iwarxbtibitlpqkbevvdhgbdospectfizwocr igybnvdtmzblo] news mit edu speech recognition uncommon languages fbclid=iwarxbtibitlpqkbevvdhgbdospectfizwocr igybnvdtmzblo paper neurips [ arxiv org abs ] arxiv org abs
[R] Local Latin Hypercube Refinement for Multi-objective Design Uncertainty Optimization,3,qmj7tq,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmj7tq/r_local_latin_hypercube_refinement_for/,3,"Many real world systems consist of input features with aleatoric  i e  irreducible  uncertainties  In engineering design applications  such uncertainties may arise from production tolerances  operational conditions as well as other environmental factors  Thus  the distribution of these features can be measured and to some extent modified  e g  by moving the mean value  

Design uncertainty optimization seeks to find the distribution parameters of input features  which optimize metrics such as the failure probability and the variance of key performance indicators besides the expected objective values  Since these often require uncertainty quantification of black box functions  the whole process is computationally quite burdensome  In this work  we propose using machine learning methods in combination with sequential sampling to reduce the required amount of computation and accelerate the uncertainty optimization task  Due to the small data setting  we limit the investigation to GPR and SVR but argue that a more suitable model can exist depending on the problem  which could be used within the proposed framework instead 

[https  arxiv org abs  ] https  arxiv org abs   ",1636028027.0,2021-11-04 13:13:47,many real world systems consist input features aleatoric e irreducible uncertainties engineering design applications uncertainties may arise production tolerances operational conditions well environmental factors thus distribution features measured extent modified e g moving mean value design uncertainty optimization seeks find distribution parameters input features optimize metrics failure probability variance key performance indicators besides expected objective values since often require uncertainty quantification black box functions whole process computationally quite burdensome work propose using machine learning methods combination sequential sampling reduce required amount computation accelerate uncertainty optimization task due small data setting limit investigation gpr svr argue suitable model exist depending problem could used within proposed framework instead [ arxiv org abs ] arxiv org abs
[D] Pairprogramming/ pair analysis in ML development teams to improve harmony?,1,qmrr7t,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmrr7t/d_pairprogramming_pair_analysis_in_ml_development/,0,"Hi 

I m gonna assemble a small team to work on an ML project  which might lead to an ML first software  and I was wondering about whether using pair programming can be beneficial in ML teams  especially in early stages  which is adventurous analysis and exploration   What is your experience with pair programming in ML and in what stages do you think it can be good  With what strategy and etc 

We are all working remotely  so that s another aspect that is interesting  What is your experience  and how do you create harmony in a newly assembled team that is working remotely ",1636052233.0,2021-11-04 19:57:13,hi gonna assemble small team work ml project might lead ml first software wondering whether using pair programming beneficial ml teams especially early stages adventurous analysis exploration experience pair programming ml stages think good strategy etc working remotely another aspect interesting experience create harmony newly assembled team working remotely
[P] iris - Open Source Photos Platform powered by PyTorch,90,qm06ct,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qm06ct/p_iris_open_source_photos_platform_powered_by/,5,"This is my submission for PyTorch Annual Hackathon   A self hosted alternative to Google Photos  Currently it contains basic features built with short scope of hackathon  The team will be continuing to work by adding new features 

[Explore Section] https  preview redd it cftmbofx png width= format=png auto=webp s=ecdeefbcfeddebc 

[Smart Search] https  preview redd it wxuofx png width= format=png auto=webp s=bddcacadfbdfebdbfd 

Go check out and support the project now from below links 

YouTube  [https  www youtube com watch v=ZMGrohochc] https  www youtube com watch v=ZMGrohochc 

DevPost  [https  devpost com software iris syna] https  devpost com software iris syna 

GitHub  [https  github com prabhuomkar iris] https  github com prabhuomkar iris 

 xB ",1635962267.0,2021-11-03 18:57:47,submission pytorch annual hackathon self hosted alternative google photos currently contains basic features built short scope hackathon team continuing work adding new features [explore section] preview redd cftmbofx png width= format=png auto=webp s=ecdeefbcfeddebc [smart search] preview redd wxuofx png width= format=png auto=webp s=bddcacadfbdfebdbfd go check support project links youtube [ www youtube com watch v=zmgrohochc] www youtube com watch v=zmgrohochc devpost [ devpost com software iris syna] devpost com software iris syna github [ github com prabhuomkar iris] github com prabhuomkar iris xb
[D] Best approach for noisy language detection,0,qmw43e,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmw43e/d_best_approach_for_noisy_language_detection/,0,"I need to predict the language  e g  English  Portuguese  Russian  from a few words   sentences  This is noisy real world data  meaning that the words might be misspelled  have poor grammar  emojis  language switching etc 

There s a bunch of different GitHub repos out there  but it s unclear which one works well  especially for noisy real world data and not e g  Wikipedia  I had hoped that websites like [https  paperswithcode com area natural language processing] https  paperswithcode com area natural language processing  would have solved this by now  but I don t see a relevant category for language detection  I don t have strong requirements for the solution to be particularly efficient although a transformer approach seems overkill for this 

Any advice on a simple Python library to use ",1636063956.0,2021-11-04 23:12:36,need predict language e g english portuguese russian words sentences noisy real world data meaning words might misspelled poor grammar emojis language switching etc bunch different github repos unclear one works well especially noisy real world data e g wikipedia hoped websites like [ paperswithcode com area natural language processing] paperswithcode com area natural language processing would solved see relevant category language detection strong requirements solution particularly efficient although transformer approach seems overkill advice simple python library use
[D] Feedback on our idea - a platform that turns code into a monetized API,0,qmq67b,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmq67b/d_feedback_on_our_idea_a_platform_that_turns_code/,6," 

Hi 

we re building a platform that turns code into an API   hosted by us  infrastructure is paid by whoever calls your service  we charge the exact amount the infra costs us  if you choose to you can also monetize your service 

as a data scientist myself I m really excited about the possibilities of easily sharing my models  capabilities as an API without dealing with dev ops   but I m biased because I love what we re building  I really want to know what do you think about using such a platform  not mentioning the name  not sure it s allowed 

how many of you both create ML models and know how to set up your own API  and monetize it  

we want smart people to create smart solutions and be able to share them easily with anyone 

creating such a service would be free  signing up is free and as a creator you can only earn if someone is using your service 

any thoughts 

Offer",1636047892.0,2021-11-04 18:44:52,hi building platform turns code api hosted us infrastructure paid whoever calls service charge exact amount infra costs us choose also monetize service data scientist really excited possibilities easily sharing models capabilities api without dealing dev ops biased love building really want know think using platform mentioning name sure allowed many create ml models know set api monetize want smart people create smart solutions able share easily anyone creating service would free signing free creator earn someone using service thoughts offer
[R] [2110.13771] AugMax: Adversarial Composition of Random Augmentations for Robust Training,4,qmik2k,MachineLearning,https://arxiv.org/abs/2110.13771,3,nan,1636025531.0,2021-11-04 12:32:11,nan
[R] Intermediate Layer Optimization for Inverse Problems using Deep Generative Models,5,qmfsnd,MachineLearning,https://arxiv.org/abs/2102.07364,1,nan,1636013147.0,2021-11-04 09:05:47,nan
[D] Paper Explained - EfficientZero: Mastering Atari Games with Limited Data (Full Video Analysis),21,qm6l31,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qm6l31/d_paper_explained_efficientzero_mastering_atari/,2,"[https  youtu be NJCLUzkn sA] https  youtu be NJCLUzkn sA 

Reinforcement Learning methods are notoriously data hungry  Notably  MuZero learns a latent world model just from scalar feedback of reward  and policy predictions  and therefore relies on scale to perform well  However  most RL algorithms fail when presented with very little data  EfficientZero makes several improvements over MuZero that allows it to learn from astonishingly small amounts of data and outperform other methods by a large margin in the low sample setting  This could be a staple algorithm for future RL research 

 xB 

OUTLINE 

    Intro   Outline

    MuZero Recap

    EfficientZero improvements

    Self Supervised consistency loss

    End to end prediction of the value prefix

    Model based off policy correction

    Experimental Results   Conclusion

 xB 

Paper  [https  arxiv org abs  ] https  arxiv org abs   

Code  [https  github com YeWR EfficientZero] https  github com YeWR EfficientZero 

Note  code not there yet as of release of this video",1635980120.0,2021-11-03 23:55:20,[ youtu njcluzkn sa] youtu njcluzkn sa reinforcement learning methods notoriously data hungry notably muzero learns latent world model scalar feedback reward policy predictions therefore relies scale perform well however rl algorithms fail presented little data efficientzero makes several improvements muzero allows learn astonishingly small amounts data outperform methods large margin low sample setting could staple algorithm future rl research xb outline intro outline muzero recap efficientzero improvements self supervised consistency loss end end prediction value prefix model based policy correction experimental results conclusion xb paper [ arxiv org abs ] arxiv org abs code [ github com yewr efficientzero] github com yewr efficientzero note code yet release video
[N] Zillow’s NN-based Zestimate Leads to Massive Losses in Home Flipping Business,662,qlilnf,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlilnf/n_zillows_nnbased_zestimate_leads_to_massive/,211,"Zillow announced that they are [laying off a quarter of their workforce] https  www cbsnews com news zillow layoffs closing zillow offers selling homes  due to a   million loss incurred by Zillow Offers  the home flipping arm of their business  The business model was reliant on [Zestimate] https  www zillow com z zestimate   a neural network based model that forecasts housing prices 

This seems like a colossal misstep on their part  It begs the question  how can other companies avoid a similar fate if they are making large gambles based on machine learning models predicting market movements  Additionally  how much should consumers rely on market predictions like Zestimate when making financial decisions  speaking as someone who recently bought a home and researched the market on Zillow during the process  ",1635899869.0,2021-11-03 01:37:49,zillow announced [laying quarter workforce] www cbsnews com news zillow layoffs closing zillow offers selling homes due million loss incurred zillow offers home flipping arm business business model reliant [zestimate] www zillow com z zestimate neural network based model forecasts housing prices seems like colossal misstep part begs question companies avoid similar fate making large gambles based machine learning models predicting market movements additionally much consumers rely market predictions like zestimate making financial decisions speaking someone recently bought home researched market zillow process
[D] AAAI 2022 Paper Reviews,61,qlt4cz,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlt4cz/d_aaai_2022_paper_reviews/,84,Now that AAAI  reviews are out  I am creating a discussion thread for this year s reviews ,1635941607.0,2021-11-03 13:13:27,aaai reviews creating discussion thread year reviews
[Project] Discover ongoing ML/AI competitions,14,qm2ov5,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qm2ov5/project_discover_ongoing_mlai_competitions/,0,"***Looking for feature suggestions for mlcontests com ***

*It s been two years since I posted here about my then new project* [ML Contests] https  mlcontests com *  It was well received and since it s been a while I thought I d post an update and ask for feedback * 

[Main page \ mlcontests com\ ] https  preview redd it tohjmawgx png width= format=png auto=webp s=ebcacabbbcadacdb 

The main page just lists ongoing competitions  There s also a newsletter where I occasionally send out updates about the competitive ML space  and a separate page which compares cloud GPUs for ML 

You can visit the site at [MLContests com] https  mlcontests com  

Traffic is steady  the newsletter is growing  there are no ads  and I d like to figure out where to take it next  **I d love to hear your thoughts on what you want from the site **

PS  If you want to contribute  it s all open source  [https  github com mlcontests mlcontests github io] https  github com mlcontests mlcontests github io    ",1635969222.0,2021-11-03 20:53:42,***looking feature suggestions mlcontests com *** *it two years since posted new project* [ml contests] mlcontests com * well received since thought post update ask feedback * [main page \ mlcontests com\ ] preview redd tohjmawgx png width= format=png auto=webp s=ebcacabbbcadacdb main page lists ongoing competitions also newsletter occasionally send updates competitive ml space separate page compares cloud gpus ml visit site [mlcontests com] mlcontests com traffic steady newsletter growing ads like figure take next **i love hear thoughts want site ** ps want contribute open source [ github com mlcontests mlcontests github io] github com mlcontests mlcontests github io
[R] Can Vision Transformers Perform Convolution?,21,qlxivf,MachineLearning,https://arxiv.org/abs/2111.01353,2,nan,1635955036.0,2021-11-03 16:57:16,nan
Neurosymbolic models vs. large transformers in program synthesis,11,qm15le,MachineLearning,https://arxiv.org/abs/2111.01633,2,nan,1635964991.0,2021-11-03 19:43:11,nan
[D] Python toolboxes for probabilistic graphical model inference,5,qm578v,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qm578v/d_python_toolboxes_for_probabilistic_graphical/,2,"Hi folks 

which libraries or toolboxes would you recommend  ideally based on personal experience  for performing inference in probabilistic graphical models  for an actual practical application and not for academic toy examples 

Minimal requirements  must be able to specify a Bayesian network or factor graph consisting of categorical nodes  some of which are hidden and others observed  and use a set of observations to identify the factors   dependencies   I think essentially any PGM toolbox will fulfill these requirements  

Bonus points given for   good documentation  maturity stability  works efficiently with many factors and many datapoints  implements many different inference methods and modeling paradigms  simplicity of use

I do know of a few promising toolboxes such as [pgmpy] https  github com pgmpy pgmpy   [pymc] https  docs pymc io en stable   and [pyro] http  pyro ai   but have not used either of them  for this purpose  and am at a bit of a loss picking one to start with ",1635976141.0,2021-11-03 22:49:01,hi folks libraries toolboxes would recommend ideally based personal experience performing inference probabilistic graphical models actual practical application academic toy examples minimal requirements must able specify bayesian network factor graph consisting categorical nodes hidden others observed use set observations identify factors dependencies think essentially pgm toolbox fulfill requirements bonus points given good documentation maturity stability works efficiently many factors many datapoints implements many different inference methods modeling paradigms simplicity use know promising toolboxes [pgmpy] github com pgmpy pgmpy [pymc] docs pymc io en stable [pyro] http pyro ai used either purpose bit loss picking one start
"[R] Neural Program Generation Modulo Static Analysis, Mukherjee et al, 2021 NeurIPS Spotlight",4,qm5nfs,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qm5nfs/r_neural_program_generation_modulo_static/,1,"Nice paper   using program analysis as a learning signal for program synthesis

 I am not the author   alas  

[paper] https  www cs utexas edu ~swarat pubs neurips nsg pdf   

[twitter  ] https  twitter com swarat status  ",1635977358.0,2021-11-03 23:09:18,nice paper using program analysis learning signal program synthesis author alas [paper] www cs utexas edu ~swarat pubs neurips nsg pdf [twitter ] twitter com swarat status
[D] Zero-shot models as input features in NLP classification tasks?,6,qlxl01,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlxl01/d_zeroshot_models_as_input_features_in_nlp/,2,"So places like huggingface offer zero shot models that pretty decently as a zero shot classifier  and are easy to implement   I was thinking  what if I just took a few zero shot models and added them as features and then fit a classifier on top 

 So let s say that I m trying to classify toxic tweets  I might use an embedding model or a verctorizer to turn the tweet into model features  And then fit some classifier on top of this to try to predict the label from these text features  

But what if I add some simple zero shot binary features  is\_angry\_model  is\_sad\_model  is\_news model etc and do this for one or more models   Or even more simply use  is\_toxic\_model  is\_toxic\_model and use their predictions as input features into a classifier  

I was planning to experiment a bit with this  but wanted to get some thoughts on it and if there has been previous similar work I can use for reference 

Thanks ",1635955196.0,2021-11-03 16:59:56,places like huggingface offer zero shot models pretty decently zero shot classifier easy implement thinking took zero shot models added features fit classifier top let say trying classify toxic tweets might use embedding model verctorizer turn tweet model features fit classifier top try predict label text features add simple zero shot binary features is\_angry\_model is\_sad\_model is\_news model etc one models even simply use is\_toxic\_model is\_toxic\_model use predictions input features classifier planning experiment bit wanted get thoughts previous similar work use reference thanks
[D] AdaConv explained - Adaptive Convolutions for Structure-Aware Style Transfer (5-minute summary by Casual GAN Papers),1,qm5axp,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qm5axp/d_adaconv_explained_adaptive_convolutions_for/,0,"Classical style transfer is based on Adaptive Instance Normalization   which is limited to transferring statistical attributes such as color distribution and textures while ignoring local geometric structures in the image  But that is the stuff of the past  let me introduce to you  Adaptive Convolutions  a drop in replacement  for AdaIN  proposed by  Prashanth Chandran and the team at Disney research  AdaConv is able to transfer the structural styles along with colors and textures in real time 

Full summary  [https  t me casual\_gan ] https  t me casual_gan  

Blog post  [https  www casualganpapers com style conditioned image to image style transfer AdaConv explained html] https  www casualganpapers com style conditioned image to image style transfer AdaConv explained html 

[AdaConv] https  preview redd it jdzruhcgx png width= format=png auto=webp s=dbffabeeebeaabebcddbbefaf 

arxiv  [https  studios disneyresearch com app uploads   Adaptive Convolutions for Structure Aware Style Transfer pdf] https  studios disneyresearch com app uploads   Adaptive Convolutions for Structure Aware Style Transfer pdf 

code  [https  github com RElbers ada conv pytorch] https  github com RElbers ada conv pytorch 

Subscribe to [Casual GAN Papers] https  t me casual_gan  and follow me on [Twitter] https  twitter com KirillDemochkin  for weekly AI paper summaries ",1635976434.0,2021-11-03 22:53:54,classical style transfer based adaptive instance normalization limited transferring statistical attributes color distribution textures ignoring local geometric structures image stuff past let introduce adaptive convolutions drop replacement adain proposed prashanth chandran team disney research adaconv able transfer structural styles along colors textures real time full summary [ casual\_gan ] casual_gan blog post [ www casualganpapers com style conditioned image image style transfer adaconv explained html] www casualganpapers com style conditioned image image style transfer adaconv explained html [adaconv] preview redd jdzruhcgx png width= format=png auto=webp s=dbffabeeebeaabebcddbbefaf arxiv [ studios disneyresearch com app uploads adaptive convolutions structure aware style transfer pdf] studios disneyresearch com app uploads adaptive convolutions structure aware style transfer pdf code [ github com relbers ada conv pytorch] github com relbers ada conv pytorch subscribe [casual gan papers] casual_gan follow [twitter] twitter com kirilldemochkin weekly ai paper summaries
"[D] ""Learn AI Together"" Discord community is looking for experienced people to share their projects and willing to help other AI enthusiasts by answering questions from time to time",7,qlrxzz,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlrxzz/d_learn_ai_together_discord_community_is_looking/,2,"Hey everyone  We are looking for researchers  teachers  teacher assistants  or professionals willing to help and exchange with people learning AI by answering questions from time to time 

We are an AI focused community of over   people where members can chat  ask questions  share resources and projects  share find job offers  etc  We are now focusing on getting experts or advanced members to join us and help us help others  Everyone else in the field is welcome to join as well  

More info about the community and to join us  [Learn AI Together] https  www louisbouchard ai learn ai together 

Excited to chat with you there 

*Note that this is not a paid opportunity and we won t be asking for hours or anything  Help whenever you can    *",1635937066.0,2021-11-03 11:57:46,hey everyone looking researchers teachers teacher assistants professionals willing help exchange people learning ai answering questions time time ai focused community people members chat ask questions share resources projects share find job offers etc focusing getting experts advanced members join us help us help others everyone else field welcome join well info community join us [learn ai together] www louisbouchard ai learn ai together excited chat *note paid opportunity asking hours anything help whenever *
"[P] Loss function that penalizes classification errors heavily, or should I just modify log-loss?",2,qlujgx,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlujgx/p_loss_function_that_penalizes_classification/,12,"I m working on a classification problem where predictions that are  good enough  are  actually good enough   So I don t need my model to spend time optimizing a top    to become a   or a     

However  I really care when the model makes incorrect decisions   I ve built a family of models of different sizes parameter counts using traditional log loss as my loss function  and each model will make catastrophically bad decisions on occasion   And for my domain  a few catastrophically bad classifications can ruin tens of thousands of good classifications   I m wondering if there s a different loss function I can use that penalizes errors even more heavily than log loss  log loss squared   

The natural question to ask is if I have errors in my data pipeline or model configuration   I ve debugged it quite a bit and I m sure that s not my problem   My domain  chess  naturally has a manifold that is so complex and twisted with many sharp edges and saddle points  It s a really difficult space to work in so any model smaller than GPT  is understandably going to have trouble ",1635946346.0,2021-11-03 14:32:26,working classification problem predictions good enough actually good enough need model spend time optimizing top become however really care model makes incorrect decisions built family models different sizes parameter counts using traditional log loss loss function model make catastrophically bad decisions occasion domain catastrophically bad classifications ruin tens thousands good classifications wondering different loss function use penalizes errors even heavily log loss log loss squared natural question ask errors data pipeline model configuration debugged quite bit sure problem domain chess naturally manifold complex twisted many sharp edges saddle points really difficult space work model smaller gpt understandably going trouble
[N] Extended submission deadline — EvoMUSART 2022 conference,1,qm0upg,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qm0upg/n_extended_submission_deadline_evomusart_2022/,0,"Good news  **The submission deadline of EvoMUSART  has been extended to November th ** 🙌

You still have time to submit your work to the th International Conference on Artificial Intelligence in Music  Sound  Art and Design  EvoMUSART  

If you work with Artificial Intelligence techniques applied to visual art  music  sound synthesis  architecture  video  poetry  design or other creative tasks  don t miss the opportunity to submit your work to EvoMUSART 

EvoMUSART  will be held in **Seville  Spain**  between  and  April   💃🇪🇸

For more information  visit the conference webpage  [evostar org  evomusart ] http  www evostar org  evomusart 

https  preview redd it puenusafx png width= format=png auto=webp s=bbfabcedceabfa",1635964139.0,2021-11-03 19:28:59,good news **the submission deadline evomusart extended november th ** 🙌 still time submit work th international conference artificial intelligence music sound art design evomusart work artificial intelligence techniques applied visual art music sound synthesis architecture video poetry design creative tasks miss opportunity submit work evomusart evomusart held **seville spain** april 💃🇪🇸 information visit conference webpage [evostar org evomusart ] http www evostar org evomusart preview redd puenusafx png width= format=png auto=webp s=bbfabcedceabfa
[Research] Towards the Generalization of Contrastive Self-Supervised Learning,17,qllc70,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qllc70/research_towards_the_generalization_of/,0,"Some interesting results of contrastive learning theory  [https  arxiv org abs  ] https  arxiv org abs    

  The generalization ability of contrastive self supervised learning depends on  factors  strength of **data augmentations**  **alignment** of positive samples  and **divergence** of class centers 
  Data augmentation enables self supervised learning  Good algorithms  which optimize the alignment and divergence factors well  with weak data augmentation still have bad performance 
  Barlow Twins  which aims to decorrelate the different vector components of the representation  implicitly optimizes the geometry of embedding space to satisfy the alignment and divergence factors actually ",1635908768.0,2021-11-03 04:06:08,interesting results contrastive learning theory [ arxiv org abs ] arxiv org abs generalization ability contrastive self supervised learning depends factors strength **data augmentations** **alignment** positive samples **divergence** class centers data augmentation enables self supervised learning good algorithms optimize alignment divergence factors well weak data augmentation still bad performance barlow twins aims decorrelate different vector components representation implicitly optimizes geometry embedding space satisfy alignment divergence factors actually
"[N] Researchers From Seoul National University, NVIDIA and Microsoft Release ‘ACAV100M’: An Automatically Curated Video Dataset For Self-Supervised Audio-Visual Learning",4,qlqqow,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlqqow/n_researchers_from_seoul_national_university/,0,"Audio visual  AV  learning is defined by delivering and applying instructional content that includes both sound and visual information  The natural relationship between visual observations and their accompanying sounds has shown strong self supervision signals for learning video representations  That is why the massive amount of online videos has become a valuable source for self supervised learning among research communities  

However  due to overdubbed audio  online videos frequently provide imperfectly aligned audio visual signals  Therefore  the models trained on uncurated films have been shown to develop poorer representations as a result of the misalignment difficulties  The existing techniques typically rely on manually curated datasets with a predetermined taxonomy of semantic ideas  where the audio visual connection is highly likely 

To overcome this gap  researchers from Seoul National University  NVIDIA and Microsoft have released an automatic dataset curation pipeline and a large video dataset for self supervised audio visual learning  termed ACAVM  automatically curated audio visual dataset   The dataset is made up of a massive number of uncurated web videos  The researchers took  million full length videos and reduced them to  million segments with the best audio visual correspondence  

Checkout the [Paper] https  arxiv org pdf   pdf   [Codes] https  github com sangho vision acavm   [Project] https  acavm github io   [Microsoft Blog] https  www microsoft com en us research blog acavm scaling up self supervised audio visual learning with automatically curated internet videos   [Video Presentation] https  www youtube com watch v=VRehiVHQ  and a [Short Read from Us] https  www marktechpost com    researchers from seoul national university nvidia and microsoft release acavm an automatically curated video dataset for self supervised audio visual learning  _ga=       

 xB 

 xB 

https  preview redd it pklqyvmcx png width= format=png auto=webp s=cedadbddccdfadf",1635931558.0,2021-11-03 10:25:58,audio visual av learning defined delivering applying instructional content includes sound visual information natural relationship visual observations accompanying sounds shown strong self supervision signals learning video representations massive amount online videos become valuable source self supervised learning among research communities however due overdubbed audio online videos frequently provide imperfectly aligned audio visual signals therefore models trained uncurated films shown develop poorer representations result misalignment difficulties existing techniques typically rely manually curated datasets predetermined taxonomy semantic ideas audio visual connection highly likely overcome gap researchers seoul national university nvidia microsoft released automatic dataset curation pipeline large video dataset self supervised audio visual learning termed acavm automatically curated audio visual dataset dataset made massive number uncurated web videos researchers took million full length videos reduced million segments best audio visual correspondence checkout [paper] arxiv org pdf pdf [codes] github com sangho vision acavm [project] acavm github io [microsoft blog] www microsoft com en us research blog acavm scaling self supervised audio visual learning automatically curated internet videos [video presentation] www youtube com watch v=vrehivhq [short read us] www marktechpost com researchers seoul national university nvidia microsoft release acavm automatically curated video dataset self supervised audio visual learning _ga= xb xb preview redd pklqyvmcx png width= format=png auto=webp s=cedadbddccdfadf
[D] Raspberry Pi and ML Frameworks - any benchmarks for training or inference?,5,qlq3rt,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlq3rt/d_raspberry_pi_and_ml_frameworks_any_benchmarks/,1,Hey folks Do you know any benchmarks of available ML frameworks  TF  Pytorch     for **training**  toy data data sets  **and inference** on a **Raspberry Pi**  or other edge devices  ,1635928436.0,2021-11-03 09:33:56,hey folks know benchmarks available ml frameworks tf pytorch **training** toy data data sets **and inference** **raspberry pi** edge devices
"[D] Are GNNs/GCNs viable for graphs with no node features, with only the unique node IDs? Are they different from DeepWalk at that point?",2,qlqls4,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlqls4/d_are_gnnsgcns_viable_for_graphs_with_no_node/,3,"I started to dig into GNNs for the first time and I have trouble understanding its advantages over NLP inspired embedding methods like DeepWalk and nodevec  Do GNNs only shine with node features  Or can they handle IDs giant one hot vectors as well  Does the usual input for GNNs only consist of a vector of handcrafted features  Are GNNs used directly for tasks like link prediction or they are just embedding generators for other models 

I appreciate all and any explanations ",1635930857.0,2021-11-03 10:14:17,started dig gnns first time trouble understanding advantages nlp inspired embedding methods like deepwalk nodevec gnns shine node features handle ids giant one hot vectors well usual input gnns consist vector handcrafted features gnns used directly tasks like link prediction embedding generators models appreciate explanations
[R] Twitter Cortex Proposes LMSOC for Socially Sensitive Pretraining,0,qlwcgx,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlwcgx/r_twitter_cortex_proposes_lmsoc_for_socially/,1,"In the new paper LMSOC  An Approach for Socially Sensitive Pretraining  a Twitter Cortex research team proposes a simple but effective approach for learning both linguistically contextualized and socially sensitive representations in large scale language models  

Here is a quick read  [Twitter Cortex Proposes LMSOC for Socially Sensitive Pretraining ] https  syncedreview com    deepmind podracer tpu based rl frameworks deliver exceptional performance at low cost  

The LMSOC code is available on the project’s [Github] https  github com twitter research lmsoc   The paper *LMSOC  An Approach for Socially Sensitive Pretraining* is on [arXiv] https  arxiv org abs    ",1635951638.0,2021-11-03 16:00:38,new paper lmsoc approach socially sensitive pretraining twitter cortex research team proposes simple effective approach learning linguistically contextualized socially sensitive representations large scale language models quick read [twitter cortex proposes lmsoc socially sensitive pretraining ] syncedreview com deepmind podracer tpu based rl frameworks deliver exceptional performance low cost lmsoc code available project’s [github] github com twitter research lmsoc paper *lmsoc approach socially sensitive pretraining* [arxiv] arxiv org abs
[P] Text-to-image models ruDALL-E Kandinsky (XXL) (12 billion parameters) and ruDALL-E Malevich (XL) (1.3 billion parameters). A demo for the latter is available.,37,qlbye5,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlbye5/p_texttoimage_models_rudalle_kandinsky_xxl_12/,12,"[Technical report  Russian ] https  habr com ru company sberbank blog   

[Technical report  translated to English by Google Translate ] https  habr com translate goog ru company sberbank blog   _x_tr_sl=auto _x_tr_tl=en _x_tr_hl=en US _x_tr_pto=nui  

[Demo for ruDALL E Malevich  XL ] https  rudalle ru demo  

[GitHub repo for ruDALL E Malevich  XL ] https  github com sberbank ai ru dalle  

[More links from my other post] https  www reddit com r bigsleep comments qln comment hjdzo  ",1635880933.0,2021-11-02 20:22:13,[technical report russian ] habr com ru company sberbank blog [technical report translated english google translate ] habr com translate goog ru company sberbank blog _x_tr_sl=auto _x_tr_tl=en _x_tr_hl=en us _x_tr_pto=nui [demo rudall e malevich xl ] rudalle ru demo [github repo rudall e malevich xl ] github com sberbank ai ru dalle [more links post] www reddit com r bigsleep comments qln comment hjdzo
[D] Why do we apply batch normalization between layers,81,ql5hdb,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/ql5hdb/d_why_do_we_apply_batch_normalization_between/,25,After batch normalization we are basically trying to get the unit  gaussian output  Initialising the data with unit gaussian seems to be a  good idea but doing so in between the network  how does that make sense ,1635863442.0,2021-11-02 15:30:42,batch normalization basically trying get unit gaussian output initialising data unit gaussian seems good idea network make sense
[D]Using Perplexity for Evaluating Language Models,0,qlvjdy,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlvjdy/dusing_perplexity_for_evaluating_language_models/,1," 

Hello 

Assume a training dataset has ten sentences  and they are used for constructing a probabilistic language model consisting of the Maximum Likelihood Estimates for all of the different combinations of bigrams for these ten sentences   To test the accuracy of this bigram language model  a test dataset consisting of twenty sentences are used for calculating the probability for each of these test sentences using the bigram conditional probabilities that were calculated using all of the sentences in the training set   To evaluate the accuracy of this bigram language model  there is a need to calculate the Perplexity measure for indicating the accuracy of the probabilistic language model   Because of the test dataset having twenty sentences  is it correct to calculate the Perplexity value for each of these twenty sentences using their probability values and then taking the average of these twenty Perplexity values for calculating the overall Perplexity value for the test dataset or is it correct to calculate the probability for each of the twenty sentences and then find the simple average of these twenty probability values to give a simple probability average which is then used for calculating the Perplexity value for the test dataset   Would this procedure apply for a test dataset having any number of sentences   Would this procedure apply for a trigram model   Would this procedure apply for any n gram models like a quadrogram model   The goal is to always have a probabilistic language model having the lowest Perplexity value 

Thanks ",1635949300.0,2021-11-03 15:21:40,hello assume training dataset ten sentences used constructing probabilistic language model consisting maximum likelihood estimates different combinations bigrams ten sentences test accuracy bigram language model test dataset consisting twenty sentences used calculating probability test sentences using bigram conditional probabilities calculated using sentences training set evaluate accuracy bigram language model need calculate perplexity measure indicating accuracy probabilistic language model test dataset twenty sentences correct calculate perplexity value twenty sentences using probability values taking average twenty perplexity values calculating overall perplexity value test dataset correct calculate probability twenty sentences find simple average twenty probability values give simple probability average used calculating perplexity value test dataset would procedure apply test dataset number sentences would procedure apply trigram model would procedure apply n gram models like quadrogram model goal always probabilistic language model lowest perplexity value thanks
[R] Neurips 2021 Accepted Paper List,47,ql9d4s,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/ql9d4s/r_neurips_2021_accepted_paper_list/,8,"List of accepted papers now appears to be public  [https  neurips cc Conferences  AcceptedPapersInitial] https  neurips cc Conferences  AcceptedPapersInitial   


Spot any particularly interesting ones ",1635873997.0,2021-11-02 18:26:37,list accepted papers appears public [ neurips cc conferences acceptedpapersinitial] neurips cc conferences acceptedpapersinitial spot particularly interesting ones
[D] What is the state-of-the-art for few-shot text classification?,1,qlugqf,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlugqf/d_what_is_the_stateoftheart_for_fewshot_text/,6,Say I have many text snippets that can be one of four classes  I also cannot get a large scale labeled dataset  I have ~  labeled examples per class   What methods are currently state of the art for such settings ,1635946116.0,2021-11-03 14:28:36,say many text snippets one four classes also cannot get large scale labeled dataset ~ labeled examples per class methods currently state art settings
[D] Which apps in the real world would you like to connect your ML models with?,1,qlsp3s,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlsp3s/d_which_apps_in_the_real_world_would_you_like_to/,5,"Hi all  As a side project I am building a tool to **connect the ML models you make in your Jupyter Notebook to apps in the real world** with just a few lines of extra code 

As an example  think of building a model that predicts *Customer Churn* in a Jupyter Notebook  We make sure that it pulls new customers from **the Mailchimp account** of your company and **make it run as a Discord or Slack bot**  You can sign up for early access to the tool [here] https  flow magicsheets io  

I am building fast  and I wanted to ask **what apps** you think **would be awesome to integrate your Machine Learning models with** 

So far we have 

* Mailchimp
* Shopify
* Slack
* Google Sheets

Thanks in advance ",1635940037.0,2021-11-03 12:47:17,hi side project building tool **connect ml models make jupyter notebook apps real world** lines extra code example think building model predicts *customer churn* jupyter notebook make sure pulls new customers **the mailchimp account** company **make run discord slack bot** sign early access tool [here] flow magicsheets io building fast wanted ask **what apps** think **would awesome integrate machine learning models with** far * mailchimp * shopify * slack * google sheets thanks advance
[D] What do Machine Learning Engineers at Facebook do?,202,qkyini,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkyini/d_what_do_machine_learning_engineers_at_facebook/,152,I was approached by an interviewer for this position and had a hard time grasping what they work on on a day to day basis  Thanks in advance ,1635836173.0,2021-11-02 07:56:13,approached interviewer position hard time grasping work day day basis thanks advance
[D] Neural architecture search and Neuroevolution,9,qlffsv,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlffsv/d_neural_architecture_search_and_neuroevolution/,2,"Hi fellow readers 

I ve come recently to a slight confusion about how to understand NAS and Neuroevolution  This is why I would like to hear your explanations 

My current understanding is as follows 

**NAS** \  technique which can be used to automate the process of designing optimizing neural networks  NNs  \[[] https  lilianweng github io lil log    neural architecture search html evaluation strategy \]  The technique is further divided into three components such as 

* Search space   defines types of layers  depth  type of connections
* Search strategy   defines the approach used to explore search space
* Evaluation strategy   evaluates the performance of build ANN from its design

**Neuroevolution** \  a technique that harnesses evolutionary algorithms  EA  to design optimize NNs  It can augment the NN by changing the topology  connections and weights \[[] http  www scholarpedia org article Neuroevolution \] based on observed action in the environment 

Confusion 

Therefore  from the above description  I don t understand if NAS and Neuroevolution can be explained as applied techniques to design optimize NN topologies  Where NAS can use any kind of algorithm to design NN topologies  RL  EA  Gradient descent  and Neuroevolution uses only EA 

**OR**

Neuroevolution can be just defined as a search strategy in NAS  This means Neuroevolution can be used in NAS  search strategy  like for example Reinforcement learning  RL  \[[] https  arxiv org abs   \] 

To simplify  I would like to understand  how can I think about NAS and Neuroevolution  when researching  My goal is to understand how I can put all the puzzles together when building an automated machine learning process for a specific task as anomaly detection  

If I made any mistakes or silly comparisons please point them out in the comments  that future readers can grasp my mistakes and your knowledge  Any comments are more than welcome 

Thank you in advance ",1635890550.0,2021-11-02 23:02:30,hi fellow readers come recently slight confusion understand nas neuroevolution would like hear explanations current understanding follows **nas** \ technique used automate process designing optimizing neural networks nns \[[] lilianweng github io lil log neural architecture search html evaluation strategy \] technique divided three components * search space defines types layers depth type connections * search strategy defines approach used explore search space * evaluation strategy evaluates performance build ann design **neuroevolution** \ technique harnesses evolutionary algorithms ea design optimize nns augment nn changing topology connections weights \[[] http www scholarpedia org article neuroevolution \] based observed action environment confusion therefore description understand nas neuroevolution explained applied techniques design optimize nn topologies nas use kind algorithm design nn topologies rl ea gradient descent neuroevolution uses ea **or** neuroevolution defined search strategy nas means neuroevolution used nas search strategy like example reinforcement learning rl \[[] arxiv org abs \] simplify would like understand think nas neuroevolution researching goal understand put puzzles together building automated machine learning process specific task anomaly detection made mistakes silly comparisons please point comments future readers grasp mistakes knowledge comments welcome thank advance
[R] Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey,2,qlkg63,MachineLearning,https://arxiv.org/abs/2111.01243,1,nan,1635905816.0,2021-11-03 03:16:56,nan
[D] Did anyone check ykilcher's video of Siraj Raval's interview,52,qkyyno,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkyyno/d_did_anyone_check_ykilchers_video_of_siraj/,60,I love Yannic s video but I did not see any point of this interview  I mean even in the interview Siraj seemed like someone who has just started learning machine learning  when he mentions about  Superintelligence digital organism god   seems like he imagines ML as a hollywood movie  much like the general person  ,1635838260.0,2021-11-02 08:31:00,love yannic video see point interview mean even interview siraj seemed like someone started learning machine learning mentions superintelligence digital organism god seems like imagines ml hollywood movie much like general person
[P] natural language only coding with co-pilot stream 11/2 10PM PST,1,qliy7s,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qliy7s/p_natural_language_only_coding_with_copilot/,2,"Kinda late to the party  but I just got access to github s copilot AI backed code auto complete tool  it can do some pretty impressive things  I have not played with it for more than an hour  and I m pretty impressed 

At PM PST today  I will be streaming at [twitch tv evanthebouncy] https  twitch tv evanthebouncy  for about   hours  where I will be attempting to perform simple coding exercises by writing \_only comments\_ and letting co pilot complete the code from my natural language inputs  It ll be fun if you can come and spam some ideas in case you haven t had a chance to play with it yet 

I will also be giving some commentary   reactions to it  as I work in program synthesis for a living  and this is a pretty cool piece of tech that will definitely change how people think about programming in the near future 

mod  if this is kinda spammy feel free to just delete the thread  idc ",1635900997.0,2021-11-03 01:56:37,kinda late party got access github copilot ai backed code auto complete tool pretty impressive things played hour pretty impressed pm pst today streaming [twitch tv evanthebouncy] twitch tv evanthebouncy hours attempting perform simple coding exercises writing \_only comments\_ letting co pilot complete code natural language inputs fun come spam ideas case chance play yet also giving commentary reactions work program synthesis living pretty cool piece tech definitely change people think programming near future mod kinda spammy feel free delete thread idc
[D] What's the best simple machine learning API service?,2,qlcj9r,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlcj9r/d_whats_the_best_simple_machine_learning_api/,3,I m looking integrate machine learning into my application  for example  I want to classify images users are uploading to my site   I m aware there s a lot of these SAAS machine learning companies but I was wondering if anyone here had recommendations as to which ones worked best for them  I basically just want to send all my data to a service  train a model  the be able to call an API to get an answer from the model ,1635882524.0,2021-11-02 20:48:44,looking integrate machine learning application example want classify images users uploading site aware lot saas machine learning companies wondering anyone recommendations ones worked best basically want send data service train model able call api get answer model
[D] AAAI FastTrack 2021 Review Results,29,qkxatt,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkxatt/d_aaai_fasttrack_2021_review_results/,34,Good luck everyone  Results gonna be out soon for AAAI  ,1635830970.0,2021-11-02 06:29:30,good luck everyone results gonna soon aaai
[D] Why hasn't BERT been scaled up/trained on a massive dataset like GPT3?,129,qklvfp,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qklvfp/d_why_hasnt_bert_been_scaled_uptrained_on_a/,41,Both architectures can be trained completely unsupervised  so why has GPT been scaled up and not BERT  Is it a software limitation ,1635795508.0,2021-11-01 20:38:28,architectures trained completely unsupervised gpt scaled bert software limitation
[P] Scientific Literature Review generation,7,ql2qjz,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/ql2qjz/p_scientific_literature_review_generation/,4," Hello everyone 

I ve developed an algorithm to automatically generate a literature review   [https  www naimai fr] https  www naimai fr   
Hopefully that could be useful for the PhDs  and the non PhDs   

For those curious to understand how it works   [https  yaassinekaddi medium com scientific literature review generation fbeae] https  yaassinekaddi medium com scientific literature review generation fbeae 

I ll be thankful if you have any remarks about that   

Cheers ",1635854811.0,2021-11-02 13:06:51,hello everyone developed algorithm automatically generate literature review [ www naimai fr] www naimai fr hopefully could useful phds non phds curious understand works [ yaassinekaddi medium com scientific literature review generation fbeae] yaassinekaddi medium com scientific literature review generation fbeae thankful remarks cheers
[R] Unsolved Problems in ML Safety,16,qkxtjo,MachineLearning,https://arxiv.org/abs/2109.13916,1,nan,1635833155.0,2021-11-02 07:05:55,nan
[R] Sequence-to-Sequence Learning with Latent Neural Grammars,11,qkylaz,MachineLearning,https://arxiv.org/abs/2109.01135,1,nan,1635836498.0,2021-11-02 08:01:38,nan
[D] how to correlate the results from an extremely imbalance data to performance relative to a random guess,6,qkwk6j,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkwk6j/d_how_to_correlate_the_results_from_an_extremely/,17,"Hi all 

At a project at work I handle with an extremely imbalance dataset    cases of positive outcome while around   negative cases 

In addition  our cases are relatively hard to separate and in some cases domain knowledge experts are struggling with manual classification 

In that context  I was asked to explore different classifiers and present them in a POC report 

At first I tried a naive approach and dumped the data as it is  used a train  validation  test splitting with stratification option on   All the models predicted  all the time  to maximize accuracy 
Then  I used over sampling with smote package in python  and changed the criteria to the area under the precision recall curve  In the text set  I predicted correctly       and I had a false positive rate of around   
Regardless of possible model modification or boundary  predict  if the model predict a higher score than    for example   I am having some problems in defining my metric to evaluate the results  Our data is imbalanced  so   is good  In addition  false negative is strongly worse than false positive 

In addition  today I presented the results to my manager and he asked me to prove him  or argue  that the results are better than random guessing  

I thought about two things to evaluate my results 
  Randomly draw  observations  with a prior of   equal to   the rest to   Than randomly guess   of them to be  and compare it to my results  Bootstrap this scheme to get a distribution  and check where is my model performance along the distribution   

 Take the examples from the test set  assign the same number or normal observations  and give the hr experts to classify them manually  Then compare results 

I will be glad to hear what do you think about the problem and the suggestions 

Thanks   ",1635828087.0,2021-11-02 05:41:27,hi project work handle extremely imbalance dataset cases positive outcome around negative cases addition cases relatively hard separate cases domain knowledge experts struggling manual classification context asked explore different classifiers present poc report first tried naive approach dumped data used train validation test splitting stratification option models predicted time maximize accuracy used sampling smote package python changed criteria area precision recall curve text set predicted correctly false positive rate around regardless possible model modification boundary predict model predict higher score example problems defining metric evaluate results data imbalanced good addition false negative strongly worse false positive addition today presented results manager asked prove argue results better random guessing thought two things evaluate results randomly draw observations prior equal rest randomly guess compare results bootstrap scheme get distribution check model performance along distribution take examples test set assign number normal observations give hr experts classify manually compare results glad hear think problem suggestions thanks
Plagiarism Case Detected @ ICLR 2022 [News][Discussion],168,qkb6ga,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkb6ga/plagiarism_case_detected_iclr_2022_newsdiscussion/,27,"[https  openreview net forum id=EOVJGAllb noteId=KsTmTUsyXa] https  openreview net forum id=EOVJGAllb noteId=KsTmTUsyXa 

 xB 

https  preview redd it zwpndspxpyw png width= format=png auto=webp s=ddfdaedafbcefcbacacef

The submission was withdrawn by the authors before the Program Chairs posted a desk reject citing a serious case of plagiarism  What is happening 

The figures and tables do look like they ve been lifted straight from previous papers ",1635763106.0,2021-11-01 11:38:26,[ openreview net forum id=eovjgallb noteid=kstmtusyxa] openreview net forum id=eovjgallb noteid=kstmtusyxa xb preview redd zwpndspxpyw png width= format=png auto=webp s=ddfdaedafbcefcbacacef submission withdrawn authors program chairs posted desk reject citing serious case plagiarism happening figures tables look like lifted straight previous papers
[D] How to generate images from text with CLIP + VQGAN (easy to follow 5-minute tutorial by Casual GAN Papers),11,qksq6q,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qksq6q/d_how_to_generate_images_from_text_with_clip/,0,"[Cartoon village in a mushroom valley trending on ArtStation] https  preview redd it gmcbyvfx png width= format=png auto=webp s=fbdffbdbcefdebbefdbbd 

Hey everyone 

Have you been playing with GANs for a while and want to create something yourself  Do you want to try out those text to image google colabs for generative art you have seen on Twitter but are not sure where to get started  Then this tutorial is for you 

My name is Kirill  and I have been writing weekly ML paper summaries for almost  months over at [casualganpapers com] https  casualganpapers com   and while they are helpful to a lot of people already working in the generative modeling field  I realized the digests are not as interesting to those just starting their generative AI journey 

This is why I am starting a new series of posts focused on quickly getting you started in the world of generative art  D  AI based image editing  and more  

Check out the first post on how to use the popular CLIP   VQGAN colabs to create beautiful generative art in just   minutes  excluding the training time  

[https  www casualganpapers com howto clip vqgan text guided image generation explained VQGAN CLIP tutorial html] https  www casualganpapers com howto clip vqgan text guided image generation explained VQGAN CLIP tutorial html 

If you enjoyed the tutorial make sure to follow Casual GAN Papers on telegram  [https  t me casual\_gan] https  t me casual_gan  or Twitter  [https  twitter com KirillDemochkin] https  twitter com KirillDemochkin  to get notified when the next post is released 

Take care   
Kirill",1635815260.0,2021-11-02 02:07:40,[cartoon village mushroom valley trending artstation] preview redd gmcbyvfx png width= format=png auto=webp s=fbdffbdbcefdebbefdbbd hey everyone playing gans want create something want try text image google colabs generative art seen twitter sure get started tutorial name kirill writing weekly ml paper summaries almost months [casualganpapers com] casualganpapers com helpful lot people already working generative modeling field realized digests interesting starting generative ai journey starting new series posts focused quickly getting started world generative art ai based image editing check first post use popular clip vqgan colabs create beautiful generative art minutes excluding training time [ www casualganpapers com howto clip vqgan text guided image generation explained vqgan clip tutorial html] www casualganpapers com howto clip vqgan text guided image generation explained vqgan clip tutorial html enjoyed tutorial make sure follow casual gan papers telegram [ casual\_gan] casual_gan twitter [ twitter com kirilldemochkin] twitter com kirilldemochkin get notified next post released take care kirill
[R] Top 7 Books to Boost Your Data Driven Outlook,179,qkes8a,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkes8a/r_top_7_books_to_boost_your_data_driven_outlook/,13,"**In this post  I will cover the best  books for data analysts  These data analytics books will teach you about the power of big data and ways to harness it **

I started my career as a Software Developer and switched to data science  years ago when big data software projects were difficult to predict and risky to conduct due to large volumes of unclassified data and many types of metrics  Using machine learning  data analysis  and visualization approaches was essential for facilitating informed decision making throughout the software development and testing process 

Mastering data analysis was one of the most challenging experiences in my life  Wading through tons of books to figure out where to start and which methods and techniques to use in a particular case can be extremely daunting and time consuming 

If you have been studying data analytics for some time  choosing the right educational resources is crucial to launching and advancing your career within this area   

     Storytelling with Data  A Data Visualization Guide for Business Professionals

As a data analyst  your aim is not just to retrieve data but also to make it intelligible  which requires you to be able to present the data in a certain way   However  presenting data does not imply dragging and dropping data fields into a chart  It entails creating a meaningful visual representation of the data  This book is based on real life scenarios and will give you some idea on the difference between colorful visualization and intelligent visualization  explaining why you should closely examine each line and color on your visual interface 

This book provides excellent guidance  examines criteria  and presents examples of how to properly deal with data 

    Mastering Tableau   Implement advanced business intelligence techniques and analytics with Tableau  rd Edition  

As a business analytics practitioner  I search for publications that can simplify complicated topics in a manner that everyone can understand  The book contains several tips and techniques that will assist you in understanding when to utilize particular chart styles  at what data granularity  and with what sort of presentation for the end user  You will begin this fascinating trip by learning essential strategies for using sophisticated math to tackle challenging situations  These strategies involve the inventive use of several sorts of computations  such as row level  aggregate level  and others  Besides  you will get concise instructions on using Tableau to solve practically any data visualization problem by knowing the tool s  inner workings and thinking creatively about possibilities   expanded capabilities 

After reading the book  you will be equipped with an arsenal of advanced chart types and methods that will allow you to display information to a range of audiences in an effective and engaging manner using clear  efficient  and engaging dashboards  Explanations and examples of effective and inefficient visualization approaches  well planned and badly created dashboards  and compromise choices when Tableau users do not embrace data visualization  will expand your knowledge of Tableau  so that you get the most of this powerful tool 

    Machine Learning with the Elastic Stack   Second Edition  

This book is a one of a kind resource for users using Elastic search  I With actual case  it focuses on the substantial growth of machine learning technology in Elastic search providing actual case studies and extensive explanation  This book is similar to having a one on one conversation with a subject matter expert  If you need to refresh your practical skills in machine learning  the book offers examples of how to apply Elastic ML in your environment  get valuable insight into your data  and how you can turn machine learning from static to intelligent  If you want to understand not just how to build tasks but also tap into the underlying models and variables  Machine Learning with the Elastic Stack is the ideal option for you  

    Data Analytics Made Easy  Analyze and present data to make informed decisions without writing any code

With data literacy being such an important component of a data driven mindset  this book is an excellent resource for data science students looking to obtain practical information and learn how to apply their analytical skills  The author does an excellent job of introducing readers to KNIME  a low code data analytics framework that allows to instantly evaluate data  Furthermore  his presentation of machine learning is user friendly  with an emphasis on theoretical knowledge and handling a variety of use cases  More significantly  De Mauro assists readers in comprehending the significance  of becoming a great data presenter  a vital talent to cultivate in order to influence decision making 

    Fundamentals of Machine Learning for Predictive Data Analytics  Second Edition  Algorithms  Worked Examples  and Case Studies  

Fundamentals of Machine Learning for Predictive Data Analytics is a detailed analysis of the most important machine learning methods used in predictive data analytics  encompassing both theoretical principles and actual implementations  Technical and mathematical knowledge is complemented with instructional practical examples  and case studies show how these models may be employed in a wider business setting 

Following a description of the journey from extracting data to gaining insights and making a prediction  the book delves into the most essential machine learning techniques  data based learning  correlation based learning  probability and error based learning  Each of these strategies starts with a no  tech description of the core principle  followed by quantitative models and algorithms demonstrated with extensive practical examples 

The authors discuss the procedures in a straightforward and succinct way  without referring to any specific programming frameworks or languages  They do a fantastic job of introducing the main concepts before diving deeper into the complexities of the logic and math underpinning the algorithms 

     Analytics Stories  Using Data to Make Good Things Happen

Analytics Stories  How to Make Good Things Happen is a serious  intelligent  and entertaining look at how analytics can tackle real world problems and situations  Analytics Stories fills the gap between data analytics and the particular challenges it solves  with topics ranging from sports to finance  politics  healthcare  and commerce 

The author does an outstanding job of conveying the notion of data storytelling to the reader  He develops around  business cases on topics ranging from education to sports  Dr  Winston mostly utilized MS Excel to interpret  analyze  display  and successfully convey the data 

     Data Pipelines Pocket Reference  Moving and Processing Data for Analytics

A data science pipeline is a set of procedures that transform raw data into meaningful business responses  Data science pipelines streamline data validation  extract  transform  load machine learning and modeling  revision  so their implementation is crucial for data analytics success  The difference between having data and truly deriving value from it is moving data from various sources and processing it to create context 

This helpful reference describes common pipeline failures and key decision factors like batches vs  streaming data input and building vs  purchasing  This book delves into fundamental concepts that apply to open source systems  consumer applications  and homegrown solutions  as well as the most common decisions made by experts 

Data Pipelines Pocket Reference is a precious resource for all of the everyday problems and activities you are likely to encounter  if you work in data analysis or a related field that will assist you in making data driven decisions for many years to come 

  Conclusion

Having a thorough grasp of data analytics and knowing how to gain actionable data driven insights are essential for a successful career in data science  Anyone interested in expanding their knowledge of data analytics can benefit from the books mentioned in this article  since they provide the most recent industry information illustrated by examples of best practices ",1635775707.0,2021-11-01 15:08:27,**in post cover best books data analysts data analytics books teach power big data ways harness ** started career software developer switched data science years ago big data software projects difficult predict risky conduct due large volumes unclassified data many types metrics using machine learning data analysis visualization approaches essential facilitating informed decision making throughout software development testing process mastering data analysis one challenging experiences life wading tons books figure start methods techniques use particular case extremely daunting time consuming studying data analytics time choosing right educational resources crucial launching advancing career within area storytelling data data visualization guide business professionals data analyst aim retrieve data also make intelligible requires able present data certain way however presenting data imply dragging dropping data fields chart entails creating meaningful visual representation data book based real life scenarios give idea difference colorful visualization intelligent visualization explaining closely examine line color visual interface book provides excellent guidance examines criteria presents examples properly deal data mastering tableau implement advanced business intelligence techniques analytics tableau rd edition business analytics practitioner search publications simplify complicated topics manner everyone understand book contains several tips techniques assist understanding utilize particular chart styles data granularity sort presentation end user begin fascinating trip learning essential strategies using sophisticated math tackle challenging situations strategies involve inventive use several sorts computations row level aggregate level others besides get concise instructions using tableau solve practically data visualization problem knowing tool inner workings thinking creatively possibilities expanded capabilities reading book equipped arsenal advanced chart types methods allow display information range audiences effective engaging manner using clear efficient engaging dashboards explanations examples effective inefficient visualization approaches well planned badly created dashboards compromise choices tableau users embrace data visualization expand knowledge tableau get powerful tool machine learning elastic stack second edition book one kind resource users using elastic search actual case focuses substantial growth machine learning technology elastic search providing actual case studies extensive explanation book similar one one conversation subject matter expert need refresh practical skills machine learning book offers examples apply elastic ml environment get valuable insight data turn machine learning static intelligent want understand build tasks also tap underlying models variables machine learning elastic stack ideal option data analytics made easy analyze present data make informed decisions without writing code data literacy important component data driven mindset book excellent resource data science students looking obtain practical information learn apply analytical skills author excellent job introducing readers knime low code data analytics framework allows instantly evaluate data furthermore presentation machine learning user friendly emphasis theoretical knowledge handling variety use cases significantly de mauro assists readers comprehending significance becoming great data presenter vital talent cultivate order influence decision making fundamentals machine learning predictive data analytics second edition algorithms worked examples case studies fundamentals machine learning predictive data analytics detailed analysis important machine learning methods used predictive data analytics encompassing theoretical principles actual implementations technical mathematical knowledge complemented instructional practical examples case studies show models may employed wider business setting following description journey extracting data gaining insights making prediction book delves essential machine learning techniques data based learning correlation based learning probability error based learning strategies starts tech description core principle followed quantitative models algorithms demonstrated extensive practical examples authors discuss procedures straightforward succinct way without referring specific programming frameworks languages fantastic job introducing main concepts diving deeper complexities logic math underpinning algorithms analytics stories using data make good things happen analytics stories make good things happen serious intelligent entertaining look analytics tackle real world problems situations analytics stories fills gap data analytics particular challenges solves topics ranging sports finance politics healthcare commerce author outstanding job conveying notion data storytelling reader develops around business cases topics ranging education sports dr winston mostly utilized ms excel interpret analyze display successfully convey data data pipelines pocket reference moving processing data analytics data science pipeline set procedures transform raw data meaningful business responses data science pipelines streamline data validation extract transform load machine learning modeling revision implementation crucial data analytics success difference data truly deriving value moving data various sources processing create context helpful reference describes common pipeline failures key decision factors like batches vs streaming data input building vs purchasing book delves fundamental concepts apply open source systems consumer applications homegrown solutions well common decisions made experts data pipelines pocket reference precious resource everyday problems activities likely encounter work data analysis related field assist making data driven decisions many years come conclusion thorough grasp data analytics knowing gain actionable data driven insights essential successful career data science anyone interested expanding knowledge data analytics benefit books mentioned article since provide recent industry information illustrated examples best practices
[D] To phd or not to phd?,33,qkfuzn,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkfuzn/d_to_phd_or_not_to_phd/,49,"I made the following post on other subs too  Just posting it here to get the input from larger machine learning community 

Hi all 

I recently completed my research based masters in computer vision and currently working in a company as a computer vision researcher  My current role requires a lot of paper reading to improve the existing models  I really like doing research and am satisfied with my current role  I have the following questions 
  If I decide to pursue a phd  I will not be able to save money for next  to  years  Which is better  years of PhD or  years of research job experience 
  My long term goal is to get a job in big companies like google and Facebook  Most of the computer vision roles in big companies require a phd with multiple publications  Can i join such companies without a phd 
  My company encourages publishing papers  Let’s say if I publish some papers in next three to four years  would that help me in competing with phd degree holders  Or I would still need an official degree 
  How hard is to get admission in a good uni after some years of research experience with no publication record 

I would be thankful if someone could comment on my questions ",1635778867.0,2021-11-01 16:01:07,made following post subs posting get input larger machine learning community hi recently completed research based masters computer vision currently working company computer vision researcher current role requires lot paper reading improve existing models really like research satisfied current role following questions decide pursue phd able save money next years better years phd years research job experience long term goal get job big companies like google facebook computer vision roles big companies require phd multiple publications join companies without phd company encourages publishing papers let’s say publish papers next three four years would help competing phd degree holders would still need official degree hard get admission good uni years research experience publication record would thankful someone could comment questions
[R] Can ICRA reviewer see the names of author,0,qkzi4q,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkzi4q/r_can_icra_reviewer_see_the_names_of_author/,8,I submitted a paper to ICRA  but I forgot to put the names of the ourselves on the paper  Question  can the reviewer still see the author names through the system ,1635840752.0,2021-11-02 09:12:32,submitted paper icra forgot put names paper question reviewer still see author names system
[R] Any movie dataset with movie summaries?,3,qkvy6l,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkvy6l/r_any_movie_dataset_with_movie_summaries/,2,"Do you know of a dataset that contains movie summaries 

Do you know if researchers are legally allowed to download IMDB movie summaries for research purposes ",1635825819.0,2021-11-02 05:03:39,know dataset contains movie summaries know researchers legally allowed download imdb movie summaries research purposes
[D] why isn't converting ML Models to plain code trivial?,0,ql0a61,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/ql0a61/d_why_isnt_converting_ml_models_to_plain_code/,27,"I ve only done across one project   mcgen   for converting ml models to plain code 
Given that even complex models can be broken down to a series of nested functions  why is this not more commonly done 

Yes  training is very complex  but inference is just passing the input through  it s nothing dynamic  Sure  the performance will suffer  but for non streaming applications it should be fine  Even a complex classification network isn t going to take long to run inference 

The Frameworks already parse the graph  or pipeline  or whatever it is they use to matrix multiplications  so why not export a plain code version   I know it s not the same  but it s probably much easier for the framework Devs to implement this rather than someone external 

It ll take a bit of doing  but having completely portable computer code  with no hosts  model serialisation etc  Seems like a good thing 

As you might imagine  I m thinking of how to make portable models for integrating with local software  a game engine   with as little hassle as possible ",1635844540.0,2021-11-02 10:15:40,done across one project mcgen converting ml models plain code given even complex models broken series nested functions commonly done yes training complex inference passing input nothing dynamic sure performance suffer non streaming applications fine even complex classification network going take long run inference frameworks already parse graph pipeline whatever use matrix multiplications export plain code version know probably much easier framework devs implement rather someone external take bit completely portable computer code hosts model serialisation etc seems like good thing might imagine thinking make portable models integrating local software game engine little hassle possible
[D] How does tensorflow perform on M1 Pro/Max?,14,qkfgxj,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkfgxj/d_how_does_tensorflow_perform_on_m1_promax/,13,Basically the title  Apple claims that tensorflow is optimized native for M chips  but how does it actually perform ,1635777741.0,2021-11-01 15:42:21,basically title apple claims tensorflow optimized native chips actually perform
[Project] These plants do not exist - Using StyleGan2,1223,qjpcut,MachineLearning,https://v.redd.it/jxy5m9bvcsw71,26,nan,1635686392.0,2021-10-31 14:19:52,nan
[D] How is MLOps done in your current workplace?,68,qk5avf,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qk5avf/d_how_is_mlops_done_in_your_current_workplace/,26,"I joined a startup recently where the the necessary backend to support ML deployment is pretty much non existent  All we have are some simple templates for CI CD modified from those designed for generic microservices  Currently it takes data scientists at least   working days  post R D  for to put a model into production as a prediction end point with logging and observability  This excludes setting up the necessary data pipelines between the models and other backend services  Whole process can take as long as  weeks  

My team and I are looking into setting up some framework and automation to cut the turn around time for putting models into production  Trying to establish some reasonable goals for this project and hope to get some insight from others have been through the same 

 xB 

* Which part of the production processes are automated by your MLOps teams and tools 
* How much effort do these tools help save and how much time does it currently take to put up a piece of R D work into production ",1635736845.0,2021-11-01 04:20:45,joined startup recently necessary backend support ml deployment pretty much non existent simple templates ci cd modified designed generic microservices currently takes data scientists least working days post r put model production prediction end point logging observability excludes setting necessary data pipelines models backend services whole process take long weeks team looking setting framework automation cut turn around time putting models production trying establish reasonable goals project hope get insight others xb * part production processes automated mlops teams tools * much effort tools help save much time currently take put piece r work production
[D] What tools exist to determine the most useful type for perdiction?,1,qksrhl,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qksrhl/d_what_tools_exist_to_determine_the_most_useful/,5,"I ve messed around with IBM s and Google s AutoML frameworks  I can t remember which output a report of which data was most helpful in predictions 

If I m not using the correct terminology  I m sorry  Basically  if I train an AI model on data such as Car Steering Angle  Gyro  Acceleramotor  speed  etc  and ground truth it to a more precise car steering angle  I want to figure out which of these data types were most useful for a good prediction 

That way I can feed in a whole lot of data  train the model  and know which data sources are irrelevant  What tools exist out there for this ",1635815370.0,2021-11-02 02:09:30,messed around ibm google automl frameworks remember output report data helpful predictions using correct terminology sorry basically train ai model data car steering angle gyro acceleramotor speed etc ground truth precise car steering angle want figure data types useful good prediction way feed whole lot data train model know data sources irrelevant tools exist
TARS: Task-Aware Representation of Sentences for Generic Text Classification (Paper Summary) [D],7,qkdfwe,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkdfwe/tars_taskaware_representation_of_sentences_for/,4,"State of the art approaches for text classification leverage a transformer architecture with a linear layer on top that outputs a class distribution for a given prediction problem  

While effective  this approach suffers from conceptual limitations that affect its utility in few shot or zero shot transfer learning scenarios 🔥

 
This paper proposes a novel formulation of text classification that addresses these limitations 

https  youtu be XTacdzVRHM

Paper  https  aclanthology org  coling main  ",1635771542.0,2021-11-01 13:59:02,state art approaches text classification leverage transformer architecture linear layer top outputs class distribution given prediction problem effective approach suffers conceptual limitations affect utility shot zero shot transfer learning scenarios 🔥 paper proposes novel formulation text classification addresses limitations youtu xtacdzvrhm paper aclanthology org coling main
[R] Music Source Separation in the Waveform Domain,55,qk1kp6,MachineLearning,https://v.redd.it/k29qmktcgvw71,2,nan,1635723558.0,2021-11-01 00:39:18,nan
100Circles - Words to Paintings via NightCafe VQGAN+CLIP [Project],474,qjn0vg,MachineLearning,https://v.redd.it/rjdmkmbmjrw71,29,nan,1635677190.0,2021-10-31 11:46:30,nan
[R] BERMo: What can BERT learn from ELMo?,3,qkewlk,MachineLearning,https://arxiv.org/abs/2110.15802,1,nan,1635776077.0,2021-11-01 15:14:37,nan
[Project] BERT Tokenizers NuGet Package for C#,9,qk90b9,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qk90b9/project_bert_tokenizers_nuget_package_for_c/,0," xB 

https  preview redd it bzjuquuxw png width= format=png auto=webp s=effdfdaacbeecccfe

 

Inspired by the challenges I faced with using BERT models with ML NET  I have built a small open source project and NuGet Package for easy tokenization in C  🚀

With this package  you don t have to worry about different vocabularies and you can build input for BERT models quicker 

👉GitHub  [https  github com NMZivkovic BertTokenizers] https  github com NMZivkovic BertTokenizers 

👉Blog Post  [https  rubikscode net    bert tokenizers for ml net ] https  rubikscode net    bert tokenizers for ml net ",1635752632.0,2021-11-01 08:43:52,xb preview redd bzjuquuxw png width= format=png auto=webp s=effdfdaacbeecccfe inspired challenges faced using bert models ml net built small open source project nuget package easy tokenization c 🚀 package worry different vocabularies build input bert models quicker 👉github [ github com nmzivkovic berttokenizers] github com nmzivkovic berttokenizers 👉blog post [ rubikscode net bert tokenizers ml net ] rubikscode net bert tokenizers ml net
[D] Does cuda latest version support all version of pytorch and tensorflow,4,qka6p0,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qka6p0/d_does_cuda_latest_version_support_all_version_of/,3,"Greetings  sorry i could not think a better place to ask this question where i can get response regarding pytorch  tensorflow and cuda version 

I want to to know if i install cuda    will it support lower version tensorflow or torch packages such as tensorflow   or pytorch   which is supported by   cuda version  Actually i want to install both tensorflow and pytorch  The best option is to install cuda   or    but i want to know can i install latest version of cuda and whether will it support both frame works ",1635758557.0,2021-11-01 10:22:37,greetings sorry could think better place ask question get response regarding pytorch tensorflow cuda version want know install cuda support lower version tensorflow torch packages tensorflow pytorch supported cuda version actually want install tensorflow pytorch best option install cuda want know install latest version cuda whether support frame works
[D] Is there a good guide/roadmap on Deeplearning with large Datasets in Clouds?,1,qkef2i,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkef2i/d_is_there_a_good_guideroadmap_on_deeplearning/,4,"Is there a good guide roadmap on Deeplearning with large Datasets in Clouds 

I have around  GB of data in  npy format to feed into a tensorflow pipeline  
Preprocessing itself takes a few hours too   Or should i completely do that offline and change the pipeline structure  ",1635774637.0,2021-11-01 14:50:37,good guide roadmap deeplearning large datasets clouds around gb data npy format feed tensorflow pipeline preprocessing takes hours completely offline change pipeline structure
[D] NLP model for chatbot for inference on 11 GB GPU?,4,qkbfst,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkbfst/d_nlp_model_for_chatbot_for_inference_on_11_gb_gpu/,5,"Hello everybody

I’ve just found the amazing Huggingface library  It is an awesome piece of work 

I would like to train a chatbot on some existing dataset or several datasets  e g  the Pile   For training  or fine tuning  the model I have no GPU memory limitations   GB GPU is available   For inference  I only have a GPU with  GB available  Inference should be feasible in real time  i e  below around  seconds  and the model should be adjustable  i e  the source code should be available to change the structure of the model 

What model is best when taking into account these requirements  Probably one of the best models is GPT J but I think for inference it needs more than  GB GPU 

Are the models in the Huggingface library fully customizable  i e  layers etc   ",1635764232.0,2021-11-01 11:57:12,hello everybody i’ve found amazing huggingface library awesome piece work would like train chatbot existing dataset several datasets e g pile training fine tuning model gpu memory limitations gb gpu available inference gpu gb available inference feasible real time e around seconds model adjustable e source code available change structure model model best taking account requirements probably one best models gpt j think inference needs gb gpu models huggingface library fully customizable e layers etc
[D] Has anyone else received an e-mail for the ICLR review?,1,qkf6mt,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkf6mt/d_has_anyone_else_received_an_email_for_the_iclr/,0,I got an e mail from openreview with a single review of the paper  I went to the openreview website to see that it was deleted  Anyone else with a similar experience ,1635776907.0,2021-11-01 15:28:27,got e mail openreview single review paper went openreview website see deleted anyone else similar experience
[D] How does AI fit into the metaverse future?,6,qk6h7n,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qk6h7n/d_how_does_ai_fit_into_the_metaverse_future/,7,"**What are popular applications and research topics of AI relevant for VR AR **

As an ML engineer  I am interested in learning more about ML topics that are   could be useful for building VR software and hardware  This is meant to be an open ended question so all kinds of opinions and perspectives will be appreciated  Thanks ",1635741241.0,2021-11-01 05:34:01,**what popular applications research topics ai relevant vr ar ** ml engineer interested learning ml topics could useful building vr software hardware meant open ended question kinds opinions perspectives appreciated thanks
[D] Reusing parts of an open source code for a potential publication and a new open source code,1,qkert8,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkert8/d_reusing_parts_of_an_open_source_code_for_a/,2,"I am currently developing a new method that builds up upon an existing work in the literature in order to address the limitations and provide reasonable improvements to what has already been done  Earlier  I reached out to the authors for possible academic collaboration  but I have not received a reply from them  Their work has already been published as a conference paper two years ago  and their code is available on github  is regularly maintained and has also been deployed as pypi package that can be installed using \`pip\` 

My question is clearly about how to use certain parts of their work without plaigarising or breaking any copyright agreements  To what extent is it acceptable to rely on other people s work for producing a new method  especially when it is open source   Since the method I am working on is largely inspired from the existing method  it seems that I am currently on track to adopt around   of their code and follow their general code structure and OOP layout ",1635775674.0,2021-11-01 15:07:54,currently developing new method builds upon existing work literature order address limitations provide reasonable improvements already done earlier reached authors possible academic collaboration received reply work already published conference paper two years ago code available github regularly maintained also deployed pypi package installed using \`pip\` question clearly use certain parts work without plaigarising breaking copyright agreements extent acceptable rely people work producing new method especially open source since method working largely inspired existing method seems currently track adopt around code follow general code structure oop layout
[P] Model Performance Monitoring in Production,0,qkh9jg,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkh9jg/p_model_performance_monitoring_in_production/,0,"We ve recently introduced model performance metrics in Graphsignal  Basically  by just logging a label and prediction the model specific metrics are automatically computed  visualized and can be monitored  Graphsignal is currently SaaS  so a free account is necessary  No raw data is sent  only statistics 

More details in the blog post [Monitoring Model Performance in Production] https  graphsignal com blog monitoring model performance in production  

And the logger repo is [https  github com graphsignal graphsignal] https  github com graphsignal graphsignal  

I hope it can be useful for those who need to monitor models in production and do not want to build own pipelines for continuously computing accuracy and other metrics  implementing alerting  etc ",1635782837.0,2021-11-01 17:07:17,recently introduced model performance metrics graphsignal basically logging label prediction model specific metrics automatically computed visualized monitored graphsignal currently saas free account necessary raw data sent statistics details blog post [monitoring model performance production] graphsignal com blog monitoring model performance production logger repo [ github com graphsignal graphsignal] github com graphsignal graphsignal hope useful need monitor models production want build pipelines continuously computing accuracy metrics implementing alerting etc
[D] Thoughts on pathways by Google Research,4,qk7tuv,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qk7tuv/d_thoughts_on_pathways_by_google_research/,10,I recently found out about this proposal called  Pathways  by Jeff Dean  [https  blog google technology ai introducing pathways next generation ai architecture ] https  blog google technology ai introducing pathways next generation ai architecture   But the article seemed very obscure  there were just ideas and not a single hint of how that would be solved  whenever a question was posed the word  Pathways  was thrown at it  Is it another huge transformer from google     Just wanted to know what everyone here thinks about it ,1635747014.0,2021-11-01 07:10:14,recently found proposal called pathways jeff dean [ blog google technology ai introducing pathways next generation ai architecture ] blog google technology ai introducing pathways next generation ai architecture article seemed obscure ideas single hint would solved whenever question posed word pathways thrown another huge transformer google wanted know everyone thinks
[D] 2D models on 3D tasks (convolutions): simple replace?,0,qka32i,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qka32i/d_2d_models_on_3d_tasks_convolutions_simple/,2,"D tasks enjoy a vast backing of successful models that can be reused  For convolutions  can one simply replace D ops by D counterparts and inherit their benefits  Any  extra steps  to improve the transition  Not interested in unrolling the D input along channels 

Pubs code help ",1635758044.0,2021-11-01 10:14:04,tasks enjoy vast backing successful models reused convolutions one simply replace ops counterparts inherit benefits extra steps improve transition interested unrolling input along channels pubs code help
[D] What makes Multi Armed Bandit Problems contextual,2,qk9uet,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qk9uet/d_what_makes_multi_armed_bandit_problems/,3,"Hi everyone 

I dive straight into my problem  What makes a multi armed bandit problem contextual  I read on [TensorFlow agent tutorial] https  www tensorflow org agents tutorials per_arm_bandits_tutorial multi armed_bandits_with_arm_features  that the agent receives the context vector  which is just the observation  at every step  that makes a bandit setting contextual  Isnt every agent in an bandit setting doing this  Since in the MAB problem the agent needs to know on which machine bandit he is and how much he knows of the probability of the machine  So how does contextual MAB defer from the standard MAB   Is it for example  extra  information ontop  For example he knows wether a machine bandit has a higher probability if the wether is raining or not 

And the second part of my question is  I m currently working with Stable Baselines   Is here the normal observation function the correlating observation function  context vector  from tf and using the observation in every step making it contextual  Couldnt find any information in the SB documentation how the contextual settings work 

To be more specific  my  extra  context in my MAB problem is a state machine the bandit uses and each state is an one armed bandit 

I hope this isnt a beginner question and I am tolerated here ",1635756922.0,2021-11-01 09:55:22,hi everyone dive straight problem makes multi armed bandit problem contextual read [tensorflow agent tutorial] www tensorflow org agents tutorials per_arm_bandits_tutorial multi armed_bandits_with_arm_features agent receives context vector observation every step makes bandit setting contextual isnt every agent bandit setting since mab problem agent needs know machine bandit much knows probability machine contextual mab defer standard mab example extra information ontop example knows wether machine bandit higher probability wether raining second part question currently working stable baselines normal observation function correlating observation function context vector tf using observation every step making contextual couldnt find information sb documentation contextual settings work specific extra context mab problem state machine bandit uses state one armed bandit hope isnt beginner question tolerated
[D] Machine Learning - WAYR (What Are You Reading) - Week 124,8,qjxfu9,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qjxfu9/d_machine_learning_wayr_what_are_you_reading_week/,2,"This is a place to share machine learning research papers  journals  and articles that you re reading this week  If it relates to what you re researching  by all means elaborate and give us your insight  otherwise it could just be an interesting paper you ve read 

Please try to provide some insight from your understanding and please don t post things which are present in wiki 

Preferably you should link the arxiv page  not the PDF  you can easily access the PDF from the summary page but not the other way around  or any other pertinent links 

Previous weeks  

                           
 
 [Week ] https  www reddit com qyjiq [Week ] https  www reddit com xw [Week ] https  www reddit com ildf [Week ] https  www reddit com sku [Week ] https  www reddit com tnax [Week ] https  reddit com sel [Week ] https  reddit com bfsxz [Week ] https  reddit com dvno [Week ] https  reddit com ffiq [Week ] https  reddit com hlto [Week ] https  reddit com kywb [Week ] https  reddit com mygsm [Week ] https  reddit com pmzxg 
 [Week ] https  www reddit com sxqm [Week ] https  www reddit com acbt [Week ] https  www reddit com jwde [Week ] https  www reddit com aby [Week ] https  www reddit com wvjfk [Week ] https  reddit com aopot [Week ] https  reddit com blov [Week ] https  reddit com deh [Week ] https  reddit com ffsz [Week ] https  reddit com huzq [Week ] https  reddit com khnx [Week ] https  reddit com nmds [Week ] https  reddit com pwz 
 [Week ] https  www reddit com tmqm [Week ] https  www reddit com cwfb [Week ] https  www reddit com  [Week ] https  www reddit com d [Week ] https  www reddit com ex [Week ] https  reddit com ayaro [Week ] https  reddit com bqlbv [Week ] https  reddit com dkoxs [Week ] https  reddit com ffib [Week ] https  reddit com iaz [Week ] https  reddit com kpsxtc [Week ] https  reddit com njfsc [Week ] https  reddit com qfi 
 [Week ] https  www reddit com ubkw [Week ] https  www reddit com fcmh [Week ] https  www reddit com hhhb [Week ] https  www reddit com js [Week ] https  reddit com aluhs [Week ] https  reddit com adssz [Week ] https  reddit com bwjm [Week ] https  reddit com drnca [Week ] https  reddit com fnr [Week ] https  reddit com ijjcep [Week ] https  reddit com kzevku [Week ] https  reddit com ntulq 
 [Week ] https  www reddit com xomf [Week ] https  www reddit com hyur [Week ] https  www reddit com teiz [Week ] https  www reddit com bav [Week ] https  reddit com tnnez [Week ] https  reddit com aigi [Week ] https  reddit com citkk [Week ] https  reddit com dxshkg [Week ] https  reddit com fvkj [Week ] https  reddit com ishj [Week ] https  reddit com llvgs [Week ] https  reddit com odph 
 [Week ] https  www reddit com zcyvk [Week ] https  www reddit com kdvd [Week ] https  www reddit com dnb [Week ] https  www reddit com efx [Week ] https  reddit com xoj [Week ] https  reddit com apctk [Week ] https  reddit com cdgko [Week ] https  reddit com enmyk [Week ] https  reddit com geavg [Week ] https  reddit com jxr [Week ] https  reddit com ljxn [Week ] https  reddit com odrudt 
 [Week ] https  www reddit com tmo [Week ] https  www reddit com obdx [Week ] https  www reddit com gngwc [Week ] https  www reddit com hccc [Week ] https  reddit com jmh [Week ] https  reddit com aucic [Week ] https  reddit com cjkyc [Week ] https  reddit com eblxk [Week ] https  reddit com gcxuf [Week ] https  reddit com jcbfs [Week ] https  reddit com luqbxl [Week ] https  reddit com omy 
 [Week ] https  www reddit com heol [Week ] https  www reddit com ryd [Week ] https  www reddit com jgdva [Week ] https  www reddit com kgcqr [Week ] https  reddit com upg [Week ] https  reddit com azjoht [Week ] https  reddit com cpjex [Week ] https  reddit com ehbfst [Week ] https  reddit com glmsv [Week ] https  reddit com jhzzv [Week ] https  reddit com muz [Week ] https  reddit com ovzj 
 [Week ] https  www reddit com kvsu [Week ] https  www reddit com ttcz [Week ] https  www reddit com mlv [Week ] https  www reddit com nayri [Week ] https  reddit com nrt [Week ] https  reddit com bry [Week ] https  reddit com cvdea [Week ] https  reddit com entcxy [Week ] https  reddit com gutd [Week ] https  reddit com jqjgo [Week ] https  reddit com mfmu [Week ] https  reddit com pknh 
 [Week ] https  www reddit com soa [Week ] https  www reddit com whwb [Week ] https  www reddit com pha [Week ] https  www reddit com qelp [Week ] https  reddit com cf [Week ] https  reddit com bakew [Week ] https  reddit com dgk [Week ] https  reddit com euctyw [Week ] https  reddit com hddfj [Week ] https  reddit com jzevt [Week ] https  reddit com moym [Week ] https  reddit com peidh 

Most upvoted papers two weeks ago 

 u Icko_  [Patches Are All You Need ] https  papers labml ai paper ddaaeecedcbabe 

 u CatalyzeX_code_bot  [Paper link] https  arxiv org abs   

Besides that  there are no rules  have fun ",1635710405.0,2021-10-31 21:00:05,place share machine learning research papers journals articles reading week relates researching means elaborate give us insight otherwise could interesting paper read please try provide insight understanding please post things present wiki preferably link arxiv page pdf easily access pdf summary page way around pertinent links previous weeks [week ] www reddit com qyjiq [week ] www reddit com xw [week ] www reddit com ildf [week ] www reddit com sku [week ] www reddit com tnax [week ] reddit com sel [week ] reddit com bfsxz [week ] reddit com dvno [week ] reddit com ffiq [week ] reddit com hlto [week ] reddit com kywb [week ] reddit com mygsm [week ] reddit com pmzxg [week ] www reddit com sxqm [week ] www reddit com acbt [week ] www reddit com jwde [week ] www reddit com aby [week ] www reddit com wvjfk [week ] reddit com aopot [week ] reddit com blov [week ] reddit com deh [week ] reddit com ffsz [week ] reddit com huzq [week ] reddit com khnx [week ] reddit com nmds [week ] reddit com pwz [week ] www reddit com tmqm [week ] www reddit com cwfb [week ] www reddit com [week ] www reddit com [week ] www reddit com ex [week ] reddit com ayaro [week ] reddit com bqlbv [week ] reddit com dkoxs [week ] reddit com ffib [week ] reddit com iaz [week ] reddit com kpsxtc [week ] reddit com njfsc [week ] reddit com qfi [week ] www reddit com ubkw [week ] www reddit com fcmh [week ] www reddit com hhhb [week ] www reddit com js [week ] reddit com aluhs [week ] reddit com adssz [week ] reddit com bwjm [week ] reddit com drnca [week ] reddit com fnr [week ] reddit com ijjcep [week ] reddit com kzevku [week ] reddit com ntulq [week ] www reddit com xomf [week ] www reddit com hyur [week ] www reddit com teiz [week ] www reddit com bav [week ] reddit com tnnez [week ] reddit com aigi [week ] reddit com citkk [week ] reddit com dxshkg [week ] reddit com fvkj [week ] reddit com ishj [week ] reddit com llvgs [week ] reddit com odph [week ] www reddit com zcyvk [week ] www reddit com kdvd [week ] www reddit com dnb [week ] www reddit com efx [week ] reddit com xoj [week ] reddit com apctk [week ] reddit com cdgko [week ] reddit com enmyk [week ] reddit com geavg [week ] reddit com jxr [week ] reddit com ljxn [week ] reddit com odrudt [week ] www reddit com tmo [week ] www reddit com obdx [week ] www reddit com gngwc [week ] www reddit com hccc [week ] reddit com jmh [week ] reddit com aucic [week ] reddit com cjkyc [week ] reddit com eblxk [week ] reddit com gcxuf [week ] reddit com jcbfs [week ] reddit com luqbxl [week ] reddit com omy [week ] www reddit com heol [week ] www reddit com ryd [week ] www reddit com jgdva [week ] www reddit com kgcqr [week ] reddit com upg [week ] reddit com azjoht [week ] reddit com cpjex [week ] reddit com ehbfst [week ] reddit com glmsv [week ] reddit com jhzzv [week ] reddit com muz [week ] reddit com ovzj [week ] www reddit com kvsu [week ] www reddit com ttcz [week ] www reddit com mlv [week ] www reddit com nayri [week ] reddit com nrt [week ] reddit com bry [week ] reddit com cvdea [week ] reddit com entcxy [week ] reddit com gutd [week ] reddit com jqjgo [week ] reddit com mfmu [week ] reddit com pknh [week ] www reddit com soa [week ] www reddit com whwb [week ] www reddit com pha [week ] www reddit com qelp [week ] reddit com cf [week ] reddit com bakew [week ] reddit com dgk [week ] reddit com euctyw [week ] reddit com hddfj [week ] reddit com jzevt [week ] reddit com moym [week ] reddit com peidh upvoted papers two weeks ago u icko_ [patches need ] papers labml ai paper ddaaeecedcbabe u catalyzex_code_bot [paper link] arxiv org abs besides rules fun
[D] Using Movies to Improve Punctuation Prediction,13,qjuxax,MachineLearning,https://youtu.be/jxOpu4hXPJY,2,nan,1635703132.0,2021-10-31 18:58:52,nan
[R] Physics Informed Neural Network suggestion and recommendation,0,qk79s9,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qk79s9/r_physics_informed_neural_network_suggestion_and/,2,Hi guys  I am learning about physics informed neural network  actual research focussing on Autoencoder  However  I just got in this new field and really want to get in depth knowledge in this area  Would you recommend any related work or papers that I should read  Thanks a lot ,1635744547.0,2021-11-01 06:29:07,hi guys learning physics informed neural network actual research focussing autoencoder however got new field really want get depth knowledge area would recommend related work papers read thanks lot
[P] StyleGAN3 + Cosplay Dataset. Happy Halloween! 🎃,805,qj3uhj,MachineLearning,https://v.redd.it/imst817wvlw71,20,nan,1635607882.0,2021-10-30 17:31:22,nan
[D] Projects to do with 100TB ASMR video dataset?,33,qjkoam,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qjkoam/d_projects_to_do_with_100tb_asmr_video_dataset/,27,"I have been archiving ASMR videos for a number of years and have built up a TB collection of videos and thumbnails  About  billion individual frames  There is only so much ASMR I can listen to before I get bored  so I m curious if there are any interesting ML projects that I can run against this data set 

I think generating single frames or thumbnails using StyleGAN could be an option but not really that exciting  Audio and video generation seems considerably more immature  

I was thinking of starting my training my own x upscaler  using the K videos as a dataset  to convert all p videos to K  Keen to hear if anyone has fun ideas ",1635666250.0,2021-10-31 08:44:10,archiving asmr videos number years built tb collection videos thumbnails billion individual frames much asmr listen get bored curious interesting ml projects run data set think generating single frames thumbnails using stylegan could option really exciting audio video generation seems considerably immature thinking starting training x upscaler using k videos dataset convert p videos k keen hear anyone fun ideas
[R] current state of the art and novel research in support vector machines,2,qk2bj6,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qk2bj6/r_current_state_of_the_art_and_novel_research_in/,9,Title pretty much says it already  I m interested in the current state of art of svms svrs and on which topics researchers currently work on in that area   i guess optimization still being a big one   Would also appreciate any paper links posted here on not so much known svm extensions etc  Go nuts     ,1635726122.0,2021-11-01 01:22:02,title pretty much says already interested current state art svms svrs topics researchers currently work area guess optimization still big one would also appreciate paper links posted much known svm extensions etc go nuts
[D] Anyone with powerful computers deploying locally?,1,qk144p,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qk144p/d_anyone_with_powerful_computers_deploying_locally/,4,I have my own computer that has a pretty good cpu gpu  I d rather not spend more time to get a static ip to set up my computer as an official server through my ISP  or move my model and setup an expensive instance in the cloud  Is there an easier way to run an inference server on my machine that i am not aware of ,1635721988.0,2021-11-01 00:13:08,computer pretty good cpu gpu rather spend time get static ip set computer official server isp move model setup expensive instance cloud easier way run inference server machine aware
[D] State-of-the-art AI research,11,qjlmfe,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qjlmfe/d_stateoftheart_ai_research/,10,"Hi everybody 

How where do you check the state of the art of a topic  e g  What’s the state of the art result for  Transformer  right now  Do you search for  Transformer  on Google scholar or Arxiv and you look for the last papers that came out 

Are there any tools available or some peer review system that states where we are now with a specific topic 

Thanks in advance ",1635670613.0,2021-10-31 09:56:53,hi everybody check state art topic e g what’s state art result transformer right search transformer google scholar arxiv look last papers came tools available peer review system states specific topic thanks advance
Training a model to recognize that a source/citation is needed in a text [R],3,qjtapm,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qjtapm/training_a_model_to_recognize_that_a/,3,"I m looking into creating a tool that will analyze text and recognize where a source is needed for the claim or fact 

Some examples could be 
 Amazon was founded in  
 Avarage conversion rates for eCommerce is  
 Gold is up   in May  

My question is if it would be possible to train a NLP model to understand this ",1635698317.0,2021-10-31 17:38:37,looking creating tool analyze text recognize source needed claim fact examples could amazon founded avarage conversion rates ecommerce gold may question would possible train nlp model understand
[R] ICCV2021 Oral -- Neural TMDlayer: Modeling Instantaneous flow of features via SDE Generators (with video explanation),1,qjx4k3,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qjx4k3/r_iccv2021_oral_neural_tmdlayer_modeling/,0,"Our TMDlayer is inspired by stochastic differential equation  SDE  and aims to model the stochastic flow of features  In principle  it can be easily added on top of any DNN layer to bring the benefits  In addition  it immediately enables transductive inference once inserted into the model  

Welcome to check out our video for a quick and easy understanding 

Video  [https  www youtube com watch v=vRnrYJqcgQ t=s] https  www youtube com watch v=vRnrYJqcgQ t=s 

Paper  [https  openaccess thecvf com content ICCV papers Meng\_Neural\_TMDlayer\_Modeling\_Instantaneous\_Flow\_of\_Features\_via\_SDE\_Generators\_ICCV\_\_paper pdf] https  openaccess thecvf com content ICCV papers Meng_Neural_TMDlayer_Modeling_Instantaneous_Flow_of_Features_via_SDE_Generators_ICCV__paper pdf 

Code  [https  github com zihangm neural tmd layer] https  github com zihangm neural tmd layer 

 xB 

Our paper abstract  We study how stochastic differential equation  SDE  based ideas can inspire new modifications to existing algorithms for a set of problems in computer vision  Loosely speaking  our formulation is related to both explicit and implicit strategies for data augmentation and group equivariance  but is derived from new results in the SDE literature on estimating infinitesimal generators of a class of stochastic processes  If and when there is nominal agreement between the needs of an application task and the inherent properties and behavior of the types of processes that we can efficiently handle  we obtain a very simple and efficient plug in layer that can be incorporated within any existing network architecture  with minimal modification and only a few additional parameters  We show promising experiments on a number of vision tasks including few shot learning  point cloud transformers and deep variational segmentation obtaining efficiency or performance improvements ",1635709456.0,2021-10-31 20:44:16,tmdlayer inspired stochastic differential equation sde aims model stochastic flow features principle easily added top dnn layer bring benefits addition immediately enables transductive inference inserted model welcome check video quick easy understanding video [ www youtube com watch v=vrnryjqcgq t=s] www youtube com watch v=vrnryjqcgq t=s paper [ openaccess thecvf com content iccv papers meng\_neural\_tmdlayer\_modeling\_instantaneous\_flow\_of\_features\_via\_sde\_generators\_iccv\_\_paper pdf] openaccess thecvf com content iccv papers meng_neural_tmdlayer_modeling_instantaneous_flow_of_features_via_sde_generators_iccv__paper pdf code [ github com zihangm neural tmd layer] github com zihangm neural tmd layer xb paper abstract study stochastic differential equation sde based ideas inspire new modifications existing algorithms set problems computer vision loosely speaking formulation related explicit implicit strategies data augmentation group equivariance derived new results sde literature estimating infinitesimal generators class stochastic processes nominal agreement needs application task inherent properties behavior types processes efficiently handle obtain simple efficient plug layer incorporated within existing network architecture minimal modification additional parameters show promising experiments number vision tasks including shot learning point cloud transformers deep variational segmentation obtaining efficiency performance improvements
[R] Is Automated Topic Model Evaluation Broken?: The Incoherence of Coherence,1,qjwdny,MachineLearning,https://arxiv.org/abs/2107.02173,2,nan,1635707275.0,2021-10-31 20:07:55,nan
[D] What factors hinder people from studying causal inference in machine learning?,10,qjj70h,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qjj70h/d_what_factors_hinder_people_from_studying_causal/,12,"Causal Inference has been widely studied and applied in many fields  but its impact seems to be relatively low in the machine learning community at the moment  I ve listened to some talks  but people are only emphasizing the importance of causality  which seems to be axiomatic   with little discussion of the main technical challenges  Are there some factors that prevent researchers in machine learning from embracing causal inference  Or in other words  are some assumptions of causal inference difficult to hold in machine learning research problems 

Any kind of discussion is welcome  Thank you very much ",1635659459.0,2021-10-31 06:50:59,causal inference widely studied applied many fields impact seems relatively low machine learning community moment listened talks people emphasizing importance causality seems axiomatic little discussion main technical challenges factors prevent researchers machine learning embracing causal inference words assumptions causal inference difficult hold machine learning research problems kind discussion welcome thank much
[R] A Closer Look at How Fine-tuning Changes BERT,5,qjim1u,MachineLearning,https://arxiv.org/abs/2106.14282,1,nan,1635656913.0,2021-10-31 06:08:33,nan
[D] Sagemaker. Pros and Cons?,11,qjeslj,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qjeslj/d_sagemaker_pros_and_cons/,9, Hello everybody  I have a question  What are the pros and cons of using Sagemaker for a machine learning pipeline  I see many company conferences using custom solutions and architectures involving docker and other frameworks within the cloud  but never using Sagemaker  I have this doubt since it presents itself as a complete solution for end to end projects,1635642145.0,2021-10-31 02:02:25,hello everybody question pros cons using sagemaker machine learning pipeline see many company conferences using custom solutions architectures involving docker frameworks within cloud never using sagemaker doubt since presents complete solution end end projects
[N] Jax now Supports Apple Silicon [CPU ONLY],22,qjarv1,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qjarv1/n_jax_now_supports_apple_silicon_cpu_only/,6,"In case you guys didn t know this  I m able to use JAX on my M MacBook Air 

Currently only CPU is supported  and I think they don t have any plans to support GPU yet 

Check this thread to install jaxlib  [https  github com google jax issues ] https  github com google jax issues  

Issue for GPU support  [https  github com google jax issues ] https  github com google jax issues  ",1635628495.0,2021-10-30 23:14:55,case guys know able use jax macbook air currently cpu supported think plans support gpu yet check thread install jaxlib [ github com google jax issues ] github com google jax issues issue gpu support [ github com google jax issues ] github com google jax issues
[D] How to detect multiple number with decimal points?,0,qjopnm,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qjopnm/d_how_to_detect_multiple_number_with_decimal/,6,"I have images which have time displayed in the bottom corner with decimal points  I want to store those time stamp along with the image ID in a spreadsheet  I tried using PyTesseract but it doesn t work very well  What off the shelf alternatives could I use 

I know it s a very simple problem but I would really appreciate any help ",1635684146.0,2021-10-31 13:42:26,images time displayed bottom corner decimal points want store time stamp along image id spreadsheet tried using pytesseract work well shelf alternatives could use know simple problem would really appreciate help
[P] StyleGAN3 - This shoe does not exist,126,qiwl21,MachineLearning,https://www.thisshoedoesnotexist.com,20,nan,1635579804.0,2021-10-30 09:43:24,nan
[R] Measuring Disagreement in Science,2,qjhe3z,MachineLearning,https://arxiv.org/abs/2107.14641,1,nan,1635651922.0,2021-10-31 04:45:22,nan
[D] Getting Started with Deep Learning in JAX with Treex in 16 lines,19,qj1d49,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qj1d49/d_getting_started_with_deep_learning_in_jax_with/,13,"If you are JAX curious but don t want to stray too far from the Pytorch way 

[Treex] https  github com cgarciae treex  is here to save the day 🌳

 xB 

https  preview redd it cjnbsflw png width= format=png auto=webp s=cabbcedeebcddd

Treex is a Pytree based Module system for JAX that is simple and intuitive  Just like in Pytorch  Treex Modules hold their parameters and respect Object Oriented semantics  Thanks to their Pytree component Treex is fully compatible with all JAX functions \`jit\`  \`vmap\`  \`grad\`  etc  and rd party code that works with Pytrees 

**Note**  For a real use case  you want to extract the loop s body into a jit ed function for performance as shown next  however the previous code is more pedagogical 

 xB 

https  preview redd it pzmuxrvflw png width= format=png auto=webp s=efaefdcbfcaccbcfff",1635600045.0,2021-10-30 15:20:45,jax curious want stray far pytorch way [treex] github com cgarciae treex save day 🌳 xb preview redd cjnbsflw png width= format=png auto=webp s=cabbcedeebcddd treex pytree based module system jax simple intuitive like pytorch treex modules hold parameters respect object oriented semantics thanks pytree component treex fully compatible jax functions \`jit\` \`vmap\` \`grad\` etc rd party code works pytrees **note** real use case want extract loop body jit ed function performance shown next however previous code pedagogical xb preview redd pzmuxrvflw png width= format=png auto=webp s=efaefdcbfcaccbcfff
Composability in Julia: Implementing Deep Equilibrium Models via Neural ODEs [P],14,qizzhr,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qizzhr/composability_in_julia_implementing_deep/,0,"https  julialang org blog   DEQ 

This is just a fun blog post showing how to link two different  implicit layer  methods  Neural ODEs and Deep Equilibrium Models  DEQ   specifically by implementing a DEQ via Neural ODEs with events  It also showcases Julia and the SciML libraries as a nice research platform for these kinds of ideas as pieces like the optimized adjoints  GPU compatibility  etc  all come for free just by using the standard nonlinear and ODE solver libraries  We ll be doing a more serious and in depth discussion of related methods in some upcoming papers  but for now enjoy a post that s more about building bridges and opening questions than anything else ",1635594987.0,2021-10-30 13:56:27,julialang org blog deq fun blog post showing link two different implicit layer methods neural odes deep equilibrium models deq specifically implementing deq via neural odes events also showcases julia sciml libraries nice research platform kinds ideas pieces like optimized adjoints gpu compatibility etc come free using standard nonlinear ode solver libraries serious depth discussion related methods upcoming papers enjoy post building bridges opening questions anything else
[News][Research] ADOP: Approximate Differentiable One-Pixel Point Rendering (Synthesize Smooth Videos from a Couple of Images),10,qj0yhj,MachineLearning,https://youtu.be/Jfph7Vld_Nw,2,nan,1635598673.0,2021-10-30 14:57:53,nan
"[D] Interview w/ Siraj Raval - Stories about YouTube, Plagiarism, and the Dangers of Fame (by Yannic Kilcher)",0,qjptfe,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qjptfe/d_interview_w_siraj_raval_stories_about_youtube/,70,"I had a super interesting conversation with Siraj Raval about YouTube  being popular  plagiarism  chasing clout  and the perils of fame  I think there is definitely something in here for everyone  Have a listen 

[https  youtu be kEhEbVZQwjM] https  youtu be kEhEbVZQwjM 

OUTLINE 

    Intro

    Welcome

    Starting out  From Economics to YouTube

    More Views  Plagiarizing Video Content

    One Step Up  Copying A Research Paper

    Was there another way 

    Clickbait Course  Make Money with Machine Learning

    Rock Bottom and the Way Forward

     Advice for Future Generations

 xB 

Siraj s Channel  [https  www youtube com c SirajRaval] https  www youtube com c SirajRaval ",1635687949.0,2021-10-31 14:45:49,super interesting conversation siraj raval youtube popular plagiarism chasing clout perils fame think definitely something everyone listen [ youtu kehebvzqwjm] youtu kehebvzqwjm outline intro welcome starting economics youtube views plagiarizing video content one step copying research paper another way clickbait course make money machine learning rock bottom way forward advice future generations xb siraj channel [ www youtube com c sirajraval] www youtube com c sirajraval
[D] What is a reasonable way to address a paper that was published and you consider to be dishonest or plain bogus?,220,qiea6g,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qiea6g/d_what_is_a_reasonable_way_to_address_a_paper/,61,I have reasons to believe that some published work did not do an honest comparison with related work  and the analysis lacks merit  misrepresents the models it was tested against  and does not explain why the proposed framework works ,1635518300.0,2021-10-29 16:38:20,reasons believe published work honest comparison related work analysis lacks merit misrepresents models tested explain proposed framework works
[D] Recommendation for pattern theory/similar literature?I’m interested in theoretical backing for texture recognition (image required),0,qj6rv4,MachineLearning,https://i.redd.it/frgzfby4mmw71.jpg,2,nan,1635616516.0,2021-10-30 19:55:16,nan
[D] What do you think about my idea to boost skip connections in ResNets without performance loss?,8,qivb0y,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qivb0y/d_what_do_you_think_about_my_idea_to_boost_skip/,9,"Consider a small block that has skip connections which is part of some bigger NN 

Symbols  input x  output y  weight w ⊂ D some distribution    A very simplified situation for demonstration  

Block  y=x wx

Suppose some loss L and a given matrix single number in this case  of gradients dL dy calculated via backpropagation 

dy dw = x  dL dw=dL dy \* dy dw = dL dy \* x

The weight w will be modified as  w  = lr \*  dL dy \* x  

We also compute **dL dx=dL dy \* w** for backprapagation to more early layers 

dy dx=w  dL dx=dL dy \* dy dx = dL dy \* w

\ 

We can omit the skip connection by initializing the weights as a number v that is  bigger than w 

init  v=w    w ⊂ D

Block  y=vx

In this case  dL dv is computed consistently as the case with the original weight w 

dy dv = x  dL dv=dL dy \* dy dv = dL dy \* x

However  the problem is that dL dx is different because dy dx=v=w  not w 

dy dx=v=w  **dL dx=dL dy \* dy dx = dL dy \*  w  **

This will affect the gradient values of other layers  This is intuitively expandable to more complex operations e g  matmul and convolution 

\ 

Because of this inconsistency  removing residual connections with a modified initialization is known to yield different worse  performance from the original ResNet architecture 

What if we initialize the weights with w  but define a skip connection function or layer with custom Autograd backward methods like the tutorial below to compute dy dx as  w    Would this yield exactly the same inference results and gradients compared to a typical skip connection in ResNet  What do you think 

The problem with such skip connections are that they increase the peak memory usage by x because we have to cache all the features of x while computing f x   and they do impact inference time 

[https  pytorch org tutorials beginner examples\_autograd two\_layer\_net\_custom\_function html] https  pytorch org tutorials beginner examples_autograd two_layer_net_custom_function html 

This is just some stuff in mind that came up while reading the paper [*RepVGG  Making VGG style ConvNets Great Again*] https  arxiv org pdf   pdf   Haven t done much research on it yet  but the DiractNet paper seems relevant  while it seems to degrade the performance without a solution to the gradient problem  it suggests that initializing W =W I I is identity matrix  should yield the same results in linear algebra operations 

[σ\ x\  is a function combining nonlinearity and batch normalization] https  preview redd it cwaosghjw png width= format=png auto=webp s=ffefaebecdbaed 

Edit  the expansion to linear algebra might not be as simple   ",1635573972.0,2021-10-30 08:06:12,consider small block skip connections part bigger nn symbols input x output weight w ⊂ distribution simplified situation demonstration block y=x wx suppose loss l given matrix single number case gradients dl dy calculated via backpropagation dy dw = x dl dw=dl dy \* dy dw = dl dy \* x weight w modified w = lr \* dl dy \* x also compute **dl dx=dl dy \* w** backprapagation early layers dy dx=w dl dx=dl dy \* dy dx = dl dy \* w \ omit skip connection initializing weights number v bigger w init v=w w ⊂ block y=vx case dl dv computed consistently case original weight w dy dv = x dl dv=dl dy \* dy dv = dl dy \* x however problem dl dx different dy dx=v=w w dy dx=v=w **dl dx=dl dy \* dy dx = dl dy \* w ** affect gradient values layers intuitively expandable complex operations e g matmul convolution \ inconsistency removing residual connections modified initialization known yield different worse performance original resnet architecture initialize weights w define skip connection function layer custom autograd backward methods like tutorial compute dy dx w would yield exactly inference results gradients compared typical skip connection resnet think problem skip connections increase peak memory usage x cache features x computing f x impact inference time [ pytorch org tutorials beginner examples\_autograd two\_layer\_net\_custom\_function html] pytorch org tutorials beginner examples_autograd two_layer_net_custom_function html stuff mind came reading paper [*repvgg making vgg style convnets great again*] arxiv org pdf pdf done much research yet diractnet paper seems relevant seems degrade performance without solution gradient problem suggests initializing w =w identity matrix yield results linear algebra operations [σ\ x\ function combining nonlinearity batch normalization] preview redd cwaosghjw png width= format=png auto=webp s=ffefaebecdbaed edit expansion linear algebra might simple
[D] How to truly understand attention mechanism in transformers?,95,qidpqx,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qidpqx/d_how_to_truly_understand_attention_mechanism_in/,33,Attention seems to be a core concept for language modeling these days  However it is not that easy to fully understand  and in my opinion  somewhat **unintuitive**  While I know what attention does  multiplying Q and K  scaling   softmax  multiply with V   I lack an intuitive understanding of what is happening  What were some explanations or resources that made attention click for you ,1635516674.0,2021-10-29 16:11:14,attention seems core concept language modeling days however easy fully understand opinion somewhat **unintuitive** know attention multiplying q k scaling softmax multiply v lack intuitive understanding happening explanations resources made attention click
[P] Large-scale language modeling tutorials with PyTorch,38,qiguvq,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qiguvq/p_largescale_language_modeling_tutorials_with/,1," xB 

https  preview redd it kggmbfw png width= format=png auto=webp s=cdddfeaedeeefcfee

Hello  I m Kevin Ko  a machine learning engineer at TUNiB  Korean AI startup  Since last year  GPT has taken the lead  and large scale language models have become mainstream of NLP research  However  from the point of view of a model scientist who has never learned distributed programming  these techniques may still feel very unfamiliar  I was one of them  and I m still going through a lot of trial and error  😂

So  I made a tutorial notebooks by organizing various techniques related to large scale language modeling  I hope this material will be of some help to people like me who are interested in distributed programming and large scale language models  but are afraid to start  I made all the materials in Korean  my native language  but if you use Google Translate  you will be able to study well  And when I have free time  I will translate all materials into English  Thank you  

[https  github com tunib ai large scale lm tutorials] https  github com tunib ai large scale lm tutorials ",1635525481.0,2021-10-29 18:38:01,xb preview redd kggmbfw png width= format=png auto=webp s=cdddfeaedeeefcfee hello kevin ko machine learning engineer tunib korean ai startup since last year gpt taken lead large scale language models become mainstream nlp research however point view model scientist never learned distributed programming techniques may still feel unfamiliar one still going lot trial error 😂 made tutorial notebooks organizing various techniques related large scale language modeling hope material help people like interested distributed programming large scale language models afraid start made materials korean native language use google translate able study well free time translate materials english thank [ github com tunib ai large scale lm tutorials] github com tunib ai large scale lm tutorials
[P] “Abstractified Multi-instance Learning (AMIL) for Biomedical Relation Extraction”,1,qiy1bn,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qiy1bn/p_abstractified_multiinstance_learning_amil_for/,0,"https  arxiv org pdf  v pdf

https  www youtube com watch v=vNBAaE_uaUg

https  akbc apps allenai org static slides  pdf

https  www akbc ws  assets pdfs VXswzJEzpg pdf

The idea is that a sentence with two entities is likely to express a relationship  The novelty of this paper is that they bunch sentences together by entity type ",1635586577.0,2021-10-30 11:36:17,arxiv org pdf v pdf www youtube com watch v=vnbaae_uaug akbc apps allenai org static slides pdf www akbc ws assets pdfs vxswzjezpg pdf idea sentence two entities likely express relationship novelty paper bunch sentences together entity type
[P] Complete End-to-End Machine Learning Portal (POC),0,qiytlg,MachineLearning,https://github.com/shreyas-jk/ML-Portal-FastAPI,1,nan,1635590041.0,2021-10-30 12:34:01,nan
[R] Do Vision Transformers See like Convolutional Neural Networks?,9,qifv7i,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qifv7i/r_do_vision_transformers_see_like_convolutional/,0,"Do Vision Transformers work in the same way as CNNs  Do the internal representational structures of ViTs and CNNs differ  

Podcast  [https  youtu be kJJoBayjg] https  youtu be kJJoBayjg ",1635522694.0,2021-10-29 17:51:34,vision transformers work way cnns internal representational structures vits cnns differ podcast [ youtu kjjobayjg] youtu kjjobayjg
[D] Google: Ondevice grammar error correction on gboard,4,qimdee,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qimdee/d_google_ondevice_grammar_error_correction_on/,0,"https  ai googleblog com   grammar correction as you type on pixel html

We are launching a grammar correction feature that is directly built into Gboard on Pixel  that works entirely on device to preserve privacy  detecting and suggesting corrections for grammatical errors while the user is typing  Building such functionality required addressing a few key obstacles  memory size limitations  latency requirements  and handling partial sentences  Currently  the feature is capable of correcting English sentences  we plan to expand to more languages in the near future  and available on almost any app with Gboard 

Gboard suggests how to correct an ungrammatical sentence as the user types ",1635541499.0,2021-10-29 23:04:59,ai googleblog com grammar correction type pixel html launching grammar correction feature directly built gboard pixel works entirely device preserve privacy detecting suggesting corrections grammatical errors user typing building functionality required addressing key obstacles memory size limitations latency requirements handling partial sentences currently feature capable correcting english sentences plan expand languages near future available almost app gboard gboard suggests correct ungrammatical sentence user types
[D] How machine learning is changing immunology,181,qhy2gd,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhy2gd/d_how_machine_learning_is_changing_immunology/,21,"Hi  ML community 

This article shows some examples of how immunology researchers are using machine learning to improve our understanding of the immune system 
https  www immunai com press immunology and machine learning online symposium

This article can spark some interest in these kinds of applications  because this is just the tip of the iceberg in terms of what is actually possible ",1635459560.0,2021-10-29 00:19:20,hi ml community article shows examples immunology researchers using machine learning improve understanding immune system www immunai com press immunology machine learning online symposium article spark interest kinds applications tip iceberg terms actually possible
[D] Advice on Short (5 minute) Talks for Virtual Conferences,0,qiqcpp,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qiqcpp/d_advice_on_short_5_minute_talks_for_virtual/,1,"I had a first author paper accepted for the virtual poster session at neurips  As part of that  I need to give a  minute talk  due November st  yes  I am a procrastinator  

This is my first time giving a talk at a non workshop conference venue  The instructions provided from neurips are very sparse  with no advice on how the talk should be structured  Can anyone here give advice on what they ve seen that s worked well in \~ minute lightning technical talks ",1635554689.0,2021-10-30 02:44:49,first author paper accepted virtual poster session neurips part need give minute talk due november st yes procrastinator first time giving talk non workshop conference venue instructions provided neurips sparse advice talk structured anyone give advice seen worked well \~ minute lightning technical talks
"[D] Google Research: Introducing Pathways, a next-generation AI architecture",81,qi0act,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qi0act/d_google_research_introducing_pathways_a/,23,"**Blog Post URL**

[https  blog google technology ai introducing pathways next generation ai architecture ] https  blog google technology ai introducing pathways next generation ai architecture 

**Summary**

GShard and Switch Transformer are two of the largest machine learning models we’ve ever created  but because both use sparse activation  they [consume less than  th the energy] https  blog google technology ai minimizing carbon footprint  that you’d expect of similarly sized dense models — while being as accurate as dense models 

So to recap  today’s machine learning models tend to overspecialize at individual tasks when they could excel at many  They rely on one form of input when they could synthesize several  And too often they resort to brute force when deftness and specialization of expertise would do 

That’s why we’re building Pathways  Pathways will enable a single AI system to generalize across thousands or millions of tasks  to understand different types of data  and to do so with remarkable efficiency – advancing us from the era of single purpose models that merely recognize patterns to one in which more general purpose intelligent systems reflect a deeper understanding of our world and can adapt to new needs 

**Intro**

Too often  machine learning systems overspecialize at individual tasks  when they could excel at many  That’s why we’re building Pathways—a new AI architecture that will handle many tasks at once  learn new tasks quickly and reflect a better understanding of the world 

When I reflect on the past two decades of computer science research  few things inspire me more than the remarkable progress we’ve seen in the field of artificial intelligence 

In   some colleagues sitting just a few feet away from me at Google realized they could use an obscure technique called machine learning to help correct misspelled Search queries   I remember I was amazed to see it work on everything from “ayambic pitnamiter” to “unnblevaiabel”   Today  AI augments many of the things that we do  whether that’s helping you [capture a nice selfie] https  ai googleblog com   take all your pictures to cleaners with html   or providing [more useful search results] https  blog google products search how ai making information more useful   or warning hundreds of millions of people [when and where flooding will occur] https  ai googleblog com   the technology behind our recent html   Twenty years of advances in research have helped elevate AI from a promising idea to an indispensable aid in billions of people’s daily lives  And for all that progress  I’m still excited about its as yet untapped potential – AI is poised to help humanity confront some of the toughest challenges we’ve ever faced  from persistent problems like illness and inequality to emerging threats like climate change 

But matching the depth and complexity of those urgent challenges will require new  more capable AI systems – systems that can combine AI’s proven approaches with nascent research directions to be able to solve problems we are unable to solve today  To that end  teams across Google Research are working on elements of a next generation AI architecture we think will help realize such systems ",1635466654.0,2021-10-29 02:17:34,**blog post url** [ blog google technology ai introducing pathways next generation ai architecture ] blog google technology ai introducing pathways next generation ai architecture **summary** gshard switch transformer two largest machine learning models we’ve ever created use sparse activation [consume less th energy] blog google technology ai minimizing carbon footprint you’d expect similarly sized dense models — accurate dense models recap today’s machine learning models tend overspecialize individual tasks could excel many rely one form input could synthesize several often resort brute force deftness specialization expertise would that’s we’re building pathways pathways enable single ai system generalize across thousands millions tasks understand different types data remarkable efficiency – advancing us era single purpose models merely recognize patterns one general purpose intelligent systems reflect deeper understanding world adapt new needs **intro** often machine learning systems overspecialize individual tasks could excel many that’s we’re building pathways—a new ai architecture handle many tasks learn new tasks quickly reflect better understanding world reflect past two decades computer science research things inspire remarkable progress we’ve seen field artificial intelligence colleagues sitting feet away google realized could use obscure technique called machine learning help correct misspelled search queries remember amazed see work everything “ayambic pitnamiter” “unnblevaiabel” today ai augments many things whether that’s helping [capture nice selfie] ai googleblog com take pictures cleaners html providing [more useful search results] blog google products search ai making information useful warning hundreds millions people [when flooding occur] ai googleblog com technology behind recent html twenty years advances research helped elevate ai promising idea indispensable aid billions people’s daily lives progress i’m still excited yet untapped potential – ai poised help humanity confront toughest challenges we’ve ever faced persistent problems like illness inequality emerging threats like climate change matching depth complexity urgent challenges require new capable ai systems – systems combine ai’s proven approaches nascent research directions able solve problems unable solve today end teams across google research working elements next generation ai architecture think help realize systems
[D] Can you apply mix-up to regression problems with multiple features?,0,qisr9d,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qisr9d/d_can_you_apply_mixup_to_regression_problems_with/,3,"So the equation for mix up is usually

yk = L\*yi     L \*yj and xk = L\*xi  L \*xj

This pertains for feature vectors  Has there been much success applying this to a regression problem with multiple features  So a feature matrix in this case  It s hard for me to imagine since some regression problems can have weird features such as categorical so if you multiply by a lambda and add to another categorical with a lambda doesn t feel like it d help  because then you re treating a categorical as a range instead  Has anyone had any experience with this  Does it work on non neural networks ",1635563542.0,2021-10-30 05:12:22,equation mix usually yk = l\*yi l \*yj xk = l\*xi l \*xj pertains feature vectors much success applying regression problem multiple features feature matrix case hard imagine since regression problems weird features categorical multiply lambda add another categorical lambda feel like help treating categorical range instead anyone experience work non neural networks
[D] EU AI Act - Overview and Roadmap for AI Manufacturers,17,qi6qtk,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qi6qtk/d_eu_ai_act_overview_and_roadmap_for_ai/,5,"Couldn’t make it last time  Were back with a new and improved free webinar 

The EU Commission is about to pass legislation that is going to profoundly change the way AI will be developed 

In the free webinar linked below  we will summarize the key requirements  of the AI Act and will present a roadmap to get your business ready for the upcoming challenges in time 

Register for this LinkedIn event to stay updated   the webinar link will be shared on time 

See you there 

[https  www linkedin com events euaiact overviewandroadmapforai  ] https  www linkedin com events euaiact overviewandroadmapforai ",1635490077.0,2021-10-29 08:47:57,couldn’t make last time back new improved free webinar eu commission pass legislation going profoundly change way ai developed free webinar linked summarize key requirements ai act present roadmap get business ready upcoming challenges time register linkedin event stay updated webinar link shared time see [ www linkedin com events euaiact overviewandroadmapforai ] www linkedin com events euaiact overviewandroadmapforai
[D] How is the landscape for new PhDs in Academia?,3,qigku2,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qigku2/d_how_is_the_landscape_for_new_phds_in_academia/,5,"I m a Canadian applying to graduate school in the field but I d like to get some perspective into what I am getting myself into 

For those who have completed PhDs related to ML or statistical learning  how has your and your colleagues experiences in landing positions in academia  I read that   of PhD graduate end up as professors however there also is counts of an exodus from academia to industry  Does that make it easier for people to land professorship positions  What proportion  roughly speaking  of PhDs in the field that want to enter academia actually do  Are postdocs common or unnecessary  I assume these answers change with location  I m interested in hearing perspectives from around the globe  I would also be quite interested in hearing people who work in academia in the Arabian Gulf  It seems like the GCC countries have been investing heavily into research institutes for AI and the profs there seem to get flush with funding ",1635524692.0,2021-10-29 18:24:52,canadian applying graduate school field like get perspective getting completed phds related ml statistical learning colleagues experiences landing positions academia read phd graduate end professors however also counts exodus academia industry make easier people land professorship positions proportion roughly speaking phds field want enter academia actually postdocs common unnecessary assume answers change location interested hearing perspectives around globe would also quite interested hearing people work academia arabian gulf seems like gcc countries investing heavily research institutes ai profs seem get flush funding
[D] 3D Medical Imaging annotation,1,qilcjy,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qilcjy/d_3d_medical_imaging_annotation/,1,I  work in medical imaging space and we need ground truth D segmentation of anatomical structures in D CT imaging  It is very time consuming process and requires domain expertise  Does anyone know of  annotation services in this space  There seems to be quite a number of services out there offering image annotation services for machine learning  but most of them are focusing on D images  The task in D medical imaging is specialized and more demanding ,1635538369.0,2021-10-29 22:12:49,work medical imaging space need ground truth segmentation anatomical structures ct imaging time consuming process requires domain expertise anyone know annotation services space seems quite number services offering image annotation services machine learning focusing images task medical imaging specialized demanding
[D] Good landmark points annotation tools,0,qil09k,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qil09k/d_good_landmark_points_annotation_tools/,1,"Hello Everyone 
 I want to generate a image dataset with some landmark points on it for the labels  For that I am looking for some landmark annotation tools  Any suggestions would be really helpful  
Thank you ",1635537386.0,2021-10-29 21:56:26,hello everyone want generate image dataset landmark points labels looking landmark annotation tools suggestions would really helpful thank
OneFlow: Redesign the Distributed Deep Learning Framework from Scratch[P],5,qi9hyr,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qi9hyr/oneflow_redesign_the_distributed_deep_learning/,1,"Deep learning frameworks such as TensorFlow and PyTorch provide a productive interface for expressing and training a DNN model on a single device or using data parallelism  Still  they may not be flexible or efficient enough in training emerging large models on distributed devices  which require more sophisticated parallelism beyond data parallelism  Plugins or wrappers have been developed to strengthen these frameworks for model or pipeline parallelism  but they complicate the usage and implementation of distributed deep learning  **Paper **[https  arxiv org pdf   pdf] https  arxiv org pdf   pdf   **Code ** [https  github com Oneflow Inc oneflow] https  github com Oneflow Inc oneflow  

Aiming at a simple  neat redesign of distributed deep learning frameworks for various parallelism paradigms  we present OneFlow  a novel distributed training framework based on an SBP  split  broadcast and partial value  abstraction and the actor model  SBP enables much easier programming of data parallelism and model parallelism than existing frameworks  and the actor model provides a succinct runtime mechanism to manage the complex dependencies imposed by resource constraints  data movement and computation in distributed deep learning 

We demonstrate the general applicability and efficiency of OneFlow for training various large DNN models with case studies and extensive experiments  The results show that OneFlow outperforms many well known customized libraries built on top of the state of the art frameworks ",1635502227.0,2021-10-29 12:10:27,deep learning frameworks tensorflow pytorch provide productive interface expressing training dnn model single device using data parallelism still may flexible efficient enough training emerging large models distributed devices require sophisticated parallelism beyond data parallelism plugins wrappers developed strengthen frameworks model pipeline parallelism complicate usage implementation distributed deep learning **paper **[ arxiv org pdf pdf] arxiv org pdf pdf **code ** [ github com oneflow inc oneflow] github com oneflow inc oneflow aiming simple neat redesign distributed deep learning frameworks various parallelism paradigms present oneflow novel distributed training framework based sbp split broadcast partial value abstraction actor model sbp enables much easier programming data parallelism model parallelism existing frameworks actor model provides succinct runtime mechanism manage complex dependencies imposed resource constraints data movement computation distributed deep learning demonstrate general applicability efficiency oneflow training various large dnn models case studies extensive experiments results show oneflow outperforms many well known customized libraries built top state art frameworks
[D] linking pythons multiprocessing with tensorflows mirrored strategy,0,qimg4k,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qimg4k/d_linking_pythons_multiprocessing_with/,0,"When performing inference on video data and you have access to a gpu cluster for inference does it make since to employ pythons multiprocessing library to read the video file faster while using tensorflows mirrored strategy to distribute each image across the gpu cluster to increase prediction speed  

Or on the other hand  would it make more sense to instantiate multiple threads the grab different frames from the video file and then have the same model loaded onto different gpus and perfrom prediction this way ",1635541723.0,2021-10-29 23:08:43,performing inference video data access gpu cluster inference make since employ pythons multiprocessing library read video file faster using tensorflows mirrored strategy distribute image across gpu cluster increase prediction speed hand would make sense instantiate multiple threads grab different frames video file model loaded onto different gpus perfrom prediction way
[R] From Machine Learning to Robotics: Challenges and Opportunities for Embodied Intelligence,7,qi5z4d,MachineLearning,https://arxiv.org/abs/2110.15245,2,nan,1635486897.0,2021-10-29 07:54:57,nan
"[R] I have been working on a learning/organizing rule of biological neurons for the past 2 years, and I am wondering whatever something similar was already discovered and/or is it worth trying to get published",162,qhljmo,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhljmo/r_i_have_been_working_on_a_learningorganizing/,52,"It s still early to publish the mathematical parts because they are changing a lot  but there are some results for the time being 

  The learning rule produces organizes most if not all primary visual cortex s non complex neurons 

https  preview redd it jlimw png width= format=png auto=webp s=fbaaecbeaadcfe

just by interconnecting several thousands of neurons  and flashing natural images to the neurons through  pixels input neurons 

  The neurons don t distinguish between interconnections and feedforward connections   e g  the interconnections are also recurrent connections like biological neurons 

   The learning rule only depends on the neuron s outputs and its inputs  there is no superprocess like backpropagation that leaks the synapses  information  also no cost function or labels anywhere 

  Likely has machine learning power  A year ago I filmed a cat for  seconds with manual camera rotations zoom in zoom out and I stacked  CNN layers with  filters neurons each  showed the network the video for organization stage  and then stacked a single neuron layer  and made it learn to detect the cat by giving just one point on the cat in a specific frame in the video  and it looks like the neuron somewhat survived the rotation scale transformations  [The video and the activation map of the cat neuron can be downloaded here ] https  easyupload io lmu 

  Converges pretty quickly  about   organization steps 

  The mathematical theoretical s bottom line is that every behavior of an intelligent object should be backed by as much information sources as possible  e g  it s useful for a neuron to depend on many inputs as possible  so when many of its inputs fail  it will continue to function as usual  also this neuron represents real information  because it’s unlikely that many different sources inputs will tell the same information by chance 

Note that this is trying to be mimic the neurons  learning rule which is dissimilar from what spiking neural networks  trying to mimic in the brain  so the neurons here use RELU  rather than integrate and fire etc ",1635422835.0,2021-10-28 14:07:15,still early publish mathematical parts changing lot results time learning rule produces organizes primary visual cortex non complex neurons preview redd jlimw png width= format=png auto=webp s=fbaaecbeaadcfe interconnecting several thousands neurons flashing natural images neurons pixels input neurons neurons distinguish interconnections feedforward connections e g interconnections also recurrent connections like biological neurons learning rule depends neuron outputs inputs superprocess like backpropagation leaks synapses information also cost function labels anywhere likely machine learning power year ago filmed cat seconds manual camera rotations zoom zoom stacked cnn layers filters neurons showed network video organization stage stacked single neuron layer made learn detect cat giving one point cat specific frame video looks like neuron somewhat survived rotation scale transformations [the video activation map cat neuron downloaded ] easyupload io lmu converges pretty quickly organization steps mathematical theoretical bottom line every behavior intelligent object backed much information sources possible e g useful neuron depend many inputs possible many inputs fail continue function usual also neuron represents real information it’s unlikely many different sources inputs tell information chance note trying mimic neurons learning rule dissimilar spiking neural networks trying mimic brain neurons use relu rather integrate fire etc
[R] Create graph from random walks in multidimensional space,1,qifnck,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qifnck/r_create_graph_from_random_walks_in/,0,"Imagine we have a camera equipped robot that performs `k` random walks in a building  not necessarily starting every time from the same location  We collect a set of image sequences `O_  O_    O_k`  each one containing the images captured by the robot in that random walk 

Our goal is to transform the dataset of `k` image sequences into a graph  where images are connected by edges if  given the data  it would be possible for the robot to go from one to the other in less than `d` steps  Moreover  edges need to be *directed* since it might be possible to go from image A to image B  but not vice versa  e g  image A is the image of a glass falling from the table  image B is the glass broken on the floor  Obviously  A B is an edge but B A is not  

The requirements are mainly that a  the task is performed by a NN model since it will need to generalize to new image data after training and  possibly  b  the complexity should be at most O n^  

Could you point me to some relevant literature  I am having trouble finding works that operate in a similar settings ",1635522084.0,2021-10-29 17:41:24,imagine camera equipped robot performs `k` random walks building necessarily starting every time location collect set image sequences `o_ o_ o_k` one containing images captured robot random walk goal transform dataset `k` image sequences graph images connected edges given data would possible robot go one less `d` steps moreover edges need *directed* since might possible go image image b vice versa e g image image glass falling table image b glass broken floor obviously b edge b requirements mainly task performed nn model since need generalize new image data training possibly b complexity n^ could point relevant literature trouble finding works operate similar settings
Question Answering on Tabular Data- A survey [Project],2,qi8jkn,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qi8jkn/question_answering_on_tabular_data_a_survey/,0,"I ve written a blog about the few popular approaches to solve the task of question answering on tabular data  and also included one of my side project which led to a solution on the same 

Check it out here 

[https  blog paperspace com tapas question answering ] https  blog paperspace com tapas question answering 

Also  this is the side project  

[https  github com abhijithneilabraham tableQA] https  github com abhijithneilabraham tableQA 

Looking for contributors in the same space  so feel free to drop a DM",1635498028.0,2021-10-29 11:00:28,written blog popular approaches solve task question answering tabular data also included one side project led solution check [ blog paperspace com tapas question answering ] blog paperspace com tapas question answering also side project [ github com abhijithneilabraham tableqa] github com abhijithneilabraham tableqa looking contributors space feel free drop dm
[D] Why only accuracy as evaluation criteria in Few-shot and zero-shot learning?,0,qi99ka,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qi99ka/d_why_only_accuracy_as_evaluation_criteria_in/,0,I have seen that majority of few and zero shot learning papers use Accuracy as a benchmark  Unlike other domains like object detection  classical image recognition  why they don t use other benchmarks like Precision  recall or other criteria apart from Accuracy  Is it due to others are doing it  so people keep following  or other performance metrics are not reliable in a few shot domain  ,1635501273.0,2021-10-29 11:54:33,seen majority zero shot learning papers use accuracy benchmark unlike domains like object detection classical image recognition use benchmarks like precision recall criteria apart accuracy due others people keep following performance metrics reliable shot domain
[D] Uncomfortably framed Keras tutorial on heart disease prediction,7,qhyhxs,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhyhxs/d_uncomfortably_framed_keras_tutorial_on_heart/,4,"https  keras io examples structured_data structured_data_classification_from_scratch 

This example in the Keras documentation makes me uncomfortable with how little it mentions generalization and the importance of evaluation 

Many developers take their first steps in ML via Keras  particulary as it s featured in so many GCP courses  so it s important to contextualize the predictions and manage expectations of what one should reasonably expect when minimizing empirical risk and hoping for generalization 

 We use the features to predict whether a patient has a heart disease  [ ] The last column   target   indicates whether the patient has a heart disease    or not     

No notion of imbalanced data or what it means for a patient to  have a heart disease   Are there different kinds  How about levels of severity  Perhaps not super important for an introductory tutorial  but this last statement makes me uncomfortable 

 This particular patient had a   percent probability of having a heart disease  as evaluated by our model  

I think this should be wrapped with cautions  else it might lead new learners of Keras into having an overconfidence in what to responsibly expect from a model 

What do you think  Am I overly sensitive here  Something about this being about predicting people s heart disease  where wrong predictions are really bad  makes it especially important  I feel ",1635460938.0,2021-10-29 00:42:18,keras io examples structured_data structured_data_classification_from_scratch example keras documentation makes uncomfortable little mentions generalization importance evaluation many developers take first steps ml via keras particulary featured many gcp courses important contextualize predictions manage expectations one reasonably expect minimizing empirical risk hoping generalization use features predict whether patient heart disease [ ] last column target indicates whether patient heart disease notion imbalanced data means patient heart disease different kinds levels severity perhaps super important introductory tutorial last statement makes uncomfortable particular patient percent probability heart disease evaluated model think wrapped cautions else might lead new learners keras overconfidence responsibly expect model think overly sensitive something predicting people heart disease wrong predictions really bad makes especially important feel
[D] How do you deal with covariate shift and concept drift in production?,27,qhnao8,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhnao8/d_how_do_you_deal_with_covariate_shift_and/,36,"To oversimplify  in reality you frequently have covariate shift  or just sample selecion bias  between your test and train set  essentially `P X ` being changed across time  Aside from this there s the case where there is no covariate shift but `p y x ` is altered nonetheless  concept drift  From what I ve read the former can be monitored in the upstream data but the latter frequently requires some feedback mechanism that involves ground truth labelling 

I have a few days off from work per year to dedicate to research and I m looking to tackle these two problems by implementing something that is  simple enough   I ve started off by reading a number of survey papers and then digging a bit deeper with some more specific papers 

So far I ve been thinking about measuring the distribution of the data upstream  either with process control charts or dividing the data into temporal windows and using KL divergence to measure covariate shift  I m especially hopeful of the process control charts as hey may be simple enough for non data stakeholders to understand  After this I ll consider a retraining   reweighting strategies like ensembles  transfer learning etc 

For the case of concept drift I ll just be looking to continuously provide feedback in terms of ground truth labels and keep track of the performance over time  Potentially plotting the daily weekly AUC in a process control chart too  After this I ll consider  the same  retraining   reweighting strategies 

Has anyone done anything similar at work  Do you have any recommended readings  If you haven t   do you have any comments on the approach so far 

Thanks in advance ",1635428403.0,2021-10-28 15:40:03,oversimplify reality frequently covariate shift sample selecion bias test train set essentially `p x ` changed across time aside case covariate shift `p x ` altered nonetheless concept drift read former monitored upstream data latter frequently requires feedback mechanism involves ground truth labelling days work per year dedicate research looking tackle two problems implementing something simple enough started reading number survey papers digging bit deeper specific papers far thinking measuring distribution data upstream either process control charts dividing data temporal windows using kl divergence measure covariate shift especially hopeful process control charts hey may simple enough non data stakeholders understand consider retraining reweighting strategies like ensembles transfer learning etc case concept drift looking continuously provide feedback terms ground truth labels keep track performance time potentially plotting daily weekly auc process control chart consider retraining reweighting strategies anyone done anything similar work recommended readings comments approach far thanks advance
[R] Learning Graph Cellular Automata,5,qhrgxr,MachineLearning,https://arxiv.org/abs/2110.14237,2,nan,1635440440.0,2021-10-28 19:00:40,nan
[D] How do you know when your model is good enough to deploy to production?,7,qhpdoo,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhpdoo/d_how_do_you_know_when_your_model_is_good_enough/,11,After you have trained a model  how do you know that it is good enough to deploy without it malfunctioning ,1635434398.0,2021-10-28 17:19:58,trained model know good enough deploy without malfunctioning
[N] Amazon launches AWS instances powered by Habana’s AI accelerator chip,100,qh95on,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qh95on/n_amazon_launches_aws_instances_powered_by/,20,"https  venturebeat com    amazon launches aws instances powered by habanas ai accelerator chip 

  Amazon Web Services  AWS   Amazon’s cloud services division  today announced the general availability of Elastic Compute Cloud  EC  DL instances  While new instance types generally aren’t particularly novel  DL  specifically DL xlarge  is the first type in AWS designed for training machine learning models  Amazon says — powered by Gaudi accelerators from Intel owned Habana Labs 
  
 ",1635376254.0,2021-10-28 01:10:54,venturebeat com amazon launches aws instances powered habanas ai accelerator chip amazon web services aws amazon’s cloud services division today announced general availability elastic compute cloud ec dl instances new instance types generally aren’t particularly novel dl specifically dl xlarge first type aws designed training machine learning models amazon says — powered gaudi accelerators intel owned habana labs
[D] State of the art in the document information extraction/parsing for resume parsing?,1,qhv879,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhv879/d_state_of_the_art_in_the_document_information/,1,"Hi everyone 


I ve been looking for state of the art research paper project code for automatically extracting information from various layout of resumes 


Typical workflow I can estimate is to convert resume to image  detect text  table etc  apply rule based heuristic approach to extract the information based on NER etc  but I think that would be an outdated approach and will not be accurate and feasible enough to cover all the cases 


Need to extract information like Name  Contact details  skills  projects  company  job tenure and other resume related data 


I d really appreciate if you have could share any information experience in this regard 


Thanks",1635451138.0,2021-10-28 21:58:58,hi everyone looking state art research paper project code automatically extracting information various layout resumes typical workflow estimate convert resume image detect text table etc apply rule based heuristic approach extract information based ner etc think would outdated approach accurate feasible enough cover cases need extract information like name contact details skills projects company job tenure resume related data really appreciate could share information experience regard thanks
[D] Package for ordinal regression with automatic cross-validation?,0,qhz9h5,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhz9h5/d_package_for_ordinal_regression_with_automatic/,2,Ideally in python or R  I d like to fit my data to a logit ordinal regression model and perform LOOCV for a l penalty  sklearn doesn t seem to support this  nor does the statsmodels module  rms in R has an option for an l penalty  but no cross validation  I suppose with a script of my own I can do what I want  but I want to avoid re inventing the wheel if possible ,1635463375.0,2021-10-29 01:22:55,ideally python r like fit data logit ordinal regression model perform loocv l penalty sklearn seem support statsmodels module rms r option l penalty cross validation suppose script want want avoid inventing wheel possible
[D][P] Would you consider using this paradigm for writing features?,0,qhy4dq,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhy4dq/dp_would_you_consider_using_this_paradigm_for/,6,"Hi I m one of the authors that created [https  github com stitchfix hamilton] https  github com stitchfix hamilton  \  see [https  multithreaded stitchfix com blog    functions dags hamilton ] https  multithreaded stitchfix com blog    functions dags hamilton  for context on how it came to be 

The core of the idea is that people write functions that look like this 

    def column_c column_a  pd Series  column_b  pd Series    pd Series 
         Some doc string 
        return column_a   column_b

instead of 

    df[ column_c ] = df[ column_a ]   df[ column_b ]

Yes  I agree  if you re doing small things  then this is probably overkill  However if you are on a team  or have lots  e g  s  of transforms defined over functions then that s where Hamilton really shines  it enables very uniform code  documentation  unit testable   no glue code  low maintenance 

But anyway  back to the question  would this be a paradigm you could see yourself coding in 

[View Poll] https  www reddit com poll qhydq ",1635459727.0,2021-10-29 00:22:07,hi one authors created [ github com stitchfix hamilton] github com stitchfix hamilton \ see [ multithreaded stitchfix com blog functions dags hamilton ] multithreaded stitchfix com blog functions dags hamilton context came core idea people write functions look like def column_c column_a pd series column_b pd series pd series doc string return column_a column_b instead df[ column_c ] = df[ column_a ] df[ column_b ] yes agree small things probably overkill however team lots e g transforms defined functions hamilton really shines enables uniform code documentation unit testable glue code low maintenance anyway back question would paradigm could see coding [view poll] www reddit com poll qhydq
[D] What even are frequencies in images?,2,qhq1zi,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhq1zi/d_what_even_are_frequencies_in_images/,9,"When dealing with newer Computer Vision methods  I struggle to understand some signal processing related concepts  first and foremost frequencies in images  I understand frequencies that occur in e g  sound  but where does a time axis come from in images 

Perhaps this lack of understanding is even deeper  e g  what is a „signal“  or signal to noise ratio  when dealing with images ",1635436425.0,2021-10-28 17:53:45,dealing newer computer vision methods struggle understand signal processing related concepts first foremost frequencies images understand frequencies occur e g sound time axis come images perhaps lack understanding even deeper e g „signal“ signal noise ratio dealing images
"[D] Peter Henderson on RL Benchmarking, Climate Impacts of AI, and AI for Law",0,qhupqr,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhupqr/d_peter_henderson_on_rl_benchmarking_climate/,1,"Hey  you might dig our new [interview] https  thegradientpub substack com p peter henderson on rl benchmarking  with Stanford JD PhD Peter Henderson  whose research is about creating robust decision making systems to create new ML methods for applications that are beneficial to society 

[https  thegradientpub substack com p peter henderson on rl benchmarking] https  thegradientpub substack com p peter henderson on rl benchmarking 

https  preview redd it ovsehrtw png width= format=png auto=webp s=fbbaddeeebfb

 We discuss 

* [Reproducibility and Reusability in Deep Reinforcement Learning] https  www peterhenderson co publication henderson    
* [Benchmark Environments for Multitask Learning in Continuous Domains] https  arxiv org pdf   pdf 
* [Reproducibility of Bench marked Deep Reinforcement Learning Tasks for Continuous Control ] https  arxiv org abs   
* [Deep Reinforcement Learning that Matters] https  www peterhenderson co publication henderson  deep 
* [Reproducibility and Replicability in Deep Reinforcement Learning  and Other Deep Learning Methods ] https  www peterhenderson co talk ssc 
* [Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning] https  arxiv org abs   
* [How  blockers can turn into a paper  A retrospective on  Towards The  Systematic Reporting of the Energy and Carbon Footprints of Machine  Learning] https  www peterhenderson co talk mlretro 
* [When Does Pretraining Help  Assessing Self Supervised Learning for Law and the CaseHOLD Dataset] https  arxiv org pdf   ”
* [How US law will evaluate artificial intelligence for Covid ] https  www bmj com content  bmj n full 

Apologies for the self promo  but hope you enjoy ",1635449652.0,2021-10-28 21:34:12,hey might dig new [interview] thegradientpub substack com p peter henderson rl benchmarking stanford jd phd peter henderson whose research creating robust decision making systems create new ml methods applications beneficial society [ thegradientpub substack com p peter henderson rl benchmarking] thegradientpub substack com p peter henderson rl benchmarking preview redd ovsehrtw png width= format=png auto=webp s=fbbaddeeebfb discuss * [reproducibility reusability deep reinforcement learning] www peterhenderson co publication henderson * [benchmark environments multitask learning continuous domains] arxiv org pdf pdf * [reproducibility bench marked deep reinforcement learning tasks continuous control ] arxiv org abs * [deep reinforcement learning matters] www peterhenderson co publication henderson deep * [reproducibility replicability deep reinforcement learning deep learning methods ] www peterhenderson co talk ssc * [towards systematic reporting energy carbon footprints machine learning] arxiv org abs * [how blockers turn paper retrospective towards systematic reporting energy carbon footprints machine learning] www peterhenderson co talk mlretro * [when pretraining help assessing self supervised learning law casehold dataset] arxiv org pdf ” * [how us law evaluate artificial intelligence covid ] www bmj com content bmj n full apologies self promo hope enjoy
[D] Migration of big datasets into the clouds,2,qhpncx,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhpncx/d_migration_of_big_datasets_into_the_clouds/,10,"lets say i have  to  GB of float  csv files to train a tensorflow model on 

  **How would migrating those into the cloud work in terms of timeefficiency **
  e g  if the machine is  Hour  wouldnt that be a whole day for uploading it already  Compressing   and decompressing it wouldnt work either  since the machine should  only  have  cores  so that would take more time than it saves i guess   
    **How does that work with vast ai  amazon aws  and google  **
    Or am i bound to buy my own workstation ",1635435168.0,2021-10-28 17:32:48,lets say gb float csv files train tensorflow model **how would migrating cloud work terms timeefficiency ** e g machine hour wouldnt whole day uploading already compressing decompressing wouldnt work either since machine cores would take time saves guess **how work vast ai amazon aws google ** bound buy workstation
[R] Two NeurIPS 2021 Papers on Weak-shot Learning,0,qi135y,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qi135y/r_two_neurips_2021_papers_on_weakshot_learning/,1,"I am glad to announce that we have two papers on weak shot learning accepted by NeurIPS   Weak shot learning is a learning paradigm with full annotations for base categories and weak annotations for novel categories 

The first paper is about weak shot classification   Weak shot Fine grained Classification via Similarity Transfer  

paper link  [https  arxiv org pdf   pdf] https  arxiv org pdf   pdf 

code and dataset  [https  github com bcmi SimTrans Weak Shot Classification] https  github com bcmi SimTrans Weak Shot Classification 

The second paper is about weak shot object detection    Mixed Supervised Object Detection by Transferring Mask Prior and Semantic Similarity   

paper link  [https  arxiv org pdf   pdf] https  arxiv org pdf   pdf 

code  [https  github com bcmi TraMaS Weak Shot Object Detection] https  github com bcmi TraMaS Weak Shot Object Detection 

I have also written a brief introduction to weak shot learning [https  arxiv org pdf   pdf] https  arxiv org pdf   pdf  and summarized the related papers codes at [https  github com bcmi Awesome Weak Shot Learning] https  github com bcmi Awesome Weak Shot Learning   Welcome to pay attention to weak shot learning ",1635469315.0,2021-10-29 03:01:55,glad announce two papers weak shot learning accepted neurips weak shot learning learning paradigm full annotations base categories weak annotations novel categories first paper weak shot classification weak shot fine grained classification via similarity transfer paper link [ arxiv org pdf pdf] arxiv org pdf pdf code dataset [ github com bcmi simtrans weak shot classification] github com bcmi simtrans weak shot classification second paper weak shot object detection mixed supervised object detection transferring mask prior semantic similarity paper link [ arxiv org pdf pdf] arxiv org pdf pdf code [ github com bcmi tramas weak shot object detection] github com bcmi tramas weak shot object detection also written brief introduction weak shot learning [ arxiv org pdf pdf] arxiv org pdf pdf summarized related papers codes [ github com bcmi awesome weak shot learning] github com bcmi awesome weak shot learning welcome pay attention weak shot learning
"HACKtheMACHINE Unmanned [Virtual challenge, $90k in prizes, FREE entry, CS Bug Bounty, AI/ML Challenge, MBSE Challenge] [D]",0,qhq5x4,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhq5x4/hackthemachine_unmanned_virtual_challenge_90k_in/,0," xB 

https  preview redd it wjvwmwprw png width= format=png auto=webp s=accabfedecdcdbdfdaae

The U S  Navy s premiere digital experience  HACKtheMACHINE is happening again from November      There are three events this time around  Hack the Pilot  Hack into a real unmanned vehicle autopilot system and get paid for finding bugs  Detective Bot  Use AI ML to detect malicious data in high security environments  Top Model  Employ MBSE to simulate unmanned swarm missions  All of this and more from the comfort of your couch  Up to  k in cash prizes  Free to enter 

This event started in  as an effort to find solutions to the Navy s biggest technological problems  while providing a venue for Sailors in the Navy cybersecurity space to interface with their private sector counterparts 

To learn more visit [HACKtheMACHINE ai] https  www HACKtheMACHINE ai ",1635436745.0,2021-10-28 17:59:05,xb preview redd wjvwmwprw png width= format=png auto=webp s=accabfedecdcdbdfdaae u navy premiere digital experience hackthemachine happening november three events time around hack pilot hack real unmanned vehicle autopilot system get paid finding bugs detective bot use ai ml detect malicious data high security environments top model employ mbse simulate unmanned swarm missions comfort couch k cash prizes free enter event started effort find solutions navy biggest technological problems providing venue sailors navy cybersecurity space interface private sector counterparts learn visit [hackthemachine ai] www hackthemachine ai
[D] Interview Questions asked in AI/DL Research Scientist positions at top techs,35,qh59gp,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qh59gp/d_interview_questions_asked_in_aidl_research/,22,"I was curious about the kind of questions that are asked in an interviews for AI DL Research Scientist  or similar  positions in top tech companies like FAANG  but not limited to  of course   I tried finding online but could only get some vague answer without specifics  Those who experienced such interviews  can you share some questions topics that you faced in such research interviews  

While answering  please consider the following 

  Be specific  Don t write something vague  like  Basics of classifiers  Standard questions on SVM 
  You can write in detail  like specific equations  theorems  follow up questions  specific papers discussed etc 
  Software Engineering style questions are not preferred  try to focus on core research questions topics 

*PS  You don t have to specify the name of the company if you don t want to  or whether you got the position or not *",1635365016.0,2021-10-27 22:03:36,curious kind questions asked interviews ai dl research scientist similar positions top tech companies like faang limited course tried finding online could get vague answer without specifics experienced interviews share questions topics faced research interviews answering please consider following specific write something vague like basics classifiers standard questions svm write detail like specific equations theorems follow questions specific papers discussed etc software engineering style questions preferred try focus core research questions topics *ps specify name company want whether got position *
[Research] Feeding coordinates (Lat/Long or Projection) into neural network properly,97,qgvkkm,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qgvkkm/research_feeding_coordinates_latlong_or/,75,"hi there 

I m looking to predict river level change with time using several stations distributed around a big geographical space  I have determined the parameters  however  one of the parameters which proved effective is the location of nearby measurements stations to the observed one  so I want my neural network to be spatially aware of each well location when its training the model 

things I have considered so far 

  Calculate the Euclidian Distance between each well in relation to the observed well and use that as input  if i have  wells  then i would add  new variables to the existing parameters which will be inputs to the ANN  
  Use Hillbert s space filling curve to convert the D coordinates into D and feed it to the ANN 

However  so far Option    would give inaccurate results for similar distances as its a scaler value not vector  and for Option     I have seen people here in the forum say that its not appropriate to use it as its inverse is not continuous and thus inappropriate to use as input 

is what I have summarized so far correct  and is there any other option aside from the above to make the network recognize geographic locations 

your help feedback would be extremely appreciated as I have been stuck at this issue for a while now 

EDIT  the responses have been extremely helpful and i am immensely grateful to all of you  i wanted to add a few things to the thread as the answers are raising a few issues in my mind 

when i created this thread  my original goal was to find a way to feed the network a map and let it know the locations of the measurement stations in it  and honestly i thought there would be a standard or a widespread way to do so but everyone is doing it in his own way and some more complicated than others and unfortunately with no degree of the effectiveness of each method 

i was speaking to a math friend who suggested to project the Lat Long into UTM zone and use either Hilbert curve or Fourier transform to convert it into one dimension or frequency domain and feed it to the network  she said she wasn t sure of the solution but it might work so i thought i would add it to the discussion as option    ",1635337758.0,2021-10-27 14:29:18,hi looking predict river level change time using several stations distributed around big geographical space determined parameters however one parameters proved effective location nearby measurements stations observed one want neural network spatially aware well location training model things considered far calculate euclidian distance well relation observed well use input wells would add new variables existing parameters inputs ann use hillbert space filling curve convert coordinates feed ann however far option would give inaccurate results similar distances scaler value vector option seen people forum say appropriate use inverse continuous thus inappropriate use input summarized far correct option aside make network recognize geographic locations help feedback would extremely appreciated stuck issue edit responses extremely helpful immensely grateful wanted add things thread answers raising issues mind created thread original goal find way feed network map let know locations measurement stations honestly thought would standard widespread way everyone way complicated others unfortunately degree effectiveness method speaking math friend suggested project lat long utm zone use either hilbert curve fourier transform convert one dimension frequency domain feed network said sure solution might work thought would add discussion option
[D] Machine learning generative models for automatic design of multi-material 3D printed composite solids,1,qhl60o,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhl60o/d_machine_learning_generative_models_for/,0,Hello   In the following paper  I cannot find the source code they implemented  I tried to reach the author no reply   I am a newbie in that area of generative design for multi material  I am wondering if anyone is familiar with similar work with source code attached     Source  [https  www sciencedi] https  www sciencedi ,1635421516.0,2021-10-28 13:45:16,hello following paper cannot find source code implemented tried reach author reply newbie area generative design multi material wondering anyone familiar similar work source code attached source [ www sciencedi] www sciencedi
[D] TargetCLIP explained - Image-Based CLIP-Guided Essence Transfer (5-minute summary by Casual GAN Papers),27,qgzoey,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qgzoey/d_targetclip_explained_imagebased_clipguided/,3,"There  has recently been a lot of interest concerning a new generation of   style transfer models  These work on a higher level of abstraction and   rather than focusing on transferring colors and textures from one image   to another  they combine the conceptual “style” of one image and the   objective “content” of another in an entirely new image altogether  A   recent paper by Hila Chefer and the team at Tel Aviv University does   just that  The authors propose TargetCLIP  a blending operator that   combines the powerful StyleGAN generator with a semantic network CLIP   to achieve a more natural blending than with each model separately  On a   practical level  this idea is implemented with two losses   one that   ensures the output image is similar to the input in the CLIP space  the   other   that the shifts in the CLIP space are linked to shifts in the   StyleGAN space 

Full summary  [https  t me casual\_gan ] https  t me casual_gan  

[TargetCLIP] https  preview redd it zvejtumkw jpg width= format=pjpg auto=webp s=aeefeecbcd 

arxiv  [https  arxiv org pdf   pdf] https  arxiv org pdf   pdf 

code  [https  github com hila chefer TargetCLIP] https  github com hila chefer TargetCLIP 

web digest  [https  www casualganpapers com clip\_image\_to\_image\_style\_transfer\_essence\_transfer TargetCLIP explained html] https  www casualganpapers com clip_image_to_image_style_transfer_essence_transfer TargetCLIP explained html 

Subscribe to [Casual GAN Papers] https  t me casual_gan  and follow me on [Twitter] https  twitter com KirillDemochkin  for weekly AI paper summaries ",1635349687.0,2021-10-27 17:48:07,recently lot interest concerning new generation style transfer models work higher level abstraction rather focusing transferring colors textures one image another combine conceptual “style” one image objective “content” another entirely new image altogether recent paper hila chefer team tel aviv university authors propose targetclip blending operator combines powerful stylegan generator semantic network clip achieve natural blending model separately practical level idea implemented two losses one ensures output image similar input clip space shifts clip space linked shifts stylegan space full summary [ casual\_gan ] casual_gan [targetclip] preview redd zvejtumkw jpg width= format=pjpg auto=webp s=aeefeecbcd arxiv [ arxiv org pdf pdf] arxiv org pdf pdf code [ github com hila chefer targetclip] github com hila chefer targetclip web digest [ www casualganpapers com clip\_image\_to\_image\_style\_transfer\_essence\_transfer targetclip explained html] www casualganpapers com clip_image_to_image_style_transfer_essence_transfer targetclip explained html subscribe [casual gan papers] casual_gan follow [twitter] twitter com kirilldemochkin weekly ai paper summaries
[Discussion] Aren't all unserpervised learning tasks basically clustering afterall ?,2,qhj1e1,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhj1e1/discussion_arent_all_unserpervised_learning_tasks/,7,"If  I think of unsupervised learning  I basically think of clustering but  also anomaly detection  learning the Gaussian mixture of a probability  distribution or learning latent variables in a data distribution 

But fundamentally isn t the purpose of anomaly detection to detect when a data point is outside a given  or learned  cluster  

Isn t  learning a mixture to regroup the points of high probability density  and regroup them inside a cluster that we condition to be Gaussian 

Isn t learning latent variable of a distribution discovering the implict clusters of your data distribution  

Isn t all the unsupervised task basically relying on a clustering  

I  ask those questions to understand unsupervised learning  because  like  many of you I think  I see those unsupervised tasks as the only  philosophical way to reach a global intelligence scheme 

I  would like to see if we cannot extend the limits of clustering  see the  impossibility theorem for example   to every unsupervised tasks 

Thank you in advance for your answers or remarks 

PS   Already post in r ArtificialInteligence but they is fewer people there  so sorry for the double  I m still new to this website ",1635412840.0,2021-10-28 11:20:40,think unsupervised learning basically think clustering also anomaly detection learning gaussian mixture probability distribution learning latent variables data distribution fundamentally purpose anomaly detection detect data point outside given learned cluster learning mixture regroup points high probability density regroup inside cluster condition gaussian learning latent variable distribution discovering implict clusters data distribution unsupervised task basically relying clustering ask questions understand unsupervised learning like many think see unsupervised tasks philosophical way reach global intelligence scheme would like see cannot extend limits clustering see impossibility theorem example every unsupervised tasks thank advance answers remarks ps already post r artificialinteligence fewer people sorry double still new website
[D] Time series generation using GANs - when to stop training?,15,qh26yp,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qh26yp/d_time_series_generation_using_gans_when_to_stop/,9,For image generation  one may inspect the synthesized images or rely on metrics like inception score and FID  But time series cannot be visually  confirmed  and I haven t been able to find peer reviewed work for time series specific metrics ,1635356640.0,2021-10-27 19:44:00,image generation one may inspect synthesized images rely metrics like inception score fid time series cannot visually confirmed able find peer reviewed work time series specific metrics
[P] PyCM 3.3 released: Comparison of Classifiers Based on Confusion Matrix,6,qh7flk,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qh7flk/p_pycm_33_released_comparison_of_classifiers/,0,"Hi ML practitioners 

We wanted to bring to your attention another release of PyCM  Multi class confusion matrix library in Python   In this version  the comparison system bugs are fixed and now it is possible to compare classifiers with more advanced settings 

 xB 

https  preview redd it dokaw png width= format=png auto=webp s=fecabcbafdcacf

  
 

https  preview redd it hzdasdaw png width= format=png auto=webp s=adccbacdbcca

      from pycm import ConfusionMatrix  Compare
      cm = ConfusionMatrix matrix={ {     }  {     }  {     }} 
      cm = ConfusionMatrix matrix={ {     }  {     }  {     }} 
      cm print_matrix 
    Predict                          
    Actual
                                   
    
                                   
    
                                   
    
    
      cm print_matrix 
    Predict                          
    Actual
                                   
    
                                   
    
                                   
      cp = Compare { cm  cm  cm  cm} class_weight={     } 
      print cp 
    Best   cm
    
    Rank  Name   Class Score       Overall Score
         cm                 
         cm                  
    
      cp best_name
     cm 
      cp = Compare { cm  cm  cm  cm} by_class=True class_weight={     } 
      print cp 
    Best   cm
    
    Rank  Name   Class Score       Overall Score
         cm                 
         cm                 
    
      cp = Compare { cm  cm  cm  cm} class_benchmark_weight={ PLRI    NLRI    DPI    AUCI    MCCI    QI  } 
      print cp 
    Best   cm
    
    Rank  Name   Class Score       Overall Score
         cm                 
         cm                 
    
      cp best_name
     cm 

Website   [www pycm ir] https  www pycm ir 

Repo   [https  github com sepandhaghighi pycm] https  github com sepandhaghighi pycm 

 xB 

Hope you find it useful ",1635371090.0,2021-10-27 23:44:50,hi ml practitioners wanted bring attention another release pycm multi class confusion matrix library python version comparison system bugs fixed possible compare classifiers advanced settings xb preview redd dokaw png width= format=png auto=webp s=fecabcbafdcacf preview redd hzdasdaw png width= format=png auto=webp s=adccbacdbcca pycm import confusionmatrix compare cm = confusionmatrix matrix={ { } { } { }} cm = confusionmatrix matrix={ { } { } { }} cm print_matrix predict actual cm print_matrix predict actual cp = compare { cm cm cm cm} class_weight={ } print cp best cm rank name class score overall score cm cm cp best_name cm cp = compare { cm cm cm cm} by_class=true class_weight={ } print cp best cm rank name class score overall score cm cm cp = compare { cm cm cm cm} class_benchmark_weight={ plri nlri dpi auci mcci qi } print cp best cm rank name class score overall score cm cm cp best_name cm website [www pycm ir] www pycm ir repo [ github com sepandhaghighi pycm] github com sepandhaghighi pycm xb hope find useful
[R] The Efficiency Misnomer,5,qh4v7u,MachineLearning,https://arxiv.org/abs/2110.12894,1,nan,1635363934.0,2021-10-27 21:45:34,nan
[D] Is Pytorch Lightning Production Ready?,26,qgqq07,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qgqq07/d_is_pytorch_lightning_production_ready/,28,"Hi 

I m planning to work on multiple RnD projects that are going to be released into products if they indicate good performance  and I was thinking of correctly choosing my stack  Ever since I ve discovered PL  my life has become way more eaiser than before  but I was always doing solo stuff working on research projects  Now we are trying to design implement an ML training loop  I was thinking of choosing PL as an interface over pytorch to automate a lot of things that we were going to write from scratch  What is the general consensus of PL for production in here  Is it unrelaible ",1635318181.0,2021-10-27 09:03:01,hi planning work multiple rnd projects going released products indicate good performance thinking correctly choosing stack ever since discovered pl life become way eaiser always solo stuff working research projects trying design implement ml training loop thinking choosing pl interface pytorch automate lot things going write scratch general consensus pl production unrelaible
"[D] Deciding to publish, but where?",12,qgtnz2,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qgtnz2/d_deciding_to_publish_but_where/,9,"Hello guys  I m just curious about where will I publish my works in the future 

As you know  there s a various of conferences in the field of AI such as ICCV  ECCV  ICML  NeurlIPS  AAAI  etc 

However  In my perspective  I don t know what is different specifcally  I know that all of them is for computer vision fields  and they all are top conferences  but where will I decide publicating my works 

Usually  this question would be conveyed to my supervisor  but I don t have it 

Thank you all ",1635330853.0,2021-10-27 12:34:13,hello guys curious publish works future know various conferences field ai iccv eccv icml neurlips aaai etc however perspective know different specifcally know computer vision fields top conferences decide publicating works usually question would conveyed supervisor thank
[D] What are some SOTA image classification architectures that run on edge-devices?,5,qgvefy,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qgvefy/d_what_are_some_sota_image_classification/,2,I am trying to deploy a classification model on a Raspberry Pi  that is set up with PyTorch     The device is connected to a camera that periodically takes sky photographs and does a simple cloud classification task  I now have a small dataset with me and I am thinking of using a classification model that can run on the GB version ,1635337173.0,2021-10-27 14:19:33,trying deploy classification model raspberry pi set pytorch device connected camera periodically takes sky photographs simple cloud classification task small dataset thinking using classification model run gb version
[D] The Ownership Dilemma of ML Pipelines In Production,0,qh62wg,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qh62wg/d_the_ownership_dilemma_of_ml_pipelines_in/,4,"One question that organizations developing machine learning need to answer is **who owns ML pipelines in production**  Is it the data scientist who creates the model  Is it the data engineer who deploys it in production  Is it someone else altogether   


My personal opinion here is that it only makes sense for the producer of the models  i e  the data scientist  to be the one who takes the model to production  Obviously this requires higher abstractions  to accommodate for the skill set gap between a production deployment vs the often script like experimentation phase in the ML development lifecycle    


I ve hashed the argument above in a blog post here  [https  towardsdatascience com taking on the ml pipeline challenge dfebbcc] https  towardsdatascience com taking on the ml pipeline challenge dfebbcc    


Would be happy to hear opinions on what people in this community think   Should the process of taking ML to production be owned by the data scientist  or should the data scientist just  stay in their lanes  and throw it over the wall to more engineering driven teams ",1635367293.0,2021-10-27 22:41:33,one question organizations developing machine learning need answer **who owns ml pipelines production** data scientist creates model data engineer deploys production someone else altogether personal opinion makes sense producer models e data scientist one takes model production obviously requires higher abstractions accommodate skill set gap production deployment vs often script like experimentation phase ml development lifecycle hashed argument blog post [ towardsdatascience com taking ml pipeline challenge dfebbcc] towardsdatascience com taking ml pipeline challenge dfebbcc would happy hear opinions people community think process taking ml production owned data scientist data scientist stay lanes throw wall engineering driven teams
"How can I train an RNN on data with multiple potentially ""correct"" outputs for each input? [R]",2,qh12bn,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qh12bn/how_can_i_train_an_rnn_on_data_with_multiple/,3,"Specifically   I m thinking about a cryptography problem in which a  text is  enciphered with a simple cipher  for example Vigenere  but  spaces are  preserved  I d like to teach an RNN  or Transformer  to do   cribbing   in the following way 

Input  a sequence containing the characters of a given enciphered word of length *L*  plus positional encodings 

Output  a sequence of numbers that is then converted to characters 

Loss  minimum Levenshtein distance between the output and any valid English word of length *L*

How  would I actually set up this architecture  namely  is the fact  that  I m not comparing against a fixed output for each sample a  concern    A  concern I have with the way I ve formulated the problem  above is that  the network could simply learn to output the same English  word for  every sequence of a given length  how could I prevent against  this ",1635353493.0,2021-10-27 18:51:33,specifically thinking cryptography problem text enciphered simple cipher example vigenere spaces preserved like teach rnn transformer cribbing following way input sequence containing characters given enciphered word length *l* plus positional encodings output sequence numbers converted characters loss minimum levenshtein distance output valid english word length *l* would actually set architecture namely fact comparing fixed output sample concern concern way formulated problem network could simply learn output english word every sequence given length could prevent
[Discussion] Model 2 Binary executable,2,qh0hcn,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qh0hcn/discussion_model_2_binary_executable/,1,I have some models that could potentially work very well in realtime CPU inference and I ve been thinking to create somehow a binary executable to use them or some kind of real software  What are my options  Did any of you create something like this ,1635351880.0,2021-10-27 18:24:40,models could potentially work well realtime cpu inference thinking create somehow binary executable use kind real software options create something like
[R] Parameter Prediction for Unseen Deep Architectures,19,qgnvyt,MachineLearning,https://arxiv.org/abs/2110.13100,5,nan,1635306796.0,2021-10-27 05:53:16,nan
[D] A Guide to Tesla’s Configurable Floating Point Formats & Arithmetic,44,qghljd,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qghljd/d_a_guide_to_teslas_configurable_floating_point/,26,"Tesla just randomly dropped a PDF with details of the custom floating point formats they ve created for their Dojo training hardware  https  tesla cdn thron com static SBYB_tesla dojo technology_OPNZM pdf

I think it s pretty interesting  They want to eliminate  bit floating point from training almost entirely  using custom  bit and even  bit floating point formats instead  with a configurable  exponent bias  that is shared between many numbers and can apparently be learned during training  Also  they have stochastic rounding which seems like a great idea for low precision formats  Worth a glance if you care about hardware ",1635286259.0,2021-10-27 00:10:59,tesla randomly dropped pdf details custom floating point formats created dojo training hardware tesla cdn thron com static sbyb_tesla dojo technology_opnzm pdf think pretty interesting want eliminate bit floating point training almost entirely using custom bit even bit floating point formats instead configurable exponent bias shared many numbers apparently learned training also stochastic rounding seems like great idea low precision formats worth glance care hardware
[D] How can companies like Facebook use Pytorch for commercial applications when BN and dropout are patented?,238,qg4750,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qg4750/d_how_can_companies_like_facebook_use_pytorch_for/,106,"Tensorflow has [Apache License  ] https  www apache org licenses LICENSE     so layers such as Batch Norm and Dropout are covered in the license  Pytorch uses [The  Clause BSD License] https  opensource org licenses BSD  Clause   which does not cover patent infringements 

Are there some loopholes to use Pytorch with these layers or do they pay for using these  Or just ignore and use them regardless of the consequences ",1635247192.0,2021-10-26 13:19:52,tensorflow [apache license ] www apache org licenses license layers batch norm dropout covered license pytorch uses [the clause bsd license] opensource org licenses bsd clause cover patent infringements loopholes use pytorch layers pay using ignore use regardless consequences
[D] how to make best use of gpu cluster?,1,qh0xbu,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qh0xbu/d_how_to_make_best_use_of_gpu_cluster/,6," 

How do you guys choose to make the most use of a gpu cluster for inference for a DL pipeline that is made up of multiple models 

Do you just do distributed inference for each model or if some of the models can run in paralell send individual models to specific gpus ",1635353109.0,2021-10-27 18:45:09,guys choose make use gpu cluster inference dl pipeline made multiple models distributed inference model models run paralell send individual models specific gpus
[Research] Machine Learning Topics(data science),0,qh8ai4,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qh8ai4/research_machine_learning_topicsdata_science/,1,Any suggestions for Data science or Machine learning for my Masters thesis  I’m in Ghana  datasciencewithpython,1635373587.0,2021-10-28 00:26:27,suggestions data science machine learning masters thesis i’m ghana datasciencewithpython
[D] Semi-supervised learning with CycleGAN,2,qguvg2,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qguvg2/d_semisupervised_learning_with_cyclegan/,7,"Hey    
I have been training CycleGAN with medical images for unsupervised image to image style transfer  The results are good but not there yet  So  I thought of doing a semi supervised learning experiment  The images are largely unpaired  but I have got paired  registered  images as well  So  I designed an experiment in which I fed   paired     unpaired data for training  Then I tried different percentages of paired data but didn t see much difference in the end result  

The above observation led me to the conclusion that CycleGAN doesn t have a loss function that helps it directly benefit from the paired data  the cycle consistency loss helps but it s still indirect  There is no direct way to compare the generated images with the ground truth in the case of paired data 

Then I had this crazy idea  disclaimer  it may sound stupid   what if we introduce another L loss term for the direct comparison of the generated image with the  in a sense  ground truth  I know for the most part of the training it will confuse the model because of the high percentage of unpaired data  and therefore  it will have a lot smaller weight compared to cycle consistency loss and identity loss   But  can this loss be considered a small noise in the case of unpaired data  And for paired data  it will actually help the model take direct advantage of data alignment 

So  I wanted to ask my fellow machine learning practitioners  especially the seasoned ones  is there any merit to this idea or is it utterly asinine ",1635335399.0,2021-10-27 13:49:59,hey training cyclegan medical images unsupervised image image style transfer results good yet thought semi supervised learning experiment images largely unpaired got paired registered images well designed experiment fed paired unpaired data training tried different percentages paired data see much difference end result observation led conclusion cyclegan loss function helps directly benefit paired data cycle consistency loss helps still indirect direct way compare generated images ground truth case paired data crazy idea disclaimer may sound stupid introduce another l loss term direct comparison generated image sense ground truth know part training confuse model high percentage unpaired data therefore lot smaller weight compared cycle consistency loss identity loss loss considered small noise case unpaired data paired data actually help model take direct advantage data alignment wanted ask fellow machine learning practitioners especially seasoned ones merit idea utterly asinine
"[P] ImageNet size-accuracy pareto front, 2012-2021",22,qghjqi,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qghjqi/p_imagenet_sizeaccuracy_pareto_front_20122021/,10," xB 

https  preview redd it taqxjjavv png width= format=png auto=webp s=edacfdfbfdceacba

Paperswithcode has expanded their [imagenet results] https  paperswithcode com sota image classification on imagenet  with parameter counts  I was curious to see how size vs accuracy has evolved over time  Caveat  some   of the results lack parameter counts  so those are excluded here ",1635286103.0,2021-10-27 00:08:23,xb preview redd taqxjjavv png width= format=png auto=webp s=edacfdfbfdceacba paperswithcode expanded [imagenet results] paperswithcode com sota image classification imagenet parameter counts curious see size vs accuracy evolved time caveat results lack parameter counts excluded
"[D] Neural Network/Algorithm for Unsupervised, Multivariate, Sequential Classification - Human Movement Patterns",2,qgucxy,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qgucxy/d_neural_networkalgorithm_for_unsupervised/,2,I\`m trying to classify human movement states  standing  walking  etc   from joint  knee  hip  etc   angle data over time  To use unlabeled movement datasets  I am looking for an unsupervised method  Does anybody have an idea what I could use ,1635333537.0,2021-10-27 13:18:57,i\`m trying classify human movement states standing walking etc joint knee hip etc angle data time use unlabeled movement datasets looking unsupervised method anybody idea could use
[D] What qualifies as a full stack ML Engineer?,75,qg5aya,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qg5aya/d_what_qualifies_as_a_full_stack_ml_engineer/,49,"I see the term  full stack ml engineer  pop up more and more  I m curious what people feel is the minimum requirement to be able to call oneself  that with a straight face  And also if it s really a trend we re seeing or just a temporary buzz word 

To my mind  when I think of a full stack ML engineer you need to have a  out of  or better proficiency in these areas

* DevOps
* CloudOps
* MLOps 
* Data Science
* Data Engineering

Depending on the specific job company you can probably get away with a couple of s in there  Curious what other people are thinking  and if you think we are moving toward or away from full stack roles in ML ",1635251274.0,2021-10-26 14:27:54,see term full stack ml engineer pop curious people feel minimum requirement able call oneself straight face also really trend seeing temporary buzz word mind think full stack ml engineer need better proficiency areas * devops * cloudops * mlops * data science * data engineering depending specific job company probably get away couple curious people thinking think moving toward away full stack roles ml
[R] Spectral Bias in Practice: The Role of Function Frequency in Generalization,6,qgltks,MachineLearning,https://arxiv.org/abs/2110.02424,2,nan,1635299661.0,2021-10-27 03:54:21,nan
[D] How you deploy your ML model?,25,qg9zcc,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qg9zcc/d_how_you_deploy_your_ml_model/,52,"Hello ML community 

I wanted to get some quick feedback from the ML engineers in this community about their deployment hardware software specification  For example  when you deploy a  layer BERT or YoloVs or some other model 

  Do use GPU or CPU   and which model of GPU or CPU 
  Do you need batch size  or larger batch size 
  If batch size   then what is the target latency requirement that usually works well 
  What software stack do you use  Is it CUDA  TensorRT  OpenVINO or other 
  What you like and what you hate about the deployment process 
  And anything else related

In general  the ML deployment space is a bit fragmented and complicated  so it would be good to see what people use and what works well  Feel free to provide any input or feedback ",1635265095.0,2021-10-26 18:18:15,hello ml community wanted get quick feedback ml engineers community deployment hardware software specification example deploy layer bert yolovs model use gpu cpu model gpu cpu need batch size larger batch size batch size target latency requirement usually works well software stack use cuda tensorrt openvino like hate deployment process anything else related general ml deployment space bit fragmented complicated would good see people use works well feel free provide input feedback
"[P] Mlflow, fastapi, streamlit template",25,qg9m51,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qg9m51/p_mlflow_fastapi_streamlit_template/,4,"Combined torch  streamlit  fastapi  mlflow in a sample project  This was done to practice the tools and because I haven t found a similar pipeline  

[https  github com zademn mnist mlops learning] https  github com zademn mnist mlops learning ",1635264076.0,2021-10-26 18:01:16,combined torch streamlit fastapi mlflow sample project done practice tools found similar pipeline [ github com zademn mnist mlops learning] github com zademn mnist mlops learning
[D] How can I mark uncertain point according to marked point in image,0,qgrm1g,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qgrm1g/d_how_can_i_mark_uncertain_point_according_to/,1,There is a photo  i already have a part of annotation of image  the annotation is incomplete  many uncertain points in it  I want convert uncertain points to certain points according to existing knowledge  I find a method named CRF Conditional Random Field   but it isn t use existing knowledge  do you have something else methods ,1635322045.0,2021-10-27 10:07:25,photo already part annotation image annotation incomplete many uncertain points want convert uncertain points certain points according existing knowledge find method named crf conditional random field use existing knowledge something else methods
[Project] Aim v3.0.0 - revamped UI and revamped backend to query experiments faster and nicer!,21,qg930d,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qg930d/project_aim_v300_revamped_ui_and_revamped_backend/,4,"Hey r Machinelearning 

I am Gev  co author of Aim  

Excited to share with you the latest version of **Aim   **  

We have been hard at work in the past couple of months and made lots of changes  

The most important changes include 

**Completely revamped UI**

* Home page and run detail page
* Runs  metrics and params explorers
* Bookmarks and Tags

**Revamped Python SDK**

* New and much more intuitive  but still quite vanilla  API to track your training runs
* New and x faster embedded storage based on Rocksdb  It will allow us to store virtually any type of AI metadata  as opposed to AimRecords that was specifically designed for metrics and hyperparams 

We have also published our roadmap 

repo  [github com aimhubio aim] http  github com aimhubio aim  

blog  [bit ly vzhcA] https  t co WHKkKIRq amp= 

**Would love your feedback on our work ** 😊

https  i redd it fthodpctv gif",1635262589.0,2021-10-26 17:36:29,hey r machinelearning gev co author aim excited share latest version **aim ** hard work past couple months made lots changes important changes include **completely revamped ui** * home page run detail page * runs metrics params explorers * bookmarks tags **revamped python sdk** * new much intuitive still quite vanilla api track training runs * new x faster embedded storage based rocksdb allow us store virtually type ai metadata opposed aimrecords specifically designed metrics hyperparams also published roadmap repo [github com aimhubio aim] http github com aimhubio aim blog [bit ly vzhca] co whkkkirq amp= **would love feedback work ** 😊 redd fthodpctv gif
"[D] How do you handle ""What are you expecting for a salary?"" during job interviews?",224,qftzey,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qftzey/d_how_do_you_handle_what_are_you_expecting_for_a/,133,"I get tripped up on this  every  time  and the salary discussions on here are more about  how much do you make  and not  how do you negotiate ML Salaries     we seem to be in a weird place because the job landscape is so vast and responsibilities are not always the same between companies for the same position title 

I m an ML Engineer with ~ years of experience  prior experience as a software engineer supporting an ML team as well as experience as a data analyst for an ML Product  I ve driven business value both customer facing and backend  had experience working as a makeshift product manager  moved on prem ML architecture to the cloud  yadda yadda yadda  I have experience 

However  I never know what to say when folks ask what salary I m expecting  I kinda just ball park in the mid  s  but I think that I might be underselling myself sometimes   especially when talking with bay area companies  They tell me  we pay bay area salaries  and I honestly have no fucking clue what that means  

Direct question  If a company expects me to move out to the bay area  should I be asking for mid s or higher  or keep it at the  average  I see of ~ ",1635206693.0,2021-10-26 02:04:53,get tripped every time salary discussions much make negotiate ml salaries seem weird place job landscape vast responsibilities always companies position title ml engineer ~ years experience prior experience software engineer supporting ml team well experience data analyst ml product driven business value customer facing backend experience working makeshift product manager moved prem ml architecture cloud yadda yadda yadda experience however never know say folks ask salary expecting kinda ball park mid think might underselling sometimes especially talking bay area companies tell pay bay area salaries honestly fucking clue means direct question company expects move bay area asking mid higher keep average see ~
[D] Help with JODIE model data (Temporal Interaction Networks),1,qgo8ir,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qgo8ir/d_help_with_jodie_model_data_temporal_interaction/,1," Hey everyone 

I’m trying to create my own Reddit dataset to implement JODIE model used in the ‘Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks’ paper  Paper  [https  arxiv org abs  ] https  arxiv org abs    Code  [https  github com srijankr jodie] https  github com srijankr jodie  The paper is a bit vague on what features they use and how the data looks like 

I also downloaded the reddit csv data they had available but it looks strange  not sure how it was formatted  There are only  columns but there are a bunch of numbers in front of those columns as seen in the picture  Does anyone know any information of how their data is set up or how it needs to be formatted to use the model 

[reddit data] https  preview redd it xbfdzcfaxv png width= format=png auto=webp s=ebdbeaecfedbccf 

Please let me know if there is a better sub I could post this  Thanks for the help ",1635308019.0,2021-10-27 06:13:39,hey everyone i’m trying create reddit dataset implement jodie model used ‘predicting dynamic embedding trajectory temporal interaction networks’ paper paper [ arxiv org abs ] arxiv org abs code [ github com srijankr jodie] github com srijankr jodie paper bit vague features use data looks like also downloaded reddit csv data available looks strange sure formatted columns bunch numbers front columns seen picture anyone know information data set needs formatted use model [reddit data] preview redd xbfdzcfaxv png width= format=png auto=webp s=ebdbeaecfedbccf please let know better sub could post thanks help
[D] Is there a clear way for doing Font Style Transfer between words with neural networks?,5,qgbelu,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qgbelu/d_is_there_a_clear_way_for_doing_font_style/,3,"I am currently working on Font Style Transfer between words  Here is an example 

 xB 

[ Left  Source image  Middle  Style images  Right  Target  ] https  preview redd it wjpyowtv png width= format=png auto=webp s=aefcbbddeefebfe 

So I tried a couple of architectures for font style transfer  One of them is [FET GAN] https  github com liweileev FET GAN  which worked pretty fine for font transfer between letters  but had some problems 

* It couldn t generalize to new fonts from style images  Only trained fonts could be transferred 
* It had a poor performance for word images since the variation in the training data is higher than letter images 

That s why I want to ask if there are other architectures that solve the above mentioned problems ",1635269034.0,2021-10-26 19:23:54,currently working font style transfer words example xb [ left source image middle style images right target ] preview redd wjpyowtv png width= format=png auto=webp s=aefcbbddeefebfe tried couple architectures font style transfer one [fet gan] github com liweileev fet gan worked pretty fine font transfer letters problems * generalize new fonts style images trained fonts could transferred * poor performance word images since variation training data higher letter images want ask architectures solve mentioned problems
[R] Neural Tangent Kernel Eigenvalues Accurately Predict Generalization,47,qfy76l,MachineLearning,https://arxiv.org/abs/2110.03922,11,nan,1635221031.0,2021-10-26 06:03:51,nan
[N] Microsoft Releases ‘ORBIT’ Dataset: A Real-World Few-Shot Dataset for Teachable Object Recognition,3,qgchx9,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qgchx9/n_microsoft_releases_orbit_dataset_a_realworld/,0,"Object recognition algorithms have come a long way in recent years  but they still require training datasets containing thousands of high quality  annotated examples for every object category 

Few shot learning addresses this huge demand for datasets by training models to recognize entirely new things from only a few examples  Meta learning algorithms  in particular  which ‘learn to learn’ utilizing episodic training  can potentially reduce the number of training examples required to train a model  The majority of few shot learning research  on the other hand  has been driven by benchmark datasets that lack the substantial variability that applications confront when deployed in the real world 

To bridge this gap  Microsoft researchers  in collaboration with the City  University of London  present the ORBIT dataset and few shot benchmark that helps learn new objects from a small number of high variation samples  This new benchmark dataset includes a total of    frames  containing   videos of  objects captured on mobile phones by  people who are blind or have low vision 

  [Quick  Min Read] https  www marktechpost com    microsoft ai research releases orbit dataset a real world few shot dataset for teachable object recognition   [Paper] https  arxiv org abs    [Github] https  github com microsoft ORBIT Dataset    [Microsoft Blog] https  www microsoft com en us research blog announcing the orbit dataset advancing real world few shot learning using teachable object recognition 

https  preview redd it zeajdfwquv png width= format=png auto=webp s=afeddaececbbdbfffcaab",1635272018.0,2021-10-26 20:13:38,object recognition algorithms come long way recent years still require training datasets containing thousands high quality annotated examples every object category shot learning addresses huge demand datasets training models recognize entirely new things examples meta learning algorithms particular ‘learn learn’ utilizing episodic training potentially reduce number training examples required train model majority shot learning research hand driven benchmark datasets lack substantial variability applications confront deployed real world bridge gap microsoft researchers collaboration city university london present orbit dataset shot benchmark helps learn new objects small number high variation samples new benchmark dataset includes total frames containing videos objects captured mobile phones people blind low vision [quick min read] www marktechpost com microsoft ai research releases orbit dataset real world shot dataset teachable object recognition [paper] arxiv org abs [github] github com microsoft orbit dataset [microsoft blog] www microsoft com en us research blog announcing orbit dataset advancing real world shot learning using teachable object recognition preview redd zeajdfwquv png width= format=png auto=webp s=afeddaececbbdbfffcaab
[D] StyleGAN3 using a starting image?,0,qgixti,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qgixti/d_stylegan3_using_a_starting_image/,0,"Hello   
I ve been playing with StyleGAN setups in notebooks for a bit  but I m really hoping to find an implementation that starts with an image of my choosing  Do you have any suggestions for where to start ",1635290239.0,2021-10-27 01:17:19,hello playing stylegan setups notebooks bit really hoping find implementation starts image choosing suggestions start
[Discussion] Biotech and HealthTech companies using ML,0,qgmvay,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qgmvay/discussion_biotech_and_healthtech_companies_using/,1,"Are there any US Canadian Biotech and HealthTech companies doing interesting work using ML   
So far  in my limited research I came across these Startup to Mid size companies

* [https  www quantum si com ] https  www quantum si com 
* [https  www butterflynetwork com ] https  www butterflynetwork com 
* [https  www laddertx com ] https  www laddertx com ",1635303183.0,2021-10-27 04:53:03,us canadian biotech healthtech companies interesting work using ml far limited research came across startup mid size companies * [ www quantum si com ] www quantum si com * [ www butterflynetwork com ] www butterflynetwork com * [ www laddertx com ] www laddertx com
[D] Efficiently loading videos in PyTorch without extracting frames,9,qg27dt,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qg27dt/d_efficiently_loading_videos_in_pytorch_without/,17,"Hi 

I was wondering if someone knew an efficient way to load videos in PyTorch without extracting frames to files before  I m working with the VoxCeleb dataset with contains more than  million videos and I have calculated that saving frames to PNG would require about  To of disk space so I would prefer not to have to extract frames 

So far I m using VideoCapture from OpenCV but I can t use my GPU to   because the data loading takes too much time   I tried Nvidia DALI but it is even slower than OpenCV 

Thank you",1635238496.0,2021-10-26 10:54:56,hi wondering someone knew efficient way load videos pytorch without extracting frames files working voxceleb dataset contains million videos calculated saving frames png would require disk space would prefer extract frames far using videocapture opencv use gpu data loading takes much time tried nvidia dali even slower opencv thank
[R] A Comprehensive Comparison of Word Embeddings in Event & Entity Coreference Resolution,1,qg5f1m,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qg5f1m/r_a_comprehensive_comparison_of_word_embeddings/,0,"Hello reddit  this is my first paper which has been accepted at Findings of EMNLP  

Words are made letters that cannot be understood by AI as is  Thus  word embeddings are tools used to encode a vocabulary of words into a mathematical space which allows deep learning models to ingest textual data  To date  many word embeddings methods exist with various characteristics 

Hence  this paper studies how the various kind and various combinations of these embeddings perform  Additionally  I found that while there exist various kind of embeddings which have been trained differently  combining them does not greatly improve performance  This has a few consequence such as the fact that word embeddings are better compared when used alone instead of alongside others otherwise their difference in performance is overshadowed by the performance already provided by other embeddings in the system 

[https  arxiv org abs  ] https  arxiv org abs   ",1635251654.0,2021-10-26 14:34:14,hello reddit first paper accepted findings emnlp words made letters cannot understood ai thus word embeddings tools used encode vocabulary words mathematical space allows deep learning models ingest textual data date many word embeddings methods exist various characteristics hence paper studies various kind various combinations embeddings perform additionally found exist various kind embeddings trained differently combining greatly improve performance consequence fact word embeddings better compared used alone instead alongside others otherwise difference performance overshadowed performance already provided embeddings system [ arxiv org abs ] arxiv org abs
[R] Facebook AI Releases SaLinA: A Flexible and Simple Library for Learning Sequential Agents,57,qfi5sz,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfi5sz/r_facebook_ai_releases_salina_a_flexible_and/,5,"A Facebook AI research team releases SaLinA  a reinforcement learning  RL  library for model based RL  differentiable environments and multi agent RL that simplifies the implementation of complex sequential learning models  

Here is a quick read  [Facebook AI Releases SaLinA  A Flexible and Simple Library for Learning Sequential Agents ] https  syncedreview com    deepmind podracer tpu based rl frameworks deliver exceptional performance at low cost  

The paper *SaLinA  Sequential Learning of Agents* is on [arXiv] https  arxiv org abs    ",1635172729.0,2021-10-25 16:38:49,facebook ai research team releases salina reinforcement learning rl library model based rl differentiable environments multi agent rl simplifies implementation complex sequential learning models quick read [facebook ai releases salina flexible simple library learning sequential agents ] syncedreview com deepmind podracer tpu based rl frameworks deliver exceptional performance low cost paper *salina sequential learning agents* [arxiv] arxiv org abs
[D] UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction,40,qfk1yt,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfk1yt/d_umap_uniform_manifold_approximation_and/,19,"I am just working on a clustering project and curious what people will think about this algorithm  [https  arxiv org abs  ] https  arxiv org abs   

I read guides and many says it is better than tSNE 

I would love to hear about your experience with any other clustering algorithms  other then the normal scikit learn ones  ",1635178013.0,2021-10-25 18:06:53,working clustering project curious people think algorithm [ arxiv org abs ] arxiv org abs read guides many says better tsne would love hear experience clustering algorithms normal scikit learn ones
[P] Pywick - High-Level Training framework for Pytorch,23,qfjjm6,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfjjm6/p_pywick_highlevel_training_framework_for_pytorch/,0,"Hi AI practitioners 

We wanted to bring to your attention another huge release of the [Pywick] https  github com achaiah pywick  training framework  Some notable features that you may find useful are 
    classification network variants
  Dockerized runtime for easy execution in the cloud  Demo included with the docker image 
  Full configuration via `yaml` files with no coding required to get started
  Bleeding edge optimizers  loss functions  activation functions etc
  Thorough documentation

Hope you find it useful ",1635176603.0,2021-10-25 17:43:23,hi ai practitioners wanted bring attention another huge release [pywick] github com achaiah pywick training framework notable features may find useful classification network variants dockerized runtime easy execution cloud demo included docker image full configuration via `yaml` files coding required get started bleeding edge optimizers loss functions activation functions etc thorough documentation hope find useful
[D] New in-depth AI interview episode out! Yuval was featured on 2 minute papers for his incredible work on AI toonification.,253,qf7ar3,MachineLearning,https://v.redd.it/u5ec6kazeiv71,3,nan,1635129850.0,2021-10-25 04:44:10,nan
Normalizing data for anomaly detection [D],2,qfy49c,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfy49c/normalizing_data_for_anomaly_detection_d/,2,Can performing a min max normalization on a dataset that contains anomalies affect the performance of a model since the values of the anomalous data points will have some kind of affect on the values of the normal data points due to the nature of min max normalization  And if so  is there anything that can be done to get around this ,1635220742.0,2021-10-26 05:59:02,performing min max normalization dataset contains anomalies affect performance model since values anomalous data points kind affect values normal data points due nature min max normalization anything done get around
[R] Microsoft AI Open-Sources ‘PyTorch-DirectML’: A Package To Train Machine Learning Models On GPUs,97,qfaxcv,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfaxcv/r_microsoft_ai_opensources_pytorchdirectml_a/,19,"The Microsoft Windows AI team has announced the [f] https  devblogs microsoft com windowsai introducing pytorch directml train your machine learning models on any gpu [irst preview of DirectML as a backend to PyTorch for training ML models] https  devblogs microsoft com windowsai introducing pytorch directml train your machine learning models on any gpu   This release allows accelerated machine learning training for PyTorch on any DirectX GPU and WSL  unlocking new potential in computing with mixed reality 

Microsoft AI team has teamed up with the PyTorch framework to release a preview package that provides scoped support for CNNs  convolutional neural networks   In this new device named “DML ” Direct ML APIs and Tensor primitives are called through by introducing minimal overhead when calling into operators  they work in much like other existing backends 

  [ Min Quick Read] https  www marktechpost com    microsoft ai open sources pytorch directml a package to train machine learning models on gpus  [Github] https  github com microsoft DirectML    [Microsoft Blog] https  devblogs microsoft com windowsai introducing pytorch directml train your machine learning models on any gpu 

https  preview redd it xiwwpljxmjv jpg width= format=pjpg auto=webp s=dfbcfabcecffbda",1635144637.0,2021-10-25 08:50:37,microsoft windows ai team announced [f] devblogs microsoft com windowsai introducing pytorch directml train machine learning models gpu [irst preview directml backend pytorch training ml models] devblogs microsoft com windowsai introducing pytorch directml train machine learning models gpu release allows accelerated machine learning training pytorch directx gpu wsl unlocking new potential computing mixed reality microsoft ai team teamed pytorch framework release preview package provides scoped support cnns convolutional neural networks new device named “dml ” direct ml apis tensor primitives called introducing minimal overhead calling operators work much like existing backends [ min quick read] www marktechpost com microsoft ai open sources pytorch directml package train machine learning models gpus [github] github com microsoft directml [microsoft blog] devblogs microsoft com windowsai introducing pytorch directml train machine learning models gpu preview redd xiwwpljxmjv jpg width= format=pjpg auto=webp s=dfbcfabcecffbda
[D] - Algorithmically choosing best training data for semantic segmentation,3,qfv2za,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfv2za/d_algorithmically_choosing_best_training_data_for/,0,"I m working on a semantic segmentation problem with access to a very large dataset of image label pairs for training  While quantity of the training data has never been an issue  the quality is  What I mean is a large percentage of our data does not have very accurate GT segmentation labels  My intuition is that its better to have smaller  but high quality image label pairs  rather than having a very large  but inaccurate image label pairs 

 xB 

  First of all  is my intuition correct and are there papers to support this
  Secondly  is there a good  algorithmic way to choose high quality training data from this large pool of image label pairs   I m thinking of computer vision algorithms or some sort of preprocessing to filter out potentially bad training examples  ",1635210357.0,2021-10-26 03:05:57,working semantic segmentation problem access large dataset image label pairs training quantity training data never issue quality mean large percentage data accurate gt segmentation labels intuition better smaller high quality image label pairs rather large inaccurate image label pairs xb first intuition correct papers support secondly good algorithmic way choose high quality training data large pool image label pairs thinking computer vision algorithms sort preprocessing filter potentially bad training examples
[D] Usefulness of self-supervised pretraining,11,qfm81h,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfm81h/d_usefulness_of_selfsupervised_pretraining/,7,"I ve skimmed a few videos papers on self supervised learning  and it seems like a useful way to pretrain a model if you have limited labeled training data  but I don t often hear a lot people using it in that way 

A few questions 

  Is it usually any more effective than pretraining an autoencoder denoising autoencoder using all of data and then fine tuning using labeled data  I m not entirely sure why it would be  I guess I might be missing the point of self supervised learning 
  Newer methods seem to require a ton of compute  I think a  small batch size  was  in one of the papers I skimmed   Can older methods Ex  Jigsaw still be useful  The task I m thinking about has a fair amount of labeled data  About equal to unlabeled   but I m wondering if pretraining would improve performance marginally ",1635184050.0,2021-10-25 19:47:30,skimmed videos papers self supervised learning seems like useful way pretrain model limited labeled training data often hear lot people using way questions usually effective pretraining autoencoder denoising autoencoder using data fine tuning using labeled data entirely sure would guess might missing point self supervised learning newer methods seem require ton compute think small batch size one papers skimmed older methods ex jigsaw still useful task thinking fair amount labeled data equal unlabeled wondering pretraining would improve performance marginally
[D] [R] Transitioning to research roles,9,qfgbtk,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfgbtk/d_r_transitioning_to_research_roles/,13,"Looking for advice on how to transition to a more research related career and how to get more research experience 

I graduated last year with a degree in Computer science and Mathematics and have been working as a data scientist analyst since then  Right now I m interested in pursuing a master s in computer science researching ML and am wondering what the best route to take is  

I have a few months of research experience working with a professor during my last year in university but that s about it  I was considering cold emailing some profs at other universities to see if I could work with them but not sure how weird that is as I m graduated and have been working full time for a year now 

I know this is a fairly saturated field and can be quite competitive when applying to top universities so I don t have too high hopes that I ll be accepted to a master s program at schools I want with profs I am interested in  I could apply to the school I graduated from but it is not as great for ML in terms of profs and I am not sure if it is worth my time 

As well  if I would not be able to get accepted to Master s  then am looking at how to transition from data scientist to more research roles and wondering how feasible that would be 
Any advice is great thanks ",1635167192.0,2021-10-25 15:06:32,looking advice transition research related career get research experience graduated last year degree computer science mathematics working data scientist analyst since right interested pursuing master computer science researching ml wondering best route take months research experience working professor last year university considering cold emailing profs universities see could work sure weird graduated working full time year know fairly saturated field quite competitive applying top universities high hopes accepted master program schools want profs interested could apply school graduated great ml terms profs sure worth time well would able get accepted master looking transition data scientist research roles wondering feasible would advice great thanks
Is semantic segmentation solved? [D],0,qfurkx,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfurkx/is_semantic_segmentation_solved_d/,5,"So  I have been going through a few models used for semantic segmentation  

U net  Seg net  PSP net  Deep Lab seems to be the popular ones  Do these models solve the problem of semantic segmentation 

Also  is it possible to bring in any changes to the existing model and make it better  I wanted to know are there any methods which can improve the existing models ",1635209279.0,2021-10-26 02:47:59,going models used semantic segmentation u net seg net psp net deep lab seems popular ones models solve problem semantic segmentation also possible bring changes existing model make better wanted know methods improve existing models
[N] OpenAI Gym maintainer plans to deprecate and replace MuJoCo and Box2D environments with Brax-based environments.,51,qf788d,MachineLearning,https://github.com/openai/gym/issues/2456,13,nan,1635129594.0,2021-10-25 04:39:54,nan
[D] MLP's are actually nonlinear ➞ linear preconditioners (with visuals!),224,qex0o7,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qex0o7/d_mlps_are_actually_nonlinear_linear/,57,"In spirit of yesterday being a [bones day] https  www tiktok com jongraz video    I put together a few visuals last night to show off something people might not always think about  Enjoy 

Let s pretend our goal was to approximate this function with data 

[\`cos\ norm\ x\ \ \` over \`\[ π  π\]\`] https  i redd it nwprueofv gif 

To demonstrate how a neural network  makes a nonlinear function linear   here I trained a  ×  [multilayer perceptron] https  github com tchlux tchlux github io blob dfecbbefcabbcbeee documents piecewise_linear_regression_model f  with [PReLU] https  pytorch org docs stable generated torch nn PReLU html  activation on the function `cos norm x ` with a random uniform k points over the `[ π  π]` square  The training was done with k steps of full batch Adam  roughly  [my own version of Adam] https  github com tchlux tchlux github io blob master documents piecewise_linear_regression_model f L L   Here s the final approximation 

[\  × \  PReLU MLP approximation to \`cos\ norm\ x\ \ \` with k points] https  i redd it jiykwiofv gif 

Not perfect  but pretty good  Now here s where things get interesting  What happens if you look at the  last embedding  of the network  what does the function look like in that space  Here s a visual where I ve taken the representations of the data at that last layer and projected them onto the first two [principal components] https  setosa io ev principal component analysis  with the true function value as the z axis 

[Last layer embedding of the k training points for the MLP approximating \`cos\ norm\ x\ \ \`] https  i redd it ztkofv gif 

Almost perfectly linear  To people that think about what a neural network does a lot  this might be obvious  But I feel like there s a new perspective here that people can benefit from 

  When we train a neural network  we are constructing a function that nonlinearly transforms data into a space where the curvature of the  target  is minimized 

In numerical analysis  transformations that you make to data to improve the accuracy of later approximations are called  preconditioners   Now preconditioning data for linear approximations has many benefits other than just minimizing the loss of your neural network  [Proven error bounds] https  tchlux github io documents tchlux  thesis slides theorem pdf  for piecewise linear approximations  many neural networks  are affected heavily by the curvature of the function being approximated  full proof is in [Section  of this paper] https  tchlux github io papers tchlux  NUMA pdf  for those interested  

 xB 

 *What does this mean though *

It means that after we train a neural network for *any* problem  computer vision  natural language  generic data science     we don t have to use the last layer of the neural network  *ahem*  linear regression  to make predictions  We can use k nearest neighbor  or a [Shepard interpolant] https  en wikipedia org wiki Inverse_distance_weighting   and the accuracy of those methods will usually be improved significantly  Check out what happens for this example when we use k nearest neighbor to make an approximation 

[Nearest neighbor approximation to \`x cos\ x\ \  sin\ y\ \` over unit cube ] https  i redd it bzssunofv gif 

Now  train a small neural network  × in size  on the \~ data points seen in the visual  transform the entire space to the last layer embedding of that network   dimensions   and visualize the resulting approximation back in our original input space  This is what the new nearest neighbor approximation looks like 

[Nearest neighbor over the same data as before  but after transforming the space with a small trained neural network ] https  i redd it xgrageoofv gif 

[Pretty neat ] https  youtu be HmJodBR vs  The maximum error of this nearest neighbor approximation decreased significantly when we used a neural network as a preconditioner  And we can use this concept *anywhere*  Want to make distributional predictions and give statistical bounds for any data science problem  Well that s really easy to do with lots of nearest neighbors  And we have all the tools to do it 

 xB 

 ***About me *** *I spend a lot of time thinking about how we can progress towards useful digital intelligence  AI   I do not research this full time  maybe one day    but rather do this as a hobby  My current line of work is on building theory for solving arbitrary approximation problems  specifically investigating a generalization of transformers  with nonlinear attention mechanisms  and how to improve the convergence   error reduction properties   guarantees of neural networks in general *  
   
 *Since this is a hobby  I don t spend lots of time looking for other people doing the same work  I just do this as fun project  Please share any research that is related or that you think would be useful or interesting *",1635097260.0,2021-10-24 19:41:00,spirit yesterday [bones day] www tiktok com jongraz video put together visuals last night show something people might always think enjoy let pretend goal approximate function data [\`cos\ norm\ x\ \ \` \`\[ π π\]\`] redd nwprueofv gif demonstrate neural network makes nonlinear function linear trained × [multilayer perceptron] github com tchlux tchlux github io blob dfecbbefcabbcbeee documents piecewise_linear_regression_model f [prelu] pytorch org docs stable generated torch nn prelu html activation function `cos norm x ` random uniform k points `[ π π]` square training done k steps full batch adam roughly [my version adam] github com tchlux tchlux github io blob master documents piecewise_linear_regression_model f l l final approximation [\ × \ prelu mlp approximation \`cos\ norm\ x\ \ \` k points] redd jiykwiofv gif perfect pretty good things get interesting happens look last embedding network function look like space visual taken representations data last layer projected onto first two [principal components] setosa io ev principal component analysis true function value z axis [last layer embedding k training points mlp approximating \`cos\ norm\ x\ \ \`] redd ztkofv gif almost perfectly linear people think neural network lot might obvious feel like new perspective people benefit train neural network constructing function nonlinearly transforms data space curvature target minimized numerical analysis transformations make data improve accuracy later approximations called preconditioners preconditioning data linear approximations many benefits minimizing loss neural network [proven error bounds] tchlux github io documents tchlux thesis slides theorem pdf piecewise linear approximations many neural networks affected heavily curvature function approximated full proof [section paper] tchlux github io papers tchlux numa pdf interested xb *what mean though * means train neural network *any* problem computer vision natural language generic data science use last layer neural network *ahem* linear regression make predictions use k nearest neighbor [shepard interpolant] en wikipedia org wiki inverse_distance_weighting accuracy methods usually improved significantly check happens example use k nearest neighbor make approximation [nearest neighbor approximation \`x cos\ x\ \ sin\ y\ \` unit cube ] redd bzssunofv gif train small neural network × size \~ data points seen visual transform entire space last layer embedding network dimensions visualize resulting approximation back original input space new nearest neighbor approximation looks like [nearest neighbor data transforming space small trained neural network ] redd xgrageoofv gif [pretty neat ] youtu hmjodbr vs maximum error nearest neighbor approximation decreased significantly used neural network preconditioner use concept *anywhere* want make distributional predictions give statistical bounds data science problem well really easy lots nearest neighbors tools xb ***about *** *i spend lot time thinking progress towards useful digital intelligence ai research full time maybe one day rather hobby current line work building theory solving arbitrary approximation problems specifically investigating generalization transformers nonlinear attention mechanisms improve convergence error reduction properties guarantees neural networks general * *since hobby spend lots time looking people work fun project please share research related think would useful interesting *
[P] These Days Style GAN be like (Code and Paper links in the comments),882,qeo7fx,MachineLearning,https://i.redd.it/5qmiz1tax5v71.jpg,65,nan,1635064767.0,2021-10-24 10:39:27,nan
[D] Creating Reduced Audio Representations,0,qfqnox,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfqnox/d_creating_reduced_audio_representations/,8,"Hi all 

I am currently trying to find methods to represent the structure of an audio clip in a few data points  The clips are short  \~ms  What would be some metrics which model the audio s structure  

Thanks ",1635196671.0,2021-10-25 23:17:51,hi currently trying find methods represent structure audio clip data points clips short \~ms would metrics model audio structure thanks
[R] Just Ask for Generalization,26,qf6hsp,MachineLearning,https://evjang.com/2021/10/23/generalization.html,1,nan,1635126991.0,2021-10-25 03:56:31,nan
[R] Efficient Visual Self-Attention - Link to a free online lecture by the author in comments,70,qeyhwb,MachineLearning,https://i.redd.it/udmgzldy2gv71.png,11,nan,1635101582.0,2021-10-24 20:53:02,nan
[R] ByteTrack: Multi-Object Tracking by Associating Every Detection Box,1183,qeihw2,MachineLearning,https://v.redd.it/sf125fyg0bv71,64,nan,1635040318.0,2021-10-24 03:51:58,nan
[D] Trouble Modelling High Dimensional Regression Problem with Autoencoder,13,qf4qxs,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qf4qxs/d_trouble_modelling_high_dimensional_regression/,22,"I ve been given the task to fit an Autoencoder to a large dataset of  dimensional vectors  After constructing some relatively simple AEs  eg  nn Linear     nn Linear     nn Linear    reverse    I ve found that it s incredibly hard to converge onto a non trivial solution  These vectors have been normalized in such a way that their means lie \~   and are bounded between    I have attached an image illustrating a few of the vectors within my dataset 

I m finding that since these vectors essentially look like noise centered around some global mean value  any model that I attempt to train collapses onto some noisy curve centered around the mean  Importantly  these vectors all represent something important  so smoothing or other preprocessing tricks are out of the question  Sadly  there doesn t seem to be much structure within the data  so a CNN model is also out 

This is basically a high dimensional regression problem with diverse data  Does anyone have any tips for training an AE model with such data  Is it even possible  Any tips or recommendations would be great 

EDIT 
I realize I was vague  These vectors are the Fourier transform of the original signal  where the first  values are the real portion and the second  values are the corresponding imaginary  phase  information  

[ Dimensional Vectors with Mean \~ ] https  preview redd it wphlnoohv png width= format=png auto=webp s=fdefabfdbbeca ",1635120965.0,2021-10-25 02:16:05,given task fit autoencoder large dataset dimensional vectors constructing relatively simple aes eg nn linear nn linear nn linear reverse found incredibly hard converge onto non trivial solution vectors normalized way means lie \~ bounded attached image illustrating vectors within dataset finding since vectors essentially look like noise centered around global mean value model attempt train collapses onto noisy curve centered around mean importantly vectors represent something important smoothing preprocessing tricks question sadly seem much structure within data cnn model also basically high dimensional regression problem diverse data anyone tips training ae model data even possible tips recommendations would great edit realize vague vectors fourier transform original signal first values real portion second values corresponding imaginary phase information [ dimensional vectors mean \~ ] preview redd wphlnoohv png width= format=png auto=webp s=fdefabfdbbeca
[D] Paper Explained - Symbolic Knowledge Distillation: from General Language Models to Commonsense Models (Video Walkthrough),21,qf1drv,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qf1drv/d_paper_explained_symbolic_knowledge_distillation/,2,"[https  youtu be kP dXKJEhY] https  youtu be kP dXKJEhY 

Symbolic knowledge models are usually trained on human generated corpora that are cumbersome and expensive to create  Such corpora consist of structured triples of symbolic knowledge  This paper takes a different approach and attempts to generate such a corpus by prompting GPT   Results show that clever prompting  combined with targeted small critic models trained on human ratings can outperform both human generated data  as well as the teacher model  GPT   itself  The results of this paper give a general recipe for automatically building corpora for various NLP tasks by extracting samples from large language models 

 xB 

OUTLINE 

    Intro   Overview

    Sponsor  Weights   Biases

    Commonsense Knowledge Graphs

    ATOMIC dataset

    Generating the corpus from a model

    Prompting GPT 

    Generating Events

    Generating Inferences

    Evaluating the created dataset

    Introducing the critic

    Using the critic to filter the data

    Training a student on the generated data

    Key Findings

    Comments   Conclusion

 xB 

Paper  [https  arxiv org abs  ] https  arxiv org abs   

Code   Corpus  [https  github com peterwestai symbolic knowledge distillation] https  github com peterwestai symbolic knowledge distillation ",1635110049.0,2021-10-24 23:14:09,[ youtu kp dxkjehy] youtu kp dxkjehy symbolic knowledge models usually trained human generated corpora cumbersome expensive create corpora consist structured triples symbolic knowledge paper takes different approach attempts generate corpus prompting gpt results show clever prompting combined targeted small critic models trained human ratings outperform human generated data well teacher model gpt results paper give general recipe automatically building corpora various nlp tasks extracting samples large language models xb outline intro overview sponsor weights biases commonsense knowledge graphs atomic dataset generating corpus model prompting gpt generating events generating inferences evaluating created dataset introducing critic using critic filter data training student generated data key findings comments conclusion xb paper [ arxiv org abs ] arxiv org abs code corpus [ github com peterwestai symbolic knowledge distillation] github com peterwestai symbolic knowledge distillation
[Discussion] Examples of classical / learning-based computer vision methods performing better when using under / overexposed images.,2,qfbi5g,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfbi5g/discussion_examples_of_classical_learningbased/,1,I am looking for examples where classical   learning based computer vision methods performing better when using under overexposed images  Any relevant paper recommendation would be great ,1635147346.0,2021-10-25 09:35:46,looking examples classical learning based computer vision methods performing better using overexposed images relevant paper recommendation would great
[P] UI to link entities to a knowledge base ontology,1,qfdwr6,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfdwr6/p_ui_to_link_entities_to_a_knowledge_base_ontology/,2,"Hi all 

I have just released a new major feature which I have been working on for a while now  It is now possible to do named entity disambiguation  or named entity linking  from DataQA  There is no other UI based solution to solve this type of problem  The way it works is 

* upload text with entity annotations
* upload your ontology
* the UI will give you suggestions as you label  so you don t need to search through your s of knowledge bases if you have a large ontology 

https  i redd it proqvrkv gif

It s all open source and available here  [https  github com dataqa dataqa] https  github com dataqa dataqa  

A tutorial on how to use it  [https  dataqa ai docs tutorials medical\_entity\_disambiguation ned\_side\_effects ] https  dataqa ai docs tutorials medical_entity_disambiguation ned_side_effects  

I would appreciate any feedback   ",1635158475.0,2021-10-25 12:41:15,hi released new major feature working possible named entity disambiguation named entity linking dataqa ui based solution solve type problem way works * upload text entity annotations * upload ontology * ui give suggestions label need search knowledge bases large ontology redd proqvrkv gif open source available [ github com dataqa dataqa] github com dataqa dataqa tutorial use [ dataqa ai docs tutorials medical\_entity\_disambiguation ned\_side\_effects ] dataqa ai docs tutorials medical_entity_disambiguation ned_side_effects would appreciate feedback
[D] Multitask Prompted Training Enables Zero-shot Task Generalization (Explained),4,qf3ln0,MachineLearning,https://youtu.be/YToXXfrIu6w,0,nan,1635117021.0,2021-10-25 01:10:21,nan
[D] Simple Questions Thread,17,qetu2q,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qetu2q/d_simple_questions_thread/,112,"Please post your questions here instead of creating a new thread  Encourage others who create new posts for questions to post here instead 

Thread will stay alive until next one so keep posting after the date in the title 

Thanks to everyone for answering questions in the previous thread ",1635087615.0,2021-10-24 17:00:15,please post questions instead creating new thread encourage others create new posts questions post instead thread stay alive next one keep posting date title thanks everyone answering questions previous thread
[D] What is the approx monthly cost of ML infrastructure training + inference at your company,0,qfdln1,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfdln1/d_what_is_the_approx_monthly_cost_of_ml/,4,"

[View Poll] https  www reddit com poll qfdln ",1635157156.0,2021-10-25 12:19:16,[view poll] www reddit com poll qfdln
[Discussion] How valuable are workshop publications compared to journal publications?,2,qf3xvu,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qf3xvu/discussion_how_valuable_are_workshop_publications/,13,Hi there  I am an Undergraduate student that has been conducting research in the Machine Learning field for about two years now  This summer  I received two top authorships at both NeurIPS and ICML  My work has been heavily skewed towards applications of Machine Learning in the field of ecology  so I was able to get my papers accepted into workshops dedicated to the intersection of climate change and machine learning  I am curious how strong of a resume item these workshop publications will be for my future  Now that I have these two publications under my belt  should I focus on a Journal publication ,1635118186.0,2021-10-25 01:29:46,hi undergraduate student conducting research machine learning field two years summer received two top authorships neurips icml work heavily skewed towards applications machine learning field ecology able get papers accepted workshops dedicated intersection climate change machine learning curious strong resume item workshop publications future two publications belt focus journal publication
[D] CIPS Follow-Up Paper explained - Harnessing the Conditioning Sensorium for Improved Image Translation (5-minute summary by Casual GAN Papers - The Author of OG CIPS),0,qf66pv,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qf66pv/d_cips_followup_paper_explained_harnessing_the/,1,"Hey everyone 

I was one of the authors of the original CIPS paper and I thought it would be fun to do a breakdown of this follow up paper that takes CIPS into the D world 

If you have been following generative ML for a while you might have noticed more and more GAN papers focusing on the underlying D  representation of the generated images  CIPS D is a D aware GAN model proposed by Peng Zhou and the team at Shanghai Jiao Tong University    Huawei that combines a low res NeRF  surprise  with a CIPS  generator  genuine surprise  to achieve high quality x D aware  image synthesis as well as transfer learning and D aware face stylization 

Fresh out of the oven  Full summary  [https  www casualganpapers com d aware gan based on cips and nerf CIPS D explained html] https  www casualganpapers com d aware gan based on cips and nerf CIPS D explained html 

[CIPS D] https  reddit com link qfpv video kpuiv player 

arxiv  [https  arxiv org pdf   pdf] https  arxiv org pdf   pdf   
code  [https  github com PeterouZh CIPS D] https  github com PeterouZh CIPS D 

Subscribe to [Casual GAN Papers] https  t me casual_gan  and follow me on [Twitter] https  twitter com KirillDemochkin  for weekly AI paper summaries ",1635125891.0,2021-10-25 03:38:11,hey everyone one authors original cips paper thought would fun breakdown follow paper takes cips world following generative ml might noticed gan papers focusing underlying representation generated images cips aware gan model proposed peng zhou team shanghai jiao tong university huawei combines low res nerf surprise cips generator genuine surprise achieve high quality x aware image synthesis well transfer learning aware face stylization fresh oven full summary [ www casualganpapers com aware gan based cips nerf cips explained html] www casualganpapers com aware gan based cips nerf cips explained html [cips d] reddit com link qfpv video kpuiv player arxiv [ arxiv org pdf pdf] arxiv org pdf pdf code [ github com peterouzh cips d] github com peterouzh cips subscribe [casual gan papers] casual_gan follow [twitter] twitter com kirilldemochkin weekly ai paper summaries
[P] The implementation of Conv2D Layer in PyTorch,0,qf4p52,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qf4p52/p_the_implementation_of_conv2d_layer_in_pytorch/,3,"I traced down the implementation of ConvD Layer down to the choice of the math library  e g cuDNN  MKLDNN  MIOpen  XNNPack  according to the device   


Thanks to Emacs  CScope  Ripgrep  

https  preview redd it gglfmrnhv png width= format=png auto=webp s=fbaaaabbabefaacc",1635120783.0,2021-10-25 02:13:03,traced implementation convd layer choice math library e g cudnn mkldnn miopen xnnpack according device thanks emacs cscope ripgrep preview redd gglfmrnhv png width= format=png auto=webp s=fbaaaabbabefaacc
[D][R] Looking for benchmarking papers for edge computing,4,qetewa,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qetewa/dr_looking_for_benchmarking_papers_for_edge/,6,"Hey everyone  I m curious if there are any good references papers resources which analyze  critique and benchmark decision making systems that are running at the  edge   think a Raspberry Pi or equivalent   I m interested on offline  edge first computing applications using AI to make decisions in near real time without constantly calling to an outside server for the answer  So any application or paper that s a little more complicated than nested if statements and focuses on doing everything locally 

NOTE  I know of the existence of NVIDIA Jetson and other mobile edge inference platforms  I see the cool real time object detection demos on here everyday  but I m expressly not looking for  mobile lightweight NN inference benchmarking   I m looking for something more in the domain of discussion similar to Tesla s Autopilot decision making  but maybe a tad more lightweight  Thanks 

Additional note  As I clarify below  the TL DR is I m looking for ML papers researchers  NN based or traditional  that are focused on doing ML  including training   live  in the field  not just inference using a downloaded and pre trained model ",1635086231.0,2021-10-24 16:37:11,hey everyone curious good references papers resources analyze critique benchmark decision making systems running edge think raspberry pi equivalent interested offline edge first computing applications using ai make decisions near real time without constantly calling outside server answer application paper little complicated nested statements focuses everything locally note know existence nvidia jetson mobile edge inference platforms see cool real time object detection demos everyday expressly looking mobile lightweight nn inference benchmarking looking something domain discussion similar tesla autopilot decision making maybe tad lightweight thanks additional note clarify tl dr looking ml papers researchers nn based traditional focused ml including training live field inference using downloaded pre trained model
[D] Multi-prompt Learning in NLP (Paper Summary),4,qerxy1,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qerxy1/d_multiprompt_learning_in_nlp_paper_summary/,0,"Prompt learning is the latest paradigm to adapt pre trained language models  PLMs  to downstream NLP tasks  🔥

Is one prompt enough❓ or Can we have an ensemble of prompts to achieve generalization❓🤔

Paper summary  https  youtu be iUNDgetRU

⏩ Paper Title  Pre train  Prompt  and Predict  A Systematic Survey of Prompting Methods in Natural Language Processing
⏩ Paper  https  arxiv org abs  v
⏩ Author  Pengfei Liu  Weizhe Yuan  Jinlan Fu  Zhengbao Jiang  Hiroaki Hayashi  Graham Neubig
⏩ Organisation  Carnegie Mellon University  National University of Singapore",1635081279.0,2021-10-24 15:14:39,prompt learning latest paradigm adapt pre trained language models plms downstream nlp tasks 🔥 one prompt enough❓ ensemble prompts achieve generalization❓🤔 paper summary youtu iundgetru ⏩ paper title pre train prompt predict systematic survey prompting methods natural language processing ⏩ paper arxiv org abs v ⏩ author pengfei liu weizhe yuan jinlan fu zhengbao jiang hiroaki hayashi graham neubig ⏩ organisation carnegie mellon university national university singapore
"[News][Research] Isolate Voice, Music and Sound Effects With AI | Mitsubishi Research Lab (MERL)",97,qe5duu,MachineLearning,https://youtu.be/Rpxufqt5r6I,6,nan,1634997627.0,2021-10-23 16:00:27,nan
[N] DeepMind's founder reveals future goals in computational biology regarding AlphaFold during a conference,314,qdyzue,MachineLearning,https://i.redd.it/4jbf2sxl35v71.png,12,nan,1634968628.0,2021-10-23 07:57:08,nan
[N] Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning,16,qee4po,MachineLearning,https://arxiv.org/abs/2110.04725,2,nan,1635025064.0,2021-10-23 23:37:44,nan
[D] How does the DeepSparse Engine work?,6,qecovo,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qecovo/d_how_does_the_deepsparse_engine_work/,6,"Neural magic has open source tools for pruning models and the [DeepSparse] https  github com neuralmagic deepsparse  efficiently executes the sparse models on CPU  Below is the only description of how it works I could find 
 [source] https  neuralmagic com blog how neural magics deep sparse technology works 
 Mimicking the Brain   Locality of Reference
In addition to sparsifying the network  our DeepSparse Engine’s breakthrough sparse kernels execute this sparse computation more effectively  The deeply sparsified computation is memory bound  which is unfortunately not good for a CPU  Our solution to this memory boundedness is to execute the neural network depth wise rather than layer after layer  It might seem like magic  but we are able to break the network into Tensor Columns  vertical stripes of computation that fit completely in cache without having to read or write to memory  Tensor Columns mimic the locality of the brain using the locality of reference of the CPUs cache hierarchy  the outputs of the column’s small section of a layer of neurons waits in cache for the next layer as the sparse execution unfolds depth wise 

Can you make sense of this  How can you execute a model depth wise rather than layer after layer ",1635020456.0,2021-10-23 22:20:56,neural magic open source tools pruning models [deepsparse] github com neuralmagic deepsparse efficiently executes sparse models cpu description works could find [source] neuralmagic com blog neural magics deep sparse technology works mimicking brain locality reference addition sparsifying network deepsparse engine’s breakthrough sparse kernels execute sparse computation effectively deeply sparsified computation memory bound unfortunately good cpu solution memory boundedness execute neural network depth wise rather layer layer might seem like magic able break network tensor columns vertical stripes computation fit completely cache without read write memory tensor columns mimic locality brain using locality reference cpus cache hierarchy outputs column’s small section layer neurons waits cache next layer sparse execution unfolds depth wise make sense execute model depth wise rather layer layer
[R] Artificial early language acquisition from videos,8,qe9bvr,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qe9bvr/r_artificial_early_language_acquisition_from/,0,"Hi 

During the last months  I built a ML model to artificially reproduce [**language acquisition from videos**] https  hyugen ai medium com artificial early language acquisition from videos cebacbe ** **  The idea was to reproduce the link between sounds and images  without using supervised learning in the usual way  I wrote an article to detail the task I created and the unsupervised learning methods I used to train the models  

It also includes some results  analysis and a demo 

* **Article**  [https  hyugen ai medium com artificial early language acquisition from videos cebacbe] https  hyugen ai medium com artificial early language acquisition from videos cebacbe 

I know that the model is incomplete for now but I hope I ll be able to enhance it  If you have some comments on my work I would like to read them and answer questions or insights you could have 

Thanks for your feedback 

^ ps  I also participate on the sub with another account  and I guess it fits with \[R\] \[D\] \[P\] tags ",1635010150.0,2021-10-23 19:29:10,hi last months built ml model artificially reproduce [**language acquisition videos**] hyugen ai medium com artificial early language acquisition videos cebacbe ** ** idea reproduce link sounds images without using supervised learning usual way wrote article detail task created unsupervised learning methods used train models also includes results analysis demo * **article** [ hyugen ai medium com artificial early language acquisition videos cebacbe] hyugen ai medium com artificial early language acquisition videos cebacbe know model incomplete hope able enhance comments work would like read answer questions insights could thanks feedback ^ ps also participate sub another account guess fits \[r\] \[d\] \[p\] tags
[P] arXiv DOOM,235,qdnrn3,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qdnrn3/p_arxiv_doom/,18,"arXiv DOOM is a parody of the ever increasing number of papers that appear on arXiv every day  it allows you to fight the one hundred most recent papers in the cs CV category 

Play here  https  sniklaus com arxivdoom

Watch demo  https  twitter com simon_niklaus status ",1634929236.0,2021-10-22 21:00:36,arxiv doom parody ever increasing number papers appear arxiv every day allows fight one hundred recent papers cs cv category play sniklaus com arxivdoom watch demo twitter com simon_niklaus status
[N] 2021 Jacquelin Perry AI fellowship in human kinematics,0,qeftil,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qeftil/n_2021_jacquelin_perry_ai_fellowship_in_human/,0," Dear ML community 

This is an invitation to apply to the  Jacquelin Perry AI fellowship in human kinematics  This fully paid fellowship runs for five weeks  from November    through to December     For the Fall  edition  we are able to compensate only those who are currently based in the US with plans to expand this internationally for the forthcoming editions 

URL  [http  www jacquelinperryfellowship org ] http  www jacquelinperryfellowship org 

**Vision statement**

The ubiquity of high sampling rate motion sensor chips has presented us with the potential to transform the humble smartphone into a centerpiece of healthcare hardware democratization  Recent literature has shown initial promise in three facets of healthcare  Gait  and posture related disorders  Geriatric care and Treatment of neurodegenerative disorders  However  these studies have been conducted in a siloed manner with small datasets and oft using low capacity shallow machine learning models  The goal of this fellowship is to responsibly unite and standardize all of the academic world s IMU sensor datasets into one open sourced dataset that is mandated to specifically fuel innovations in the areas of healthcare mentioned above  In doing so  we d like to celebrate the incredible pioneering efforts of Dr  Jacquelin Perry     ",1635030693.0,2021-10-24 01:11:33,dear ml community invitation apply jacquelin perry ai fellowship human kinematics fully paid fellowship runs five weeks november december fall edition able compensate currently based us plans expand internationally forthcoming editions url [http www jacquelinperryfellowship org ] http www jacquelinperryfellowship org **vision statement** ubiquity high sampling rate motion sensor chips presented us potential transform humble smartphone centerpiece healthcare hardware democratization recent literature shown initial promise three facets healthcare gait posture related disorders geriatric care treatment neurodegenerative disorders however studies conducted siloed manner small datasets oft using low capacity shallow machine learning models goal fellowship responsibly unite standardize academic world imu sensor datasets one open sourced dataset mandated specifically fuel innovations areas healthcare mentioned like celebrate incredible pioneering efforts dr jacquelin perry
[R] Shaking the foundations: delusions in sequence models for interaction and control,10,qe0sl3,MachineLearning,https://arxiv.org/abs/2110.10819,5,nan,1634977126.0,2021-10-23 10:18:46,nan
[N] Deepfaking Genitalia Into Blurred Porn Leads to Man's Arrest in Japan,546,qd990q,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qd990q/n_deepfaking_genitalia_into_blurred_porn_leads_to/,43,"[https  www gizmodo com au   deepfaking genitalia into blurred porn leads to mans arrest in japan ] https  www gizmodo com au   deepfaking genitalia into blurred porn leads to mans arrest in japan 

If you want to try out the neural network yourself  you can check out my fork of the code  [https  github com tom doerr TecoGAN Docker] https  github com tom doerr TecoGAN Docker 

The fork adds a docker environment  which makes it much easier to get the code running ",1634876882.0,2021-10-22 06:28:02,[ www gizmodo com au deepfaking genitalia blurred porn leads mans arrest japan ] www gizmodo com au deepfaking genitalia blurred porn leads mans arrest japan want try neural network check fork code [ github com tom doerr tecogan docker] github com tom doerr tecogan docker fork adds docker environment makes much easier get code running
[D] Apple’s text recognition models do not seem to generalise well to rotated images.,94,qdge9o,MachineLearning,https://www.reddit.com/gallery/qdge9o,31,nan,1634907536.0,2021-10-22 14:58:56,nan
[R] New model leveraging flu data generates highly accurate prediction of COVID-19 spread,0,qe4seb,MachineLearning,https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009363,0,nan,1634995106.0,2021-10-23 15:18:26,nan
[R] Reduced false positives in autism screening via digital biomarkers inferred from deep comorbidity patterns,0,qe4rig,MachineLearning,https://www.science.org/doi/10.1126/sciadv.abf0354,0,nan,1634995014.0,2021-10-23 15:16:54,nan
[D] Leveraging Out-of-domain Data to Improve Punctuation Restoration via Text Similarity,0,qe8sbv,MachineLearning,https://youtu.be/jxOpu4hXPJY,0,nan,1635008511.0,2021-10-23 19:01:51,nan
[D] StyleGAN2 vs StyleGAN3,5,qdo1t4,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qdo1t4/d_stylegan2_vs_stylegan3/,4,"Hello 

As you know StyleGAN has just released  I am curious about the difference between StyleGAN and StyleGAN  Is the structure kinda similar or is there a huge difference ",1634930072.0,2021-10-22 21:14:32,hello know stylegan released curious difference stylegan stylegan structure kinda similar huge difference
[R] Deeper Is Not Necessarily Better: Princeton U & Intel’s 12-Layer Parallel Networks Achieve Performance Competitive With SOTA Deep Networks,11,qdii2u,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qdii2u/r_deeper_is_not_necessarily_better_princeton_u/,7,"In the new paper Non deep Networks  a research team from Princeton University and Intel Labs argues it is possible to achieve high performance with “non deep” neural networks  presenting ParNet  Parallel Networks   a novel  layer architecture that achieves performance competitive with its state of the art deep counterparts  

Here is a quick read  [Deeper Is Not Necessarily Better  Princeton U   Intel’s  Layer Parallel Networks Achieve Performance Competitive With SOTA Deep Networks ] https  syncedreview com    deepmind podracer tpu based rl frameworks deliver exceptional performance at low cost  

The code is available on the project’s [GitHub] https  github com imankgoyal NonDeepNetworks   The paper *Non deep Networks* is on [arXiv] https  arxiv org abs    ",1634914063.0,2021-10-22 16:47:43,new paper non deep networks research team princeton university intel labs argues possible achieve high performance “non deep” neural networks presenting parnet parallel networks novel layer architecture achieves performance competitive state art deep counterparts quick read [deeper necessarily better princeton u intel’s layer parallel networks achieve performance competitive sota deep networks ] syncedreview com deepmind podracer tpu based rl frameworks deliver exceptional performance low cost code available project’s [github] github com imankgoyal nondeepnetworks paper *non deep networks* [arxiv] arxiv org abs
[D] Replicating Clip+VQGAN Settings Again,0,qdwbzo,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qdwbzo/d_replicating_clipvqgan_settings_again/,3,Hey guys  I imported a reference image of a building for the text to image code to follow  I loved the results  Because the reference image is of a building  I want to replicate THE EXACT SAME results that the text to image AI created with the first reference image  but on a different perspective of the building  hence a different reference image  Does anyone know how I can lock the settings in place  so the code can generate the same result again but using a different reference image  Thanks  The code I used is here  [https  colab research google com drive goYwMFeMXXMtv cnQiSTUNEeT] https  colab research google com drive goYwMFeMXXMtv cnQiSTUNEeT ,1634957563.0,2021-10-23 04:52:43,hey guys imported reference image building text image code follow loved results reference image building want replicate exact results text image ai created first reference image different perspective building hence different reference image anyone know lock settings place code generate result using different reference image thanks code used [ colab research google com drive goywmfemxxmtv cnqistuneet] colab research google com drive goywmfemxxmtv cnqistuneet
[D] should anomaly detection with auto-encoder only be trained with normal data?,1,qduekv,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qduekv/d_should_anomaly_detection_with_autoencoder_only/,8,"Some papers proposed anomaly detection with AE VAE  What bugs me it that they remove anomalous data prior to training  in my opinion this is not unsupervised   I actually do not feel good about removing the anomalous data 

One paper I saw claimed it is not necessary for VAE 
Yet  I wonder if by training with both normal and anomalous data some model assumptions might be violated and if using normal data for training is “dirty” trick  At last anomalies  k or  k should influence the gradient",1634950284.0,2021-10-23 02:51:24,papers proposed anomaly detection ae vae bugs remove anomalous data prior training opinion unsupervised actually feel good removing anomalous data one paper saw claimed necessary vae yet wonder training normal anomalous data model assumptions might violated using normal data training “dirty” trick last anomalies k k influence gradient
"[P] An analysis of 7,020,950 NFT transactions on the Ethereum blockchain",272,qcychj,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qcychj/p_an_analysis_of_7020950_nft_transactions_on_the/,29,"Hi everyone  Not sure how interested  r MachineLearning is in crypto  but figured that you were the perfect folks to present this dataset  [https  www kaggle com simiotic ethereum nfts] https  www kaggle com simiotic ethereum nfts 

Our dataset contains    NFT transactions that took place on the Ethereum blockchain between April    and September     across   accounts 

We discovered that the top    of NFT owners control    of NFTs  This isn’t so different from traditional free markets    

We also discovered a formula to distinguish utility tokens from security tokens  Utility tokens  like the Ethereum Name Service  come with concrete external functionality  Security tokens are usually purchased purely as investments 

The full report is available on GitHub  [https  github com bugout dev moonstream blob main datasets nfts papers ethereum nfts pdf] https  github com bugout dev moonstream blob main datasets nfts papers ethereum nfts pdf 

We also published a public Kaggle notebook on which we ran our analysis  [https  www kaggle com simiotic ethereum nft analysis] https  www kaggle com simiotic ethereum nft analysis ",1634841416.0,2021-10-21 20:36:56,hi everyone sure interested r machinelearning crypto figured perfect folks present dataset [ www kaggle com simiotic ethereum nfts] www kaggle com simiotic ethereum nfts dataset contains nft transactions took place ethereum blockchain april september across accounts discovered top nft owners control nfts isn’t different traditional free markets also discovered formula distinguish utility tokens security tokens utility tokens like ethereum name service come concrete external functionality security tokens usually purchased purely investments full report available github [ github com bugout dev moonstream blob main datasets nfts papers ethereum nfts pdf] github com bugout dev moonstream blob main datasets nfts papers ethereum nfts pdf also published public kaggle notebook ran analysis [ www kaggle com simiotic ethereum nft analysis] www kaggle com simiotic ethereum nft analysis
[N] Symposium on Trustworthy Machine Learning on Thurs Oct 28,2,qdq3k0,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qdq3k0/n_symposium_on_trustworthy_machine_learning_on/,0,"Hi  I am involved with the Trustworthy ML Initiative  I hope it s OK for me to post a recent event organized by us  with a joint effort from Montreal AI Ethics Institute  just in case there are friends here interested in the trustworthy ML related topics   D

Some of the details are pasted below  or better viewed here  [https  tinyurl com trustmlsymposium] https  tinyurl com trustmlsymposium 

\ 

The Trustworthy ML Initiative celebrates its one year anniversary with this special event  held jointly with Montreal AI Ethics Institute 

To achieve the promise of AI as a tool for societal impact  black box models must not only be  accurate  but also satisfy trustworthiness properties that facilitate open collaboration and ensure ethical and safe outcomes  The purpose of this un symposium is to discuss the interdisciplinary topics of robustness  fairness  privacy  and ethics of AI tools  In particular  we want to highlight the significant gap in deploying these AI models in practice when the stakes are high for commercial applications of AI where millions of human lives are at risk  We welcome researchers  stakeholders  and domain experts to join us 

Agenda  Eastern Time  

 am   Opening remarks  What is the invisible elephant in the room 

 am   Panel on  Interdisciplinary Research in Trustworthy ML   Challenges and Way Forward   

Panelists  Danielle Belgrave  DeepMind   Tom Dietterich  Oregon State Univ   Kush Varshney  IBM Research   Moderator  Subho Majumdar  Splunk 

 am   Break

    Townhall on  Practical Challenges of Applying Trustworthy ML in Industry   

Panelists  Stella Biderman  EleutherAI   Cristian Canton Ferrer  Facebook   Krishnaram Kenthapadi  Amazon   Moderator  Abhishek Gupta  Montreal AI Ethics Institute 

    Break  

Adjourn to separate Zoom link [https  usweb zoom us j  pwd=cjBXaWVuRlRuZUIVGczcXJTZVZmQT] https  usweb zoom us j  pwd=cjBXaWVuRlRuZUIVGczcXJTZVZmQT 

  to pm   Social with breakout rooms  Host  Chirag Agarwal  Harvard Univ 

\    Interpretability  Host  Chhavi Yadav  UCSD 

\    Is unfairness a security risk  Host  Mohammad Yaghini  U Toronto 

\    Robustness  Host  Haohan Wang  CMU 

\    Fairness  Host  Marta Lemanczyk  Hasso Plattner Institute 

\    Causal inference  Host  Maggie Makar  U Michigan ",1634936230.0,2021-10-22 22:57:10,hi involved trustworthy ml initiative hope ok post recent event organized us joint effort montreal ai ethics institute case friends interested trustworthy ml related topics details pasted better viewed [ tinyurl com trustmlsymposium] tinyurl com trustmlsymposium \ trustworthy ml initiative celebrates one year anniversary special event held jointly montreal ai ethics institute achieve promise ai tool societal impact black box models must accurate also satisfy trustworthiness properties facilitate open collaboration ensure ethical safe outcomes purpose un symposium discuss interdisciplinary topics robustness fairness privacy ethics ai tools particular want highlight significant gap deploying ai models practice stakes high commercial applications ai millions human lives risk welcome researchers stakeholders domain experts join us agenda eastern time opening remarks invisible elephant room panel interdisciplinary research trustworthy ml challenges way forward panelists danielle belgrave deepmind tom dietterich oregon state univ kush varshney ibm research moderator subho majumdar splunk break townhall practical challenges applying trustworthy ml industry panelists stella biderman eleutherai cristian canton ferrer facebook krishnaram kenthapadi amazon moderator abhishek gupta montreal ai ethics institute break adjourn separate zoom link [ usweb zoom us j pwd=cjbxawvurlruzuivgczcxjtzvzmqt] usweb zoom us j pwd=cjbxawvurlruzuivgczcxjtzvzmqt pm social breakout rooms host chirag agarwal harvard univ \ interpretability host chhavi yadav ucsd \ unfairness security risk host mohammad yaghini u toronto \ robustness host haohan wang cmu \ fairness host marta lemanczyk hasso plattner institute \ causal inference host maggie makar u michigan
[R] Overcome survivorship bias in liver transplant patients,4,qdkt5e,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qdkt5e/r_overcome_survivorship_bias_in_liver_transplant/,3,"Hi  I m working on my master thesis in the field of ML and AI and I m stuck with a problem related to [survivorship bias] https  en wikipedia org wiki Survivorship_bias  

I have \~ patients from the national waiting list for liver transplantation  Every patient has around  features  age  gender  pathologies  diagnosis     and my job is to obtain the **transplant benefit  how much the patient life was extended thanks to the transplant** 

Example  given a patient  he was added to the waiting list in   He will die in  if he doesn t receive a transplant  He will die in  if he receives a transplant  This means that the transplant benefit is  months or  years 

The first step is to build a model to regress how much a patient will survive without receiving a transplant  For example  given model M and patient p_i  I want that M p_i  = y_i  where y_i is the amount of months p_i survives without getting the transplant 

The problem related to survivorship bias is the following  **patients aren t from the same  population ** [visualization] https  imgur com IZtT  
   some patients weren t able to receive a transplant  e g  discarded because incompatible   group A
   other patients received a transplant and survived some more time  group B

At the moment  I m forced to train my model M on group A patients because only they haven t received a transplant and they are died because of this  On the contrary group B patients have received a transplant  so cannot be used at this stage 

The issue arises when I want to compute the transplant benefit of transplanted patient p_j 
   check when p_j died after the transplant  months_t
   use model M to obtain survival without transplant  M p_j  = months_w
   transplant benefit = months_t   months_w

Do you see the issue  M was trained only on group A patients  so it is highly inaccurate when applied to group B patients  It s a survivorship bias problem because the group that I can use for training doesn t represent the entire population but only a subset  that is  only patients discarded and incompatible 

Is there a way to overcome this problem 
It s two days that I m searching papers and articles on ways to solve this issue  but most of the  solutions  propose to use data I have no access to  e g  train model M on group B patients  but I haven t their death date _without_ transplant  

**TL DR**
I have two groups of patients  A and B  Group B has missing labels so I train my model only on group A patients  But this creates huge problems because the different groups have very different populations and characteristics  group B contains patients that survived while group A patients were discarded  survivorship bias  ",1634920676.0,2021-10-22 18:37:56,hi working master thesis field ml ai stuck problem related [survivorship bias] en wikipedia org wiki survivorship_bias \~ patients national waiting list liver transplantation every patient around features age gender pathologies diagnosis job obtain **transplant benefit much patient life extended thanks transplant** example given patient added waiting list die receive transplant die receives transplant means transplant benefit months years first step build model regress much patient survive without receiving transplant example given model patient p_i want p_i = y_i y_i amount months p_i survives without getting transplant problem related survivorship bias following **patients population ** [visualization] imgur com iztt patients able receive transplant e g discarded incompatible group patients received transplant survived time group b moment forced train model group patients received transplant died contrary group b patients received transplant cannot used stage issue arises want compute transplant benefit transplanted patient p_j check p_j died transplant months_t use model obtain survival without transplant p_j = months_w transplant benefit = months_t months_w see issue trained group patients highly inaccurate applied group b patients survivorship bias problem group use training represent entire population subset patients discarded incompatible way overcome problem two days searching papers articles ways solve issue solutions propose use data access e g train model group b patients death date _without_ transplant **tl dr** two groups patients b group b missing labels train model group patients creates huge problems different groups different populations characteristics group b contains patients survived group patients discarded survivorship bias
"[Discussion] Framerate interpolation with other, lower quality video, which has a higher framerate",1,qdsycu,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qdsycu/discussion_framerate_interpolation_with_other/,6,There are two same content videos  The catch is that one is low quality Fps  but the other one is high quality Fps  Is there a tool to interpolate the high quality to  fps that can be trained with the low quality video to reduce interpolation artifacts ,1634945119.0,2021-10-23 01:25:19,two content videos catch one low quality fps one high quality fps tool interpolate high quality fps trained low quality video reduce interpolation artifacts
"[P] Easy to install, use, extend, run experiments and sink results: PyTorch Implementation for ProSelfLC-CVPR 2021",0,qdshgh,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qdshgh/p_easy_to_install_use_extend_run_experiments_and/,0,"PyTorch Implementation for ProSelfLC CVPR 

[https  github com XinshaoAmosWang ProSelfLC CVPR] https  github com XinshaoAmosWang ProSelfLC CVPR 

* Easy to install
* Easy to use
* Easy to extend  new losses  new networks  new dataset and loaders
* Easy to run experiments and sink results

 xB 

[Accuracy curve] https  preview redd it bhwdwv png width= format=png auto=webp s=ebdfaaebcaaae 

 xB 

 xB 

[Loss curve] https  preview redd it cshqliv png width= format=png auto=webp s=abfdbbebacbdaadcbee 

 xB 

[params csv] https  preview redd it vprjrnv png width= format=png auto=webp s=cbedffebadbf 

 xB 

[accuracy\_loss xlsx] https  preview redd it bqzrpizrv png width= format=png auto=webp s=cfcedfcbceadbabdfc ",1634943537.0,2021-10-23 00:58:57,pytorch implementation proselflc cvpr [ github com xinshaoamoswang proselflc cvpr] github com xinshaoamoswang proselflc cvpr * easy install * easy use * easy extend new losses new networks new dataset loaders * easy run experiments sink results xb [accuracy curve] preview redd bhwdwv png width= format=png auto=webp s=ebdfaaebcaaae xb xb [loss curve] preview redd cshqliv png width= format=png auto=webp s=abfdbbebacbdaadcbee xb [params csv] preview redd vprjrnv png width= format=png auto=webp s=cbedffebadbf xb [accuracy\_loss xlsx] preview redd bqzrpizrv png width= format=png auto=webp s=cfcedfcbceadbabdfc
Mujoco Tutorial [R],0,qe0zpt,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qe0zpt/mujoco_tutorial_r/,3,Can someone recommend an in depth tutorial for Mujoco  not the Python interface   Thanks ,1634978163.0,2021-10-23 10:36:03,someone recommend depth tutorial mujoco python interface thanks
[R] Handling Large Consecutive Missing data for forecasting using LSTM,11,qddelh,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qddelh/r_handling_large_consecutive_missing_data_for/,8,"Hi there  I am trying to use **LSTM for water level forecasting during the monsoon**  I have multiple input water stations and one output water station  I am taking  days of data to predict the output station s water level the next day  I have a dataset from  to   The problem with my dataset is that it contains many missing values  Most of the **data are missing during the non monsoon months**  Also  data is missing from  to  and in other places  many months too  I have imputed the data using python libraries  but the results are not good as the **gap is too large** 

Imputing such data is creating too much noise  As my focus is primarily on monsoon data  **can I discard the non monsoon months **

Another thing I wanted to know is that is it okay to perform **Litwise Deletion** on a dataset that will be used to train a neural network model for forecasting  Especially when I take a length of input data like consecutive days of data as input  

Personally  as my primary goal is to make water level predictions in monsoon time  I want to discard the non monsoon months and remove any missing data by Litwise deletion  I did perform the forecasting prediction by discarding the non monsoon months  and I got **better results** than imputing the whole dataset for monsoon and non monsoon months  ",1634895938.0,2021-10-22 11:45:38,hi trying use **lstm water level forecasting monsoon** multiple input water stations one output water station taking days data predict output station water level next day dataset problem dataset contains many missing values **data missing non monsoon months** also data missing places many months imputed data using python libraries results good **gap large** imputing data creating much noise focus primarily monsoon data **can discard non monsoon months ** another thing wanted know okay perform **litwise deletion** dataset used train neural network model forecasting especially take length input data like consecutive days data input personally primary goal make water level predictions monsoon time want discard non monsoon months remove missing data litwise deletion perform forecasting prediction discarding non monsoon months got **better results** imputing whole dataset monsoon non monsoon months
"[N] PyTorch 1.10 Release, including CUDA Graphs APIs, Frontend and compiler improvements",161,qcvfaj,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qcvfaj/n_pytorch_110_release_including_cuda_graphs_apis/,20,"Source  [https  github com pytorch pytorch releases tag v  ] https  github com pytorch pytorch releases tag v    and [https  pytorch org blog pytorch   released ] https  pytorch org blog pytorch   released 

  Highlights

We are excited to announce the release of PyTorch    This release  is composed of over   commits since    made by  contributors   We want to sincerely thank our community for continuously improving  PyTorch 

PyTorch   updates are focused on improving training and performance of PyTorch  and developer usability  Highlights include 

 xB 

* CUDA Graphs APIs are integrated to reduce CPU overheads for CUDA workloads 
* Several frontend APIs such as FX  torch special  and nn ModuleParametrization  have moved from beta to stable 
* Support for automatic fusion in JIT Compiler expands to CPUs in addition to GPUs 
* Android NNAPI support is now available in beta ",1634833126.0,2021-10-21 18:18:46,source [ github com pytorch pytorch releases tag v ] github com pytorch pytorch releases tag v [ pytorch org blog pytorch released ] pytorch org blog pytorch released highlights excited announce release pytorch release composed commits since made contributors want sincerely thank community continuously improving pytorch pytorch updates focused improving training performance pytorch developer usability highlights include xb * cuda graphs apis integrated reduce cpu overheads cuda workloads * several frontend apis fx torch special nn moduleparametrization moved beta stable * support automatic fusion jit compiler expands cpus addition gpus * android nnapi support available beta
[R] Teach Me to Explain: A Review of Datasets for Explainable NLP,8,qdadbs,MachineLearning,https://arxiv.org/abs/2102.12060,2,nan,1634881716.0,2021-10-22 07:48:36,nan
[D] NearestNeighbors - Gowers Distance consumes RAM very quickly,1,qdkz5o,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qdkz5o/d_nearestneighbors_gowers_distance_consumes_ram/,4,"I am trying to cluster a mixed dataset having both numerical and  categorical dataset in Python using  gower s  distance followed by  hyper parameter tuned DBSCAN algorithm  Traditional clustering algos don t work for categorical attributes columns and I don t want to drop such attributes  The complete code and the dataset can be referred [here] https  github com arjun majumdar machine learning musing blob master Mixed_Dataset_Clustering_Example ipynb  

*Gower s distance* creates a square matrix of dimensions  m x m where  m   is the number of rows in the dataset  For the Adult dataset   it s shape is       So  gower s distance creates a square matrix of  shape       Assuming each number takes  bit floating point  precision  this matrix takes up roughly  GB of RAM 

To hyper parameter tune DBSCAN algo  DMDBSCAN method is used  On using the *NearestNeighbors* algorithm on this matrix 

    neighbors = NearestNeighbors n_neighbors =  
    nbrs = neighbors fit loaded_gower_dist  

crashes the Kernel in Google Colab environment which has around  GB of RAM  Any way to avoid this  Also  this shows that Gower s distance doesn t scale well as the dataset size grows 

Can you suggest better way s  to avoid this which also scales well ",1634921161.0,2021-10-22 18:46:01,trying cluster mixed dataset numerical categorical dataset python using gower distance followed hyper parameter tuned dbscan algorithm traditional clustering algos work categorical attributes columns want drop attributes complete code dataset referred [here] github com arjun majumdar machine learning musing blob master mixed_dataset_clustering_example ipynb *gower distance* creates square matrix dimensions x number rows dataset adult dataset shape gower distance creates square matrix shape assuming number takes bit floating point precision matrix takes roughly gb ram hyper parameter tune dbscan algo dmdbscan method used using *nearestneighbors* algorithm matrix neighbors = nearestneighbors n_neighbors = nbrs = neighbors fit loaded_gower_dist crashes kernel google colab environment around gb ram way avoid also shows gower distance scale well dataset size grows suggest better way avoid also scales well
[D] How good are the new M1 Pro / Max macs with ARM processors for deep learning with PyTorch?,52,qcyw4l,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qcyw4l/d_how_good_are_the_new_m1_pro_max_macs_with_arm/,13,"Hi 

I need a mac for mobile development but I also use PyTorch  I have seen that TensorFlow has the [TensorFlow metal] https  developer apple com metal tensorflow plugin  plugin provided by Apple for this purpose  but I don t know how PyTorch behaves in this field 

Do you have some insights  

Thanks",1634843006.0,2021-10-21 21:03:26,hi need mac mobile development also use pytorch seen tensorflow [tensorflow metal] developer apple com metal tensorflow plugin plugin provided apple purpose know pytorch behaves field insights thanks
"[R] Meta-learning, social cognition and consciousness in brains and machines",0,qdjcv2,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qdjcv2/r_metalearning_social_cognition_and_consciousness/,2,"Published in Neural Networks on  Oct 

Open Access  https  www sciencedirect com science article pii S

**Abstract**

The intersection between neuroscience and artificial intelligence  AI  research has created synergistic effects in both fields  While neuroscientific discoveries have inspired the development of AI architectures  new ideas and algorithms from AI research have produced new ways to study brain mechanisms  A well known example is the case of reinforcement learning  RL   which has stimulated neuroscience research on how animals learn to adjust their behavior to maximize reward  In this review article  we cover recent collaborative work between the two fields in the context of meta learning and its extension to social cognition and consciousness  Meta learning refers to the ability to learn how to learn  such as learning to adjust hyperparameters of existing learning algorithms and how to use existing models and knowledge to efficiently solve new tasks  This meta learning capability is important for making existing AI systems more adaptive and flexible to efficiently solve new tasks  Since this is one of the areas where there is a gap between human performance and current AI systems  successful collaboration should produce new ideas and progress  Starting from the role of RL algorithms in driving neuroscience  we discuss recent developments in deep RL applied to modeling prefrontal cortex functions  Even from a broader perspective  we discuss the similarities and differences between social cognition and meta learning  and finally conclude with speculations on the potential links between intelligence as endowed by model based RL and consciousness  For future work we highlight data efficiency  autonomy and intrinsic motivation as key research areas for advancing both fields 

*Keywords*

Model based reinforcement learning  Meta learning  Social cognition  Consciousness",1634916519.0,2021-10-22 17:28:39,published neural networks oct open access www sciencedirect com science article pii **abstract** intersection neuroscience artificial intelligence ai research created synergistic effects fields neuroscientific discoveries inspired development ai architectures new ideas algorithms ai research produced new ways study brain mechanisms well known example case reinforcement learning rl stimulated neuroscience research animals learn adjust behavior maximize reward review article cover recent collaborative work two fields context meta learning extension social cognition consciousness meta learning refers ability learn learn learning adjust hyperparameters existing learning algorithms use existing models knowledge efficiently solve new tasks meta learning capability important making existing ai systems adaptive flexible efficiently solve new tasks since one areas gap human performance current ai systems successful collaboration produce new ideas progress starting role rl algorithms driving neuroscience discuss recent developments deep rl applied modeling prefrontal cortex functions even broader perspective discuss similarities differences social cognition meta learning finally conclude speculations potential links intelligence endowed model based rl consciousness future work highlight data efficiency autonomy intrinsic motivation key research areas advancing fields *keywords* model based reinforcement learning meta learning social cognition consciousness
[R] AI Researchers From Huawei and Shanghai Jiao Tong University Introduce ‘CIPS-3D’: A 3D-Aware Generator of GANs,7,qd7cve,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qd7cve/r_ai_researchers_from_huawei_and_shanghai_jiao/,1,"The StyleGAN architecture is a great way to generate high quality images  but it lacks the ability to control camera poses precisely  The recent NeRF based Generators have made progress towards creating real results so far as they can’t produce photorealistic images 

Researchers at Huawei and Shanghai Jiao Tong University have developed [CIPS D] https  github com PeterouZh CIPS D   an approach that synthesizes each pixel value independently  just as its D version did 

The proposed generator consists of a shallow D NeRF network simplified to alleviate memory complexity and has the capacity for deep D INR  implicit neural representation  networks without any spatial convolution or up sampling operations  The proposed generator’s design is consistent with the well known semantic hierarchical principle of GANs  where early layers  i e  the shallow NeRF network in the generator  determine pose and middle high  i e  the INR network in the generator  control color scheme  The early NeRF network enables the research team to control camera pose explicitly easily 

  [Quick  Min Read] https  www marktechpost com    ai researchers from huawei and shanghai jiao tong university introduce cips d a d aware generator of gans    [Paper] https  arxiv org pdf   pdf    [Github] https  github com PeterouZh CIPS D 

 xB 

https  reddit com link qdcve video ialxtxwu player",1634869864.0,2021-10-22 04:31:04,stylegan architecture great way generate high quality images lacks ability control camera poses precisely recent nerf based generators made progress towards creating real results far can’t produce photorealistic images researchers huawei shanghai jiao tong university developed [cips d] github com peterouzh cips approach synthesizes pixel value independently version proposed generator consists shallow nerf network simplified alleviate memory complexity capacity deep inr implicit neural representation networks without spatial convolution sampling operations proposed generator’s design consistent well known semantic hierarchical principle gans early layers e shallow nerf network generator determine pose middle high e inr network generator control color scheme early nerf network enables research team control camera pose explicitly easily [quick min read] www marktechpost com ai researchers huawei shanghai jiao tong university introduce cips aware generator gans [paper] arxiv org pdf pdf [github] github com peterouzh cips xb reddit com link qdcve video ialxtxwu player
[R] DeepMind’s Fictitious Co-Play Trains RL Agents to Collaborate with Novel Humans Without Using Human Data,29,qct7u0,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qct7u0/r_deepminds_fictitious_coplay_trains_rl_agents_to/,1,"A DeepMind research team explores the problem of how to train agents to collaborate well with novel human partners without using human data and presents Fictitious Co Play  FCP   a surprisingly simple approach designed to address this challenge  

Here is a quick read  [DeepMind’s Fictitious Co Play Trains RL Agents to Collaborate with Novel Humans Without Using Human Data ] https  syncedreview com    deepmind podracer tpu based rl frameworks deliver exceptional performance at low cost  

The paper *Collaborating With Humans Without Human Data* is on [arXiv] https  arxiv org abs    ",1634826936.0,2021-10-21 16:35:36,deepmind research team explores problem train agents collaborate well novel human partners without using human data presents fictitious co play fcp surprisingly simple approach designed address challenge quick read [deepmind’s fictitious co play trains rl agents collaborate novel humans without using human data ] syncedreview com deepmind podracer tpu based rl frameworks deliver exceptional performance low cost paper *collaborating humans without human data* [arxiv] arxiv org abs
[R] Unknown Object Segmentation from Stereo Images,1,qdchtj,MachineLearning,https://arxiv.org/abs/2103.06796,2,nan,1634891615.0,2021-10-22 10:33:35,nan
[D] Pure math relevant to machine learning?,123,qckeph,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qckeph/d_pure_math_relevant_to_machine_learning/,62,"I am currently a UG in Math and CS and am interested in machine learning  I would like to know what areas of pure abstract math are being applied to  or give insight into the field of machine learning  deep learning  neural networks etc  I tried searching for similar questions but most of them are    years old and  I would like to know about the current scene 

Apart from the absolute basics like calculus  statistics and linear algebra  are there other areas of math being applied  For example  abstract algebra  complex analysis  topology  measure theory etc  I have read blogposts by Jeremy Kun  Chris Olah and a few others about these things  but am not sure if they are accurate descriptions of the work being done currently 

Also  is there any future scope of this becoming relevant ",1634793771.0,2021-10-21 07:22:51,currently ug math cs interested machine learning would like know areas pure abstract math applied give insight field machine learning deep learning neural networks etc tried searching similar questions years old would like know current scene apart absolute basics like calculus statistics linear algebra areas math applied example abstract algebra complex analysis topology measure theory etc read blogposts jeremy kun chris olah others things sure accurate descriptions work done currently also future scope becoming relevant
"[D] Link grammars, symbolic representations, and about using similarities and substitutions algorithms.",0,qd7khs,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qd7khs/d_link_grammars_symbolic_representations_and/,6,"This hypothesis is highly related to the presentations given by Linas Vepštas first and then Anton Kolonin at SingularityNet AGI  Conference about  link grammars  symbolic representations  and about using similarities and substitutions algorithms 

In  [https  www youtube com watch v=RcgydmdlYg] https  www youtube com watch v=RcgydmdlYg 

You need to see the first two presentations to understand what I m trying to comunicate here 

I’ll try to be very concise  In short my statement is that with similarities and substitutions you can have solutions of previously unsolved problems  In other words  generate new knowledge using previous related knowledge 

I apologize in advance for my lack of proper technical terms  But  if the idea get through  that will be enough for me  I hope my explanation is understandable 

The following is a graph composed by sub graphs  the nodes are linked by the equal ‘=’ relation  which goes in both directions in case there is no more links  by default is left to right  The nodes can be single values or sets 

For briefness and explain ability I will use this notation instead of an actual graph  image  

About my notations  

* \[v  v\] is a set 
*  =  is the relation 
* Every line can be seen as a sub graph 
* I will use   to add a comment for explanation
* A ‘query’ is a new node that has no similarity relation in the graph and therefore the algorithm needs to be used

**The initial state of the graph and a case of a query and the result **

    Alice = programmer
    Bob = engineer
    Mary = designer
    John = programmer
    Rose = engineer
    Joe = designer
    [engineer  job] = [model  system]
    [programmer  job] = [develop  system]
    [designer  job] = [design  UI]
    Alice = available
    Bob = available
    Mary = available
    Alice = [available  programmer]
    Bob = [available  engineer]
    Mary = [available  designer]
      initially available  but later relations are updated to working
    John = working 
    Rose = working
    Joe = working
    R = name
    R = name
    solution = [
        [[available  engineer]  [engineer  job]]  
        [[available  programmer]  [programmer  job]]  
        [[available  designer]  [designer  job]]
    ]
    [new  request] = [unsolved  request]
    [request  name] = request
    [solution  name] = solution
    [resolve  request] = work
    work = [solution  for  request]
      if we have a [unsolved  request] we want to [resolve  request]
      this is a simplification  It could have more meaning if we add more 
      details to this relation
    [unsolved  request] = [resolve  request] 
      the query is [Medical  report  system]
      the query is linked to be equal to a [new  request]
    [new  request] = [Medical  report  system]
      generated [request  R] to identify the query 
    [Medical  report  system] = [request  R] 
      given that [unsolved  request] = [new  request]
    [request  R] = [unsolved  request] 
    [request  R] = [request  name]   given that R = name
    [request  R] = request
    [request  R] = [resolve  request]   given that  [unsolved request]=[request R]
    [resolve  [request  R]] = work   given that  request = [request  R]
      here [solution  R] can be generated by a stored procedure for simplicity  
      and in this case is used to
      identify the node that will be the actual solution
      given that  work = [resolve  request]  and work = [solution  for  request]
    [solution  for  [request  R]] = [solution  R] 
    [solution  R] = [solution  name] 
    [solution  R] = solution
      final state  here the replacement for the node ‘solution’ is 
      used to produce the final relation
    [solution  R] = [
      [Rose  [model  system]]  
      [John  [develop  system]]  
      [Joe  [design  UI]]
    ]
      For the next part  to do a new query I will do the following to avoid ambiguity
      this can be resolve in multiple ways  in this case I’ll go with this
    [request  R]  = [new  request] 

**At this point I will do a new query to illustrate how with substitutions we can generate a new knowledge  given the previous state of graph **

    [new  request] = [Hotel  management  system]   new query is [Hotel  management  system]
    [Hotel  management  system] = [request  R]   generate a identification node
    [request  R] = [request  name]   given that  R = name
    [request  name] = [request  R]   we have this relation  then
    [request  R] = [request  R]   therefore
    [request  R] = [solution  R]   is the current most similar  but
    [request  R] is not similar enough to [solution  R] because 
      If we take into account the full sequence of substitutions we will notice 
      that [Hotel management system] is not equal to [Medical report system]
      We can do better  if we use  solution  instead of [solution  R] 
    
    [solution  R] = solution
    [request  R] = solution   given the previous relation
    [request  R] = [
        [[available  engineer]  [engineer  job]]  
        [[available  programmer]  [programmer  job]]  
        [[available  designer]  [designer  job]]
    ]
    [request  R] = [
        [Bob  [model  system]]  
        [Alice  [develop  system]]  
        [Mary  [design  UI]]
    ]
      final output
    [solution  R] = [
        [Bob  [model  system]]  
        [Alice  [develop  system]]  
        [Mary  [design  UI]]
    ]

This example is not really that interesting  and looking at the result it seems pretty obvious  But the key here is that is generalizable  and is just substitutions 

This substitution mechanism can be applied to multiple cases to solve any kind of situations  given that it has enough previous knowledge  Is like applying a formula  step by step  Every step is guided by a previously known relation  This mechanism should be the algorithm applied directly to the graph  so that is the process with which the graph change from one state to the next state ",1634870602.0,2021-10-22 04:43:22,hypothesis highly related presentations given linas vepštas first anton kolonin singularitynet agi conference link grammars symbolic representations using similarities substitutions algorithms [ www youtube com watch v=rcgydmdlyg] www youtube com watch v=rcgydmdlyg need see first two presentations understand trying comunicate i’ll try concise short statement similarities substitutions solutions previously unsolved problems words generate new knowledge using previous related knowledge apologize advance lack proper technical terms idea get enough hope explanation understandable following graph composed sub graphs nodes linked equal ‘=’ relation goes directions case links default left right nodes single values sets briefness explain ability use notation instead actual graph image notations * \[v v\] set * = relation * every line seen sub graph * use add comment explanation * ‘query’ new node similarity relation graph therefore algorithm needs used **the initial state graph case query result ** alice = programmer bob = engineer mary = designer john = programmer rose = engineer joe = designer [engineer job] = [model system] [programmer job] = [develop system] [designer job] = [design ui] alice = available bob = available mary = available alice = [available programmer] bob = [available engineer] mary = [available designer] initially available later relations updated working john = working rose = working joe = working r = name r = name solution = [ [[available engineer] [engineer job]] [[available programmer] [programmer job]] [[available designer] [designer job]] ] [new request] = [unsolved request] [request name] = request [solution name] = solution [resolve request] = work work = [solution request] [unsolved request] want [resolve request] simplification could meaning add details relation [unsolved request] = [resolve request] query [medical report system] query linked equal [new request] [new request] = [medical report system] generated [request r] identify query [medical report system] = [request r] given [unsolved request] = [new request] [request r] = [unsolved request] [request r] = [request name] given r = name [request r] = request [request r] = [resolve request] given [unsolved request]=[request r] [resolve [request r]] = work given request = [request r] [solution r] generated stored procedure simplicity case used identify node actual solution given work = [resolve request] work = [solution request] [solution [request r]] = [solution r] [solution r] = [solution name] [solution r] = solution final state replacement node ‘solution’ used produce final relation [solution r] = [ [rose [model system]] [john [develop system]] [joe [design ui]] ] next part new query following avoid ambiguity resolve multiple ways case i’ll go [request r] = [new request] **at point new query illustrate substitutions generate new knowledge given previous state graph ** [new request] = [hotel management system] new query [hotel management system] [hotel management system] = [request r] generate identification node [request r] = [request name] given r = name [request name] = [request r] relation [request r] = [request r] therefore [request r] = [solution r] current similar [request r] similar enough [solution r] take account full sequence substitutions notice [hotel management system] equal [medical report system] better use solution instead [solution r] [solution r] = solution [request r] = solution given previous relation [request r] = [ [[available engineer] [engineer job]] [[available programmer] [programmer job]] [[available designer] [designer job]] ] [request r] = [ [bob [model system]] [alice [develop system]] [mary [design ui]] ] final output [solution r] = [ [bob [model system]] [alice [develop system]] [mary [design ui]] ] example really interesting looking result seems pretty obvious key generalizable substitutions substitution mechanism applied multiple cases solve kind situations given enough previous knowledge like applying formula step step every step guided previously known relation mechanism algorithm applied directly graph process graph change one state next state
[D] Sensorium Paper explained - Harnessing the Conditioning Sensorium for Improved Image Translation (5-minute summary by Casual GAN Papers),7,qcu7js,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qcu7js/d_sensorium_paper_explained_harnessing_the/,0,"Image  to image translation appears more or less “solved” on the surface   yet  there are still several important challenges to overcome  One such   challenge is the ambiguity in multi modal  reference guided   image to image domain translation  Believing that the choice of what to   preserve as the “content” of the input image  and “style” should be   transferred from the target image during domain translation depends   heavily on the task at hand  Cooper Nederhood and his colleagues propose   Sensorium  a new model that conditions its output on the information   from various off the shelf pretrained models depending on the task    Sensorium enables higher quality domain translation for more complex  scenes 

Fresh out of the oven  Full summary  [https  www casualganpapers com multimodal style conditioned image to image domain translation Sensorium explained html] https  www casualganpapers com multimodal style conditioned image to image domain translation Sensorium explained html 

[Sensorium] https  preview redd it eywowemtu png width= format=png auto=webp s=ceaebdcefbfecf 

arxiv  [https  arxiv org abs  ] https  arxiv org abs     
code   

Subscribe to [Casual GAN Papers] https  t me casual_gan  and follow me on [Twitter] https  twitter com KirillDemochkin  for weekly AI paper summaries ",1634829699.0,2021-10-21 17:21:39,image image translation appears less “solved” surface yet still several important challenges overcome one challenge ambiguity multi modal reference guided image image domain translation believing choice preserve “content” input image “style” transferred target image domain translation depends heavily task hand cooper nederhood colleagues propose sensorium new model conditions output information various shelf pretrained models depending task sensorium enables higher quality domain translation complex scenes fresh oven full summary [ www casualganpapers com multimodal style conditioned image image domain translation sensorium explained html] www casualganpapers com multimodal style conditioned image image domain translation sensorium explained html [sensorium] preview redd eywowemtu png width= format=png auto=webp s=ceaebdcefbfecf arxiv [ arxiv org abs ] arxiv org abs code subscribe [casual gan papers] casual_gan follow [twitter] twitter com kirilldemochkin weekly ai paper summaries
[D] Going through Deep Ensemble Paper and Have Question,2,qczrqk,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qczrqk/d_going_through_deep_ensemble_paper_and_have/,6,"The deep ensemble paper says they capture many modes but not the curvature of each   Variation methods say they capture the curvature of one of the modes   The obvious question is then  why not do ensembles of variation methods 

Does anyone do this   How are the results 

Paper is here  [https  arxiv org abs  ] https  arxiv org abs   ",1634845476.0,2021-10-21 21:44:36,deep ensemble paper says capture many modes curvature variation methods say capture curvature one modes obvious question ensembles variation methods anyone results paper [ arxiv org abs ] arxiv org abs
"[N] Open Colloquium by Prof. Max Welling: ""Is the next deep learning disruption in the physical sciences?""",112,qcaia2,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qcaia2/n_open_colloquium_by_prof_max_welling_is_the_next/,16," xB 

https  preview redd it ugolpou jpg width= format=pjpg auto=webp s=cfdbacdeaacefb

We invite everyone to the online research colloquium by Max Welling from the University of Amsterdam  Participation is free 

**When **  October     GMT    Moscow time 

**Title**  Is the next deep learning disruption in the physical sciences 

**Join ** [https  cs hse ru en announcements  html] https  cs hse ru en announcements  html  

**Abstract ** 

A number of fields  most prominently speech  vision and NLP have been disrupted by deep learning technology  A natural question is   which application areas will follow next    My prediction is that the physical sciences will experience an unprecedented acceleration by combining the tools of simulation on HPC clusters with the tools of deep learning to improve and accelerate this process  Together  they form a virtuous cycle where simulations create data that feeds into deep learning models which in turn improves the simulations  In a way  this is like building a self learning computational microscope for the physical sciences  In this talk I will illustrate this using two recent pieces of work from my lab  molecular simulation and PDE solving  In molecular simulation we try to predict molecular properties or digitally synthesize molecules with prescribed properties  We have built a number of equivariant graph neural networks to achieve this  Partial differential equations  PDEs  are the most used mathematical model in natural sciences to describe physical processes  Intriguingly  we find that PDE solvers can be learned from data using graph neural networks as well  which has the added benefit that we can learn a solver that can generalize across PDEs and different boundary conditions  Moreover  it may open the door to ab initio learning of PDEs directly from data 

About the speaker  

**Prof  Dr  Max** **Welling** is a research chair in Machine Learning at the University of Amsterdam and a Distinguished Scientist at MSR  He is a fellow at the Canadian Institute for Advanced Research  CIFAR  and the European Lab for Learning and Intelligent Systems  ELLIS  where he also serves on the founding board  His previous appointments include VP at Qualcomm Technologies  professor at UC Irvine  postdoc at U  Toronto and UCL under supervision of prof  Geoffrey Hinton  and postdoc at Caltech under supervision of prof  Pietro Perona  He finished his PhD in theoretical high energy physics under supervision of Nobel laureate prof  Gerard ‘t Hooft ",1634762680.0,2021-10-20 22:44:40,xb preview redd ugolpou jpg width= format=pjpg auto=webp s=cfdbacdeaacefb invite everyone online research colloquium max welling university amsterdam participation free **when ** october gmt moscow time **title** next deep learning disruption physical sciences **join ** [ cs hse ru en announcements html] cs hse ru en announcements html **abstract ** number fields prominently speech vision nlp disrupted deep learning technology natural question application areas follow next prediction physical sciences experience unprecedented acceleration combining tools simulation hpc clusters tools deep learning improve accelerate process together form virtuous cycle simulations create data feeds deep learning models turn improves simulations way like building self learning computational microscope physical sciences talk illustrate using two recent pieces work lab molecular simulation pde solving molecular simulation try predict molecular properties digitally synthesize molecules prescribed properties built number equivariant graph neural networks achieve partial differential equations pdes used mathematical model natural sciences describe physical processes intriguingly find pde solvers learned data using graph neural networks well added benefit learn solver generalize across pdes different boundary conditions moreover may open door ab initio learning pdes directly data speaker **prof dr max** **welling** research chair machine learning university amsterdam distinguished scientist msr fellow canadian institute advanced research cifar european lab learning intelligent systems ellis also serves founding board previous appointments include vp qualcomm technologies professor uc irvine postdoc u toronto ucl supervision prof geoffrey hinton postdoc caltech supervision prof pietro perona finished phd theoretical high energy physics supervision nobel laureate prof gerard ‘t hooft
[D] Are there any apps that run model inference on the mobile device (Android or iPhone) itself?,1,qd2w6w,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qd2w6w/d_are_there_any_apps_that_run_model_inference_on/,9,"As the title says  I want to know if there are there any apps that run model inference on the mobile device  Android or iPhone  itself 


The devices are becoming more powerful and now with pixel  including what Google is calling an AI chip  I am trying to understand the use cases for running the inference on the mobile device versus running it on the cloud and accessing via a service call ",1634854778.0,2021-10-22 00:19:38,title says want know apps run model inference mobile device android iphone devices becoming powerful pixel including google calling ai chip trying understand use cases running inference mobile device versus running cloud accessing via service call
[P] Introducing WebEnv,1,qd2llj,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qd2llj/p_introducing_webenv/,0,"Announcing version    of the [WebEnv] https  github com Antipurity webenv  environment for joint self supervised and reinforcement learning  It exposes a web browser to ML agents  and focuses on deployment and user experience with a pre trained agent 

Sounds unassuming  but it hides surprising depth  because without a huge robot fleet that humans interact with every day  the Web is the closest environment that we can get to an AGI training environment 

WebEnv is not exactly coming for your job of surfing the Web  but  well  it s complicated  It is intended to eventually create a platform by providing a clear business case for continuously training and scaling up general purpose agents that serve users  allowing culture to develop around turning Web surfing into a well integrated and useful virtual life 

First needs proof that semi useful agents can be trained  though 

I don t have the compute for it  Maybe it s far too early  or maybe it can help push the boundaries of RL in the real  enough  world  but we ll never know unless we try ",1634853873.0,2021-10-22 00:04:33,announcing version [webenv] github com antipurity webenv environment joint self supervised reinforcement learning exposes web browser ml agents focuses deployment user experience pre trained agent sounds unassuming hides surprising depth without huge robot fleet humans interact every day web closest environment get agi training environment webenv exactly coming job surfing web well complicated intended eventually create platform providing clear business case continuously training scaling general purpose agents serve users allowing culture develop around turning web surfing well integrated useful virtual life first needs proof semi useful agents trained though compute maybe far early maybe help push boundaries rl real enough world never know unless try
[R] Discovering and Achieving Goals via World Models,17,qckqv7,MachineLearning,https://arxiv.org/abs/2110.09514,6,nan,1634795056.0,2021-10-21 07:44:16,nan
[D] When are multiple networks superior to a single network?,1,qczy6d,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qczy6d/d_when_are_multiple_networks_superior_to_a_single/,16," 

Hello 

My background is in material science  I am using neural networks to predict various material and physical properties  density  stiffness  toughness  strength  etc  

I first built a single neural network taking  inputs and predicting  outputs  material properties   The performance was quite high for some properties but very poor for others   based on R scores 

I decided to split the network into multiple  specialized networks   taking in the same inputs as the previous model  but only outputting  or  properties at maximum  I was happily surprised to see that the performance of the neural networks was now high for almost all properties 

I understand that mathematically  we are not solving the same problem  but does anyone have some insights on why can t a single model make highly accurate predictions for all properties 

Is it possible but would necessitate more careful tuning of hyperparameters  or are specialized networks always superior 

Thank you  Looking forward to your inputs ",1634846003.0,2021-10-21 21:53:23,hello background material science using neural networks predict various material physical properties density stiffness toughness strength etc first built single neural network taking inputs predicting outputs material properties performance quite high properties poor others based r scores decided split network multiple specialized networks taking inputs previous model outputting properties maximum happily surprised see performance neural networks high almost properties understand mathematically solving problem anyone insights single model make highly accurate predictions properties possible would necessitate careful tuning hyperparameters specialized networks always superior thank looking forward inputs
[D] How would you deploy an optimization type model? ex: CLIP+VQGAN,10,qckgsj,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qckgsj/d_how_would_you_deploy_an_optimization_type_model/,8,Theres a lot of resources on trying to commercially deploy models for inference but what if you need to deploy a model that needs to optimize at every request like CLIP based models if input text is given   is there a way to deploy such a model in a way that scales ,1634793993.0,2021-10-21 07:26:33,theres lot resources trying commercially deploy models inference need deploy model needs optimize every request like clip based models input text given way deploy model way scales
[R] LCS: Learning Compressible Subspaces for Adaptive Network Compression at Inference Time,6,qcm2dr,MachineLearning,https://arxiv.org/abs/2110.04252,2,nan,1634800319.0,2021-10-21 09:11:59,nan
[D] Look for papers on applied deep learning ensemble topics,0,qcvd2b,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qcvd2b/d_look_for_papers_on_applied_deep_learning/,1,"I am looking for papers with either empirical or theoretical evidence demonstrate the following about convolutional neural nets 

  Same CNN trained with different seeds doesn t make the same mistakes on test 
  Adversarial examples are transferable among different CNN architectures 
  Different CNNs architectures trained on same dataset produce uncorrelated errors on test set 

Reddit kudos will be given to good finds ",1634832946.0,2021-10-21 18:15:46,looking papers either empirical theoretical evidence demonstrate following convolutional neural nets cnn trained different seeds make mistakes test adversarial examples transferable among different cnn architectures different cnns architectures trained dataset produce uncorrelated errors test set reddit kudos given good finds
[P] Effects of Metadata filtering with HNSW on Recall and Query time,1,qctl9b,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qctl9b/p_effects_of_metadata_filtering_with_hnsw_on/,2,"A common limitation of deep neural networks is that you cannot filter using symbolic queries  think of a SQL ` WHERE foo=bar ` clause   When using vector search engines to search through deep neural network embeddings  such filters can be added  This article investigates how setting such filters affects both recall and query time when using the HNSW  Hierarchical Navigable Small Worlds  Approximate Nearest Neighbor algorithm 

[Effects of filtered HNSW Searches on Recall and Latency   Towards Data Science] https  towardsdatascience com effects of filtered hnsw searches on recall and latency becfc ",1634827997.0,2021-10-21 16:53:17,common limitation deep neural networks cannot filter using symbolic queries think sql ` foo=bar ` clause using vector search engines search deep neural network embeddings filters added article investigates setting filters affects recall query time using hnsw hierarchical navigable small worlds approximate nearest neighbor algorithm [effects filtered hnsw searches recall latency towards data science] towardsdatascience com effects filtered hnsw searches recall latency becfc
[D] DAE have their Keras tickets closed due to inactivity but no fix?,128,qbz44d,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbz44d/d_dae_have_their_keras_tickets_closed_due_to/,52,"Keras these days 

 Lots of comments discussing bug 

 xB 

 Lots of silence since no attention given to bug   


Also Keras    Closing this issue due to lack of recent activity  Please feel free to reopen if you still have concern Thanks   ",1634730950.0,2021-10-20 13:55:50,keras days lots comments discussing bug xb lots silence since attention given bug also keras closing issue due lack recent activity please feel free reopen still concern thanks
[D] How do you store time series data for RnD of a team project?,1,qcmpax,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qcmpax/d_how_do_you_store_time_series_data_for_rnd_of_a/,8,"Hi 

We are trying to create an ML development loop  ML CI CD if it is the right term    and we have fairly big time series data  Prior to this  for our very initial RnD phase  some of the people in the group worked with the raw data on disk to see how it is behaving  But this is not necessarily scalable for us I think from now on  We want to generate  trainable  datasets from this time series  mix it with our new datasets moving on  and we don t have a lot of storage for all people to work with  storage unit each  all have the identical data  Each person will experiment on a new model and even though we have a queue for them running  we want to make it as concurrent as possible 

Does it even makes sense to offer concurrency on the data i o  And if yes  How do companies work around concurrent data i o needs for huge datasets  and are there any resource for me to read about that  I used to work as a freelancer before and usually did not work at problems at this scale ",1634802919.0,2021-10-21 09:55:19,hi trying create ml development loop ml ci cd right term fairly big time series data prior initial rnd phase people group worked raw data disk see behaving necessarily scalable us think want generate trainable datasets time series mix new datasets moving lot storage people work storage unit identical data person experiment new model even though queue running want make concurrent possible even makes sense offer concurrency data yes companies work around concurrent data needs huge datasets resource read used work freelancer usually work problems scale
[R] NormFormer: Improved Transformer Pretraining with Extra Normalization,18,qc42it,MachineLearning,https://arxiv.org/abs/2110.09456,3,nan,1634745368.0,2021-10-20 17:56:08,nan
[R] An Introduction to Probabilistic Programming,184,qbq5bs,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbq5bs/r_an_introduction_to_probabilistic_programming/,17,"A book length   pages  treatment of probabilistic programming  Stan  Pymc  etc   

Abstract   This book is a graduate level introduction to probabilistic programming  It not only provides a thorough background for anyone wishing to use a probabilistic programming system  but also introduces the techniques needed to design and build these systems  It is aimed at people who have an undergraduate level understanding of either or  ideally  both probabilistic machine learning and programming languages 
We start with a discussion of model based reasoning and explain why conditioning is a foundational computation central to the fields of probabilistic machine learning and artificial intelligence  We then introduce a first order probabilistic programming language  PPL  whose programs correspond to graphical models with a known  finite  set of random variables  In the context of this PPL we introduce fundamental inference algorithms and describe how they can be implemented 
We then turn to higher order probabilistic programming languages  Programs in such languages can define models with dynamic computation graphs  which may not instantiate the same set of random variables in each execution  Inference requires methods that generate samples by repeatedly evaluating the program  Foundational algorithms for this kind of language are discussed in the context of an interface between program executions and an inference controller 
Finally we consider the intersection of probabilistic and differentiable programming  We begin with a discussion of automatic differentiation  and how it can be used to implement efficient inference methods based on Hamiltonian Monte Carlo  We then discuss gradient based maximum likelihood estimation in programs that are parameterized using neural networks  how to amortize inference using by learning neural approximations to the program posterior  and how language features impact the design of deep probabilistic programming systems  

Link  https  arxiv org abs  ",1634694207.0,2021-10-20 03:43:27,book length pages treatment probabilistic programming stan pymc etc abstract book graduate level introduction probabilistic programming provides thorough background anyone wishing use probabilistic programming system also introduces techniques needed design build systems aimed people undergraduate level understanding either ideally probabilistic machine learning programming languages start discussion model based reasoning explain conditioning foundational computation central fields probabilistic machine learning artificial intelligence introduce first order probabilistic programming language ppl whose programs correspond graphical models known finite set random variables context ppl introduce fundamental inference algorithms describe implemented turn higher order probabilistic programming languages programs languages define models dynamic computation graphs may instantiate set random variables execution inference requires methods generate samples repeatedly evaluating program foundational algorithms kind language discussed context interface program executions inference controller finally consider intersection probabilistic differentiable programming begin discussion automatic differentiation used implement efficient inference methods based hamiltonian monte carlo discuss gradient based maximum likelihood estimation programs parameterized using neural networks amortize inference using learning neural approximations program posterior language features impact design deep probabilistic programming systems link arxiv org abs
[D] Swin Transformers: Why are shifted windows better than sliding windows?,13,qc4ph5,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qc4ph5/d_swin_transformers_why_are_shifted_windows/,3,"I understand the purpose behind why the windows are not the same for each layer in order to improve connectivity 

What I do not understand is why shifted windows have lower latency when compared to their sliding window counterparts  The paper  [https  arxiv org pdf   pdf] https  arxiv org pdf   pdf  says the following 

 A key design element of Swin Transformer is its shift of the window partition between consecutive self attention layers  as illustrated in Figure   The shifted windows bridge the windows of the preceding layer  providing connections among them that significantly enhance modeling power  This strategy is also efficient in regards to real world latency  all query patches within a window share the same key set  which facilitates memory access in hardware  In contrast  earlier sliding window based self attention approaches \[  \] suffer from low latency on general hardware due to different key sets for different query pixels  Our experiments show that the proposed shifted window approach has much lower latency than the sliding window method  yet is similar in modeling power  The shifted window approach also proves beneficial for all MLP architectures  

I don t understand he query key relations and how they are related to the different window strategies ",1634747064.0,2021-10-20 18:24:24,understand purpose behind windows layer order improve connectivity understand shifted windows lower latency compared sliding window counterparts paper [ arxiv org pdf pdf] arxiv org pdf pdf says following key design element swin transformer shift window partition consecutive self attention layers illustrated figure shifted windows bridge windows preceding layer providing connections among significantly enhance modeling power strategy also efficient regards real world latency query patches within window share key set facilitates memory access hardware contrast earlier sliding window based self attention approaches \[ \] suffer low latency general hardware due different key sets different query pixels experiments show proposed shifted window approach much lower latency sliding window method yet similar modeling power shifted window approach also proves beneficial mlp architectures understand query key relations related different window strategies
[R] Direct simultaneous speech to speech translation,2,qcga8k,MachineLearning,https://arxiv.org/abs/2110.08250,1,nan,1634779862.0,2021-10-21 03:31:02,nan
[R] CVPR expands social media ban,86,qbslai,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbslai/r_cvpr_expands_social_media_ban/,20,"Statement by the  CVPR Program Chairs regarding the Social Media Policy 
  Per the motion passed in the CVPR PAMI TC meeting  authors should NOT use social media to promote their paper submissions to CVPR during the review period  We are imposing a policy slightly stronger than what passed in the motion  where we define the social media silence period  By our definition  the social media silence period starts four weeks before the paper submission deadline  until the final paper decision notifications are sent to authors  Per the currently planned schedule  the social media silence period is from    to     Any social media promotion of a paper incurred in this period  proactively initiated by the authors  is deemed a policy violation 
Updates to the author guidelines and FAQ s will be posted shortly 

https  twitter com CVPR status  photo ",1634702977.0,2021-10-20 06:09:37,statement cvpr program chairs regarding social media policy per motion passed cvpr pami tc meeting authors use social media promote paper submissions cvpr review period imposing policy slightly stronger passed motion define social media silence period definition social media silence period starts four weeks paper submission deadline final paper decision notifications sent authors per currently planned schedule social media silence period social media promotion paper incurred period proactively initiated authors deemed policy violation updates author guidelines faq posted shortly twitter com cvpr status photo
"Silhouette score vs SSE, which is most important as K means clustering evaluation? ""[P]""",2,qcbfos,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qcbfos/silhouette_score_vs_sse_which_is_most_important/,1,"I m trying to compute the optimal K clusters  The SSE suggests that  clusters would be optimal  however  the Silhouette score is lowest at  clusters 

 xB 

First  did I compute it correctly 

Second  which evaluation metric should be leading 

 xB 

```

for n\_clusters in range    

clusterer = KMeans n\_clusters=n\_clusters 

preds = clusterer fit\_predict data 

centers = clusterer cluster\_centers\_

 xB 

score = silhouette\_score data  preds 

print For n\_clusters = {}  silhouette score is {}  format n\_clusters  score 

```

 xB 

These are the results for Silhouette score 

\  For n\_clusters =   silhouette score is   

\  For n\_clusters =   silhouette score is   

 xB 

```

sse = {}

for k in range     

kmeans = KMeans n\_clusters=k  max\_iter=  fit iris 

\ data\[ clusters \] = kmeans labels\_

\ print data\[ clusters \] 

sse\[k\] = kmeans inertia\_   Inertia  Sum of distances of samples to their closest cluster center

sse

```

These for SSE

\      

\      

\      

\      ",1634765251.0,2021-10-20 23:27:31,trying compute optimal k clusters sse suggests clusters would optimal however silhouette score lowest clusters xb first compute correctly second evaluation metric leading xb ``` n\_clusters range clusterer = kmeans n\_clusters=n\_clusters preds = clusterer fit\_predict data centers = clusterer cluster\_centers\_ xb score = silhouette\_score data preds print n\_clusters = {} silhouette score {} format n\_clusters score ``` xb results silhouette score \ n\_clusters = silhouette score \ n\_clusters = silhouette score xb ``` sse = {} k range kmeans = kmeans n\_clusters=k max\_iter= fit iris \ data\[ clusters \] = kmeans labels\_ \ print data\[ clusters \] sse\[k\] = kmeans inertia\_ inertia sum distances samples closest cluster center sse ``` sse \ \ \ \
[P] see if two statements contradict eachother,0,qc8ysr,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qc8ysr/p_see_if_two_statements_contradict_eachother/,4,I am wondering about current best practice to see if two statements contradict or agree with each other  Is there any implementation with BERT or something else with NLP people can fine tune and use ,1634758466.0,2021-10-20 21:34:26,wondering current best practice see two statements contradict agree implementation bert something else nlp people fine tune use
[D] Boring machine learning is where it's at,5,qbyhlz,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbyhlz/d_boring_machine_learning_is_where_its_at/,17,"*Note  This is a short opinion piece I wrote  which I thought might be relevant to post here given all the  career  talk that s been going on  I d be rather curious to hear if people here think this is a valid perspective  or if I m glancing over some important stuff *

It surprises me that when people think of  software that brings about the singularity  they think of text models  or of RL agents  But they sneer at decision tree boosting and the like as boring algorithms for boring problems 

To me  this seems counter intuitive  and the fact that most people researching ML are interested in subjects like vision and language is flabergasting  For one  because getting anywhere productive in these fields is really hard  for another  because their usefulness seems relatively minimal 

I ve said it before and I ll say it again  human brains are very good at the stuff they ve been doing for a long time  This ranges from things like controlling a human like body to things like writing prose and poetry  Seneca was as good of a philosophy writer as any modern  Shakespear as good of a playwright as any contemporary  That is not to say that new works and diversity in literature isn t useful  both from the perspective of diversity and of updating to language and zeitgeist  but it s not game changing 

Human brains are shit at certain tasks  things like finding the strongest correlation with some variables in an n million times n million valued matrix  Or heck  even finding the most productive categories to quantify a spreadsheet with a few dozen categorical columns and a few thousand rows  That s not to mention things like optimizing d structures under complex constraints or figuring out probabilistic periodicity in a multi dimensional timeseries 

The later sort of problem is where machine learning has found the most amount of practical usage  problems that look  silly  to a researcher but implacable to a human mind  On the contrary   years in  computer vision is still struggling to find any meaningfully large market fits outside of self driving  There are a few interesting applications  but they have limited impact and a low market cap  The most interesting applications  related to bioimaging  happen to be things people are quite bad at  They are very divergent from the objective of creating human like vision capabilities  since the results you want are anything but human like 

Even worst  there s the problem that human like  AI  will be redundant the moment it s implemented  Self driving cars are a real challenge precisely until the point when they become viable enough that everybody uses them  afterwards  every car is running on software and we can replace all the fancy CV based decision making with simple control structures that rely on very constrained and  sane  behaviour from all other cars  Google assistant being able to call a restaurant or hospital and make a booking for you  or act as the receptionist taking that call  is relevant right until everyone starts using it  afterwards everything will already be digitized and we can switch to better and much simpler booking APIs 

That s not to say *all* human like  AI  will be made redundant  but we can say that its applications are mainly well known and will diminish over time  giving way to simpler automation as people start being replaced with algorithms  I say its applications are  well known  because they boil down to  the stuff that humans can do right now which is boring or hard enough that we d like to stop doing it   There s a huge market for this  but it s huge in the same way as the whale oil market was in the th century  It s a market without that much growth potential 

On the other hand  the applications of  inhuman  algorithms are boundless  or at least only bounded by imagination  I ve argued before that science hasn t yet caught up to the last  years of machine learning  People prefer designing equations by hand and validating them with arcane  and easy to fake  misinterpret and misuse  statistics  rather than using algorithmically generate solutions and validating them with simple  rock solid methods such as CV  People like Horvath are hailed as genius level polymaths in molecular biology for calling  scikit learn functions on a tiny dataset 

*Note  Horvath s work is great and I in no way want to pick on him specifically  the world would be much worse without him  I hope epigenetic clocks predict he ll live and work well into old age  I don t think he personally ever claimed the ML side of his work is in any way special or impressive  this is just what I ve heard other biologists say *

This is not to say that the scientific establishment is doomed or anything  it s just slow at using new technologies  especially those that shift the onus of what a researcher ought to be doing  The same goes for industry  A lot of high paying  high status positions involve doing work algorithms are better at  precisely because it s extremely difficult for people  and thus you need the smartest people for it 

However  market forces and common sense are at work  and there s a constant uptick in usage  While I don t believe this can bring about a singularity so to speak  it will accelerate research and will open up new paradigms  mainly around data gathering and storage  and new problems that will allow ML to take centre stage 

So in that sense  it seems obvious to postulate a limited and decreasing market for human like intelligence and a boundless and increasing market for  inhuman  intelligence 

This is mainly why I like to focus my work on the latter  even if it s often less flashy and more boring  One entirely avoidable issue with this is that the bar of doing better than a person is low  and the state of benchmarking is so poor as to make head to head competition between techniques difficult  Though this in itself is the problem I m aiming to help solve 

That s about it  so I say go grab a spreadsheet and figure out how to get the best result on a boring economics problem with a boring algorithm  Don t worry so much about making a painting or movie with GANs  we re already really good at doing that and enjoy doing it ",1634728658.0,2021-10-20 13:17:38,*note short opinion piece wrote thought might relevant post given career talk going rather curious hear people think valid perspective glancing important stuff * surprises people think software brings singularity think text models rl agents sneer decision tree boosting like boring algorithms boring problems seems counter intuitive fact people researching ml interested subjects like vision language flabergasting one getting anywhere productive fields really hard another usefulness seems relatively minimal said say human brains good stuff long time ranges things like controlling human like body things like writing prose poetry seneca good philosophy writer modern shakespear good playwright contemporary say new works diversity literature useful perspective diversity updating language zeitgeist game changing human brains shit certain tasks things like finding strongest correlation variables n million times n million valued matrix heck even finding productive categories quantify spreadsheet dozen categorical columns thousand rows mention things like optimizing structures complex constraints figuring probabilistic periodicity multi dimensional timeseries later sort problem machine learning found amount practical usage problems look silly researcher implacable human mind contrary years computer vision still struggling find meaningfully large market fits outside self driving interesting applications limited impact low market cap interesting applications related bioimaging happen things people quite bad divergent objective creating human like vision capabilities since results want anything human like even worst problem human like ai redundant moment implemented self driving cars real challenge precisely point become viable enough everybody uses afterwards every car running software replace fancy cv based decision making simple control structures rely constrained sane behaviour cars google assistant able call restaurant hospital make booking act receptionist taking call relevant right everyone starts using afterwards everything already digitized switch better much simpler booking apis say *all* human like ai made redundant say applications mainly well known diminish time giving way simpler automation people start replaced algorithms say applications well known boil stuff humans right boring hard enough like stop huge market huge way whale oil market th century market without much growth potential hand applications inhuman algorithms boundless least bounded imagination argued science yet caught last years machine learning people prefer designing equations hand validating arcane easy fake misinterpret misuse statistics rather using algorithmically generate solutions validating simple rock solid methods cv people like horvath hailed genius level polymaths molecular biology calling scikit learn functions tiny dataset *note horvath work great way want pick specifically world would much worse without hope epigenetic clocks predict live work well old age think personally ever claimed ml side work way special impressive heard biologists say * say scientific establishment doomed anything slow using new technologies especially shift onus researcher ought goes industry lot high paying high status positions involve work algorithms better precisely extremely difficult people thus need smartest people however market forces common sense work constant uptick usage believe bring singularity speak accelerate research open new paradigms mainly around data gathering storage new problems allow ml take centre stage sense seems obvious postulate limited decreasing market human like intelligence boundless increasing market inhuman intelligence mainly like focus work latter even often less flashy boring one entirely avoidable issue bar better person low state benchmarking poor make head head competition techniques difficult though problem aiming help solve say go grab spreadsheet figure get best result boring economics problem boring algorithm worry much making painting movie gans already really good enjoy
[P] Collection of Kaggle Past Solutions (to learn ideas and techniques),322,qb8q56,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qb8q56/p_collection_of_kaggle_past_solutions_to_learn/,10," xB 

https  preview redd it uwkxwwdu jpg width= format=pjpg auto=webp s=dbabcbfbeacababcff

I have collected here \[ \] almost all available solutions and ideas with codes shared by top performers in the past Kaggle competitions  This list will gets updated as soon as a new competition finishes  It allows you to search over the Kaggle past competitions solutions and ideas  Please share it with your friends 

\[\] [https  github com faridrashidi kaggle solutions] https  github com faridrashidi kaggle solutions 

\[\] [https  farid one kaggle solutions ] https  farid one kaggle solutions ",1634639653.0,2021-10-19 12:34:13,xb preview redd uwkxwwdu jpg width= format=pjpg auto=webp s=dbabcbfbeacababcff collected \[ \] almost available solutions ideas codes shared top performers past kaggle competitions list gets updated soon new competition finishes allows search kaggle past competitions solutions ideas please share friends \[\] [ github com faridrashidi kaggle solutions] github com faridrashidi kaggle solutions \[\] [ farid one kaggle solutions ] farid one kaggle solutions
[D] Thoughts on decentralized deep learning,8,qbufcu,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbufcu/d_thoughts_on_decentralized_deep_learning/,4,I recently got interested in decentralized training of deep learning models and wanted to know if anyone else thinks this is a pretty interesting topic  Also what are your thoughts on hivemind  [https  github com learning at home hivemind] https  github com learning at home hivemind  ,1634710681.0,2021-10-20 08:18:01,recently got interested decentralized training deep learning models wanted know anyone else thinks pretty interesting topic also thoughts hivemind [ github com learning home hivemind] github com learning home hivemind
[R] World Model Learning and Inference,17,qbqnid,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbqnid/r_world_model_learning_and_inference/,0,"New paper published in Neural Networks  Dec  Edition 

Open access  https  www sciencedirect com science article pii S

Authors  Karl Friston  Rosalyn J  Moran  Yukie Nagai  Tadahiro Taniguchi  Hiroaki Gomi  Josh Tenenbaum

**Abstract**

Understanding information processing in the brain—and creating general purpose artificial intelligence—are long standing aspirations of scientists and engineers worldwide  The distinctive features of human intelligence are high level cognition and control in various interactions with the world including the self  which are not defined in advance and are vary over time  The challenge of building human like intelligent machines  as well as progress in brain science and behavioural analyses  robotics  and their associated theoretical formalisations  speaks to the importance of the world model learning and inference  In this article  after briefly surveying the history and challenges of internal model learning and probabilistic learning  we introduce the free energy principle  which provides a useful framework within which to consider neuronal computation and probabilistic world models  Next  we showcase examples of human behaviour and cognition explained under that principle  We then describe symbol emergence in the context of probabilistic modelling  as a topic at the frontiers of cognitive robotics  Lastly  we review recent progress in creating human like intelligence by using novel probabilistic programming languages  The striking consensus that emerges from these studies is that probabilistic descriptions of learning and inference are powerful and effective ways to create human like artificial intelligent machines and to understand intelligence in the context of how humans interact with their world ",1634695896.0,2021-10-20 04:11:36,new paper published neural networks dec edition open access www sciencedirect com science article pii authors karl friston rosalyn j moran yukie nagai tadahiro taniguchi hiroaki gomi josh tenenbaum **abstract** understanding information processing brain—and creating general purpose artificial intelligence—are long standing aspirations scientists engineers worldwide distinctive features human intelligence high level cognition control various interactions world including self defined advance vary time challenge building human like intelligent machines well progress brain science behavioural analyses robotics associated theoretical formalisations speaks importance world model learning inference article briefly surveying history challenges internal model learning probabilistic learning introduce free energy principle provides useful framework within consider neuronal computation probabilistic world models next showcase examples human behaviour cognition explained principle describe symbol emergence context probabilistic modelling topic frontiers cognitive robotics lastly review recent progress creating human like intelligence using novel probabilistic programming languages striking consensus emerges studies probabilistic descriptions learning inference powerful effective ways create human like artificial intelligent machines understand intelligence context humans interact world
[D] Offering free ML learning advice!,114,qbdjuc,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbdjuc/d_offering_free_ml_learning_advice/,112,"Hi 

I m an ML scientist who worked in an industry position for multiple years  I had no ML background when I joined the industry and am self taught  I also eventually taught courses on ML at a university 

I wanted to help folks with any questions they might have about learning ML or upskilling themselves  Feel free to post questions below  and I d be happy to answer anything I can ",1634656335.0,2021-10-19 17:12:15,hi ml scientist worked industry position multiple years ml background joined industry self taught also eventually taught courses ml university wanted help folks questions might learning ml upskilling feel free post questions happy answer anything
[D] What is your ML experiment workflow? Discussion on training a model to the Latex tables,0,qc2apm,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qc2apm/d_what_is_your_ml_experiment_workflow_discussion/,0,"I hope this is a discussion for everyone working on ML  I have been looking for ways to tune my experimental setup so I could easily train models  but at the same time export results faster  

My current experimental setup is running experiments on a machine with a GPU  then collecting results to Weights and Biases  [https  wandb ai ] https  wandb ai   then to Latex tables  I m trying to automate as much as possible since collecting results takes time and to make things easier  

What are your experimental workflows ",1634740591.0,2021-10-20 16:36:31,hope discussion everyone working ml looking ways tune experimental setup could easily train models time export results faster current experimental setup running experiments machine gpu collecting results weights biases [ wandb ai ] wandb ai latex tables trying automate much possible since collecting results takes time make things easier experimental workflows
[D] How make to search for research contribution,1,qc19x7,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qc19x7/d_how_make_to_search_for_research_contribution/,15,"I am in my th year of my doctorate  currently have no luck in publishing my paper event I dont make it to submission  My supv always mentioning that my research is not provide strong contribution for the venue I am targeting  I have many drafts on hold because of this  not be able to submit to any machine learning venue 

My question is what makes strong contribution to a paper  I see many papers are published because they provide a method that outmatch the SOTA ",1634737655.0,2021-10-20 15:47:35,th year doctorate currently luck publishing paper event dont make submission supv always mentioning research provide strong contribution venue targeting many drafts hold able submit machine learning venue question makes strong contribution paper see many papers published provide method outmatch sota
[R] FlexConv: Continuous Kernel Convolutions with Differentiable Kernel Sizes,52,qbdm52,MachineLearning,https://arxiv.org/abs/2110.08059,8,nan,1634656532.0,2021-10-19 17:15:32,nan
[D] Open-set classification vs. few-shot learning,1,qbz5xq,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbz5xq/d_openset_classification_vs_fewshot_learning/,3,Is is true to say that few single shot learning falls under open set classification  As different literatures gives confusion answers about it  Some say it falls under few shot open shot  some say its completely different from open set  I want to know your opinion about this ,1634731123.0,2021-10-20 13:58:43,true say single shot learning falls open set classification different literatures gives confusion answers say falls shot open shot say completely different open set want know opinion
[P] Apply KITT4SME Open Call for a grant up to €100k,0,qc3hne,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qc3hne/p_apply_kitt4sme_open_call_for_a_grant_up_to_100k/,1,"Hi everyone 

My name is Zeki  from SUPSI  the coordinator partner of a H funded project  KITTSME  Our project aims to deliver a platform which offers affordable  tested and validated AI based solutions for the manufacturing industry  Currently we are looking for AI developer SMEs to integrate their solutions to our platform via our open calls which provides an EU funding of €k 

If you are 

* an SME established in the [H eligible countries] https  ec europa eu info research and innovation statistics framework programme facts and figures horizon  country profiles_en   and
* having an AI based solution for the use of manufacturing sector 

you can apply to our open calls to receive an EU funding of **€k** 

We are organizing an info webinar on Monday   Oct at   CEST  to present the details of the open call and answering your questions  You can register yourself [here] https  forms gle KzAAzudiPTJMzM  and find the answers to your questions at the event 

Until that time  you can find more info on [kittsme eu open call ] https  kittsme eu open call  and you can always reach me via here or via the contact form on the website 

Sorry if I created the post in a wrong place  but any suggestions or comments would be more than welcome 

[Open call at a glance] https  preview redd it lvnsxuimu png width= format=png auto=webp s=deaabccbddeceece 

[Info webinar details] https  preview redd it vrvofyimu png width= format=png auto=webp s=dafaeccbaeecaada 

Hope to hearing from you 

Zeki",1634743793.0,2021-10-20 17:29:53,hi everyone name zeki supsi coordinator partner h funded project kittsme project aims deliver platform offers affordable tested validated ai based solutions manufacturing industry currently looking ai developer smes integrate solutions platform via open calls provides eu funding €k * sme established [h eligible countries] ec europa eu info research innovation statistics framework programme facts figures horizon country profiles_en * ai based solution use manufacturing sector apply open calls receive eu funding **€k** organizing info webinar monday oct cest present details open call answering questions register [here] forms gle kzaazudiptjmzm find answers questions event time find info [kittsme eu open call ] kittsme eu open call always reach via via contact form website sorry created post wrong place suggestions comments would welcome [open call glance] preview redd lvnsxuimu png width= format=png auto=webp s=deaabccbddeceece [info webinar details] preview redd vrvofyimu png width= format=png auto=webp s=dafaeccbaeecaada hope hearing zeki
[R] Phase transitions in when feedback is useful,4,qbql7p,MachineLearning,https://arxiv.org/abs/2110.07873,1,nan,1634695679.0,2021-10-20 04:07:59,nan
[R] Learning in High Dimension Always Amounts to Extrapolation,39,qbbknr,MachineLearning,https://arxiv.org/abs/2110.09485,20,nan,1634650228.0,2021-10-19 15:30:28,nan
[D] iForest with single feature,0,qbvpnz,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbvpnz/d_iforest_with_single_feature/,1,"Is it possible to train an isolation forest with a single feature  And what would that look like 

I remember with SVMs  it was better to have less features if you can not provide enough training data  And SVMs can work with only one  Is that the same with iForests 

 xB 

Thanks for every answer in advance ",1634716554.0,2021-10-20 09:55:54,possible train isolation forest single feature would look like remember svms better less features provide enough training data svms work one iforests xb thanks every answer advance
[P] Mapping an image to a 3D face model (iPhone AR compatible),20,qbcfu6,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbcfu6/p_mapping_an_image_to_a_3d_face_model_iphone_ar/,4,"I ve been interested in D models for AR recently  I set out to create a script to generate D head models from selfies which led me down the rabbit hole of creating a  free  service for this 


I retrained [DECA] https  github com YadiraF DECA  using the original dataset and additional data that I generated and then combined this with a library I wrote to convert  obj files to  usdz  a format used on iPhone to describe D models that are compatible with its AR viewer 


Here is the service  [https  facemodel me] https  facemodel me 


I m in the process of tidying up the GitHub repo and will make it public if there is interest  I m posting here to get feedback on the generated D models and to discuss whether there are other use cases for downstream ML applications that people here would find useful ",1634653016.0,2021-10-19 16:16:56,interested models ar recently set create script generate head models selfies led rabbit hole creating free service retrained [deca] github com yadiraf deca using original dataset additional data generated combined library wrote convert obj files usdz format used iphone describe models compatible ar viewer service [ facemodel me] facemodel process tidying github repo make public interest posting get feedback generated models discuss whether use cases downstream ml applications people would find useful
[R] StyleNeRF: A 3D-Aware Generator for High-Resolution Image Synthesis with Explicit Style Control,9,qbdj8n,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbdj8n/r_stylenerf_a_3daware_generator_for/,0,"In a paper currently under double blind review for ICLR   researchers propose StyleNeRF  a D aware generative model that can synthesize high resolution images at interactive rates while preserving high quality D consistency  and can even generalize to unseen views with control on styles and poses  

Here is a quick read  StyleNeRF  [A D Aware Generator for High Resolution Image Synthesis with Explicit Style Control ] https  syncedreview com    deepmind podracer tpu based rl frameworks deliver exceptional performance at low cost  

The paper *StyleNeRF  A Style based D Aware Generator for High resolution Image Synthesis* is on [OpenReview] https  openreview net forum id=iUuzzTMUwK  ",1634656283.0,2021-10-19 17:11:23,paper currently double blind review iclr researchers propose stylenerf aware generative model synthesize high resolution images interactive rates preserving high quality consistency even generalize unseen views control styles poses quick read stylenerf [a aware generator high resolution image synthesis explicit style control ] syncedreview com deepmind podracer tpu based rl frameworks deliver exceptional performance low cost paper *stylenerf style based aware generator high resolution image synthesis* [openreview] openreview net forum id=iuuzztmuwk
"[N] DeepMind acquires MuJoCo, makes it freely available",550,qaouds,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qaouds/n_deepmind_acquires_mujoco_makes_it_freely/,36,See the [blog post] https  deepmind com blog announcements mujoco   Awesome news ,1634570505.0,2021-10-18 17:21:45,see [blog post] deepmind com blog announcements mujoco awesome news
[P] the copent package v0.2.1 now on PyPI,1,qbpl8y,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbpl8y/p_the_copent_package_v021_now_on_pypi/,1,"The nonparametric methods for estimating copula entropy and transfer entropy are implemented for statistical  conditional  independence testing in the package 

Copula Entropy is a mathematical concept for multivariate statistical independence measuring and testing  and proved to be equivalent to mutual information  Estimating copula entropy can be applied to many cases  including but not limited to variable selection and causal discovery  by estimating transfer entropy   Please refer to Ma and Sun     [doi   S    ] https  doi org   FS     \  and Ma     [arXiv  ] https  arxiv org abs   \  for more information 

In this version  a bug which may cause log error is fixed 

PyPI  [https  pypi org project copent ] http  pypi org project copent 

GitHub   [https  github com majianthu pycopent ] https  github com majianthu pycopent 

Any comments are welcome ",1634692246.0,2021-10-20 03:10:46,nonparametric methods estimating copula entropy transfer entropy implemented statistical conditional independence testing package copula entropy mathematical concept multivariate statistical independence measuring testing proved equivalent mutual information estimating copula entropy applied many cases including limited variable selection causal discovery estimating transfer entropy please refer sun [doi ] doi org fs \ [arxiv ] arxiv org abs \ information version bug may cause log error fixed pypi [ pypi org project copent ] http pypi org project copent github [ github com majianthu pycopent ] github com majianthu pycopent comments welcome
[R][P] OpenFL: An open-source framework for Federated Learning,2,qbetp0,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbetp0/rp_openfl_an_opensource_framework_for_federated/,1,"Federated learning  FL  is a computational paradigm that enables organisations to collaborate on machine learning  ML  projects without sharing sensitive data  such as  patient records  financial data  or classified secrets  Open Federated Learning  OpenFL  is an open source framework for training ML algorithms using the data private collaborative learning paradigm of FL  OpenFL works with training pipelines built with both TensorFlow and PyTorch  and can be easily extended to other ML and deep learning frameworks  Here  we summarise the motivation and development characteristics of OpenFL  with the intention of facilitating its application to existing ML model training in a production environment  Finally  we describe the first use of the OpenFL framework to train consensus ML models in a consortium of international healthcare organisations  as well as how it facilitates the first computational competition on FL 

The main goal of OpenFL framework is provide an easy enough tool to deal with FL training jobs  To start working with OpenFL  you could use *pip*  *Docker*  or *source code* installation 

PyPI 

    pip install openfl

Docker 

    docker pull intel openfl

From source 

    git clone https  github com intel openfl git
    cd openfl
    pip install  e  

Some useful links can be found below 

Paper  [https  arxiv org abs  ] https  arxiv org abs   

Code  [https  github com intel openfl] https  github com intel openfl 

Docs  [https  openfl readthedocs io en latest ] https  openfl readthedocs io en latest 

Slack  [https  join slack com t openfl shared\_invite zt ovzbohvn TfApk\~YS\_iZhjJyaTw] https  join slack com t openfl shared_invite zt ovzbohvn TfApk~YS_iZhjJyaTw ",1634660035.0,2021-10-19 18:13:55,federated learning fl computational paradigm enables organisations collaborate machine learning ml projects without sharing sensitive data patient records financial data classified secrets open federated learning openfl open source framework training ml algorithms using data private collaborative learning paradigm fl openfl works training pipelines built tensorflow pytorch easily extended ml deep learning frameworks summarise motivation development characteristics openfl intention facilitating application existing ml model training production environment finally describe first use openfl framework train consensus ml models consortium international healthcare organisations well facilitates first computational competition fl main goal openfl framework provide easy enough tool deal fl training jobs start working openfl could use *pip* *docker* *source code* installation pypi pip install openfl docker docker pull intel openfl source git clone github com intel openfl git cd openfl pip install e useful links found paper [ arxiv org abs ] arxiv org abs code [ github com intel openfl] github com intel openfl docs [ openfl readthedocs io en latest ] openfl readthedocs io en latest slack [ join slack com openfl shared\_invite zt ovzbohvn tfapk\~ys\_izhjjyatw] join slack com openfl shared_invite zt ovzbohvn tfapk~ys_izhjjyatw
[D] LaMa Paper explained - Resolution-robust Large Mask Inpainting with Fourier Convolutions (5-minute summary by Casual GAN Papers),2,qbe17v,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbe17v/d_lama_paper_explained_resolutionrobust_large/,1,"Ever   tried to take a scenic picture just to be photobombed by some random  tourists  Don’t worry  Roman Suvorov and the team at SAIC Moscow  recently unveiled a model called LaMa  large mask inpainting  that takes  care of it for you  The model excels at inpainting large irregular  masks using fast Fourier convolutions that have a receptive field equal  to the entire image and a specialized wide receptive field perceptual  loss that boosts the consistency for distant regions of an image  A    surprising yet extremely useful outcome of the paper is that the  pretrained model scales up to k resolutions quite trivially 

Fresh out of the oven  Full summary  [https  www casualganpapers com large masks fourier convolutions inpainting LaMa explained html] https  www casualganpapers com large masks fourier convolutions inpainting LaMa explained html 

[LaMa] https  i redd it mfcffu gif 

arxiv  [https  arxiv org pdf   pdf] https  arxiv org pdf   pdf   
code  [https  github com saic mdal lama] https  github com saic mdal lama 

Subscribe to [Casual GAN Papers] https  t me casual_gan  and follow me on [Twitter] https  twitter com KirillDemochkin  for weekly AI paper summaries ",1634657735.0,2021-10-19 17:35:35,ever tried take scenic picture photobombed random tourists don’t worry roman suvorov team saic moscow recently unveiled model called lama large mask inpainting takes care model excels inpainting large irregular masks using fast fourier convolutions receptive field equal entire image specialized wide receptive field perceptual loss boosts consistency distant regions image surprising yet extremely useful outcome paper pretrained model scales k resolutions quite trivially fresh oven full summary [ www casualganpapers com large masks fourier convolutions inpainting lama explained html] www casualganpapers com large masks fourier convolutions inpainting lama explained html [lama] redd mfcffu gif arxiv [ arxiv org pdf pdf] arxiv org pdf pdf code [ github com saic mdal lama] github com saic mdal lama subscribe [casual gan papers] casual_gan follow [twitter] twitter com kirilldemochkin weekly ai paper summaries
[D] One Class SVM with dirty trainingset,3,qba50e,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qba50e/d_one_class_svm_with_dirty_trainingset/,5,"How or why can you train an OC SVM with outliers in the trainingset 

For exmaple in [scikit] https  scikit learn org stable modules generated sklearn svm OneClassSVM html  you can specify  nu  for an upper bound on the fraction of training errors  default   Errors    I thought you need to provide clean traningdata for a clean boundary   How are the initial outliers dealt with 

Thank you ",1634645311.0,2021-10-19 14:08:31,train oc svm outliers trainingset exmaple [scikit] scikit learn org stable modules generated sklearn svm oneclasssvm html specify nu upper bound fraction training errors default errors thought need provide clean traningdata clean boundary initial outliers dealt thank
"[R] Facebook AI Introduce ‘SaLinA’: A Lightweight Library To Implement Sequential Decision Models, Including Reinforcement Learning Algorithms (Paper, Github link included)",18,qb137c,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qb137c/r_facebook_ai_introduce_salina_a_lightweight/,1,"Deep Learning libraries are great for facilitating the implementation of complex differentiable functions  These functions typically have shapes like f x  → y  where x is a set of input tensors  and y is output tensors produced by executing multiple computations over those inputs  In order to implement a new f function and create a new prototype  one will need to assemble various blocks  or modules  through composition operators  Despite of the easy process  this approach cannot handle the implementation of sequential decision methods  Classical platforms are well suited for managing the acquisition  processing  and transformation of information in an efficient way 

When it comes to reinforcement learning  RL   these all implementations get critical  A classical deep learning framework is not enough to capture the interaction of an agent with their environment  Still  extra code can be written that does not integrate well into these platforms  It has been considered to use multiple reinforcement learning  RL  frameworks for these tasks  but they still have two drawbacks 

* New abstractions are being created all the time in order to model more complex systems  However  these new ideas often have a high adoption cost and low flexibility  making them difficult for laypersons who may not be familiar with reinforcement learning techniques 
* The use cases for RL are as vast and varied as the problems it solves  For that reason  there is no one size fits all library available on these platforms because each platform has been designed to solve a specific type of problem with their unique features from model based algorithms through batch processing or multiagent playback strategies  among other things – but they can’t do everything 

As a solution to the above two problems  Facebook researchers introduce [‘SaLinA’] https  arxiv org pdf   pdf   SaLina works towards making the implementation of sequential decision processes  including reinforcement learning related  natural and simple for practitioners with a basic understanding of how neural networks can be implemented  SaLina proposes to solve any sequential decision problem by using simple ‘agents’ that process information sequentially  The targeted audience are not only RL researchers or computer vision researchers  but also NLP experts looking for a natural way of modelling conversations in their models  making them more intuitive and easy to understand than previous methods 

  [Quick  Min Read] https  www marktechpost com    facebook ai introduce salina a lightweight library to implement sequential decision models including reinforcement learning algorithms    [Paper] https  arxiv org pdf   pdf  [Github] https  github com facebookresearch salina    [Twitter Thread] https  twitter com LudovicDenoyer status  s= 

 xB 

 xB 

https  preview redd it naiwwsabu jpg width= format=pjpg auto=webp s=cabfbdb",1634607660.0,2021-10-19 03:41:00,deep learning libraries great facilitating implementation complex differentiable functions functions typically shapes like f x → x set input tensors output tensors produced executing multiple computations inputs order implement new f function create new prototype one need assemble various blocks modules composition operators despite easy process approach cannot handle implementation sequential decision methods classical platforms well suited managing acquisition processing transformation information efficient way comes reinforcement learning rl implementations get critical classical deep learning framework enough capture interaction agent environment still extra code written integrate well platforms considered use multiple reinforcement learning rl frameworks tasks still two drawbacks * new abstractions created time order model complex systems however new ideas often high adoption cost low flexibility making difficult laypersons may familiar reinforcement learning techniques * use cases rl vast varied problems solves reason one size fits library available platforms platform designed solve specific type problem unique features model based algorithms batch processing multiagent playback strategies among things – can’t everything solution two problems facebook researchers introduce [‘salina’] arxiv org pdf pdf salina works towards making implementation sequential decision processes including reinforcement learning related natural simple practitioners basic understanding neural networks implemented salina proposes solve sequential decision problem using simple ‘agents’ process information sequentially targeted audience rl researchers computer vision researchers also nlp experts looking natural way modelling conversations models making intuitive easy understand previous methods [quick min read] www marktechpost com facebook ai introduce salina lightweight library implement sequential decision models including reinforcement learning algorithms [paper] arxiv org pdf pdf [github] github com facebookresearch salina [twitter thread] twitter com ludovicdenoyer status s= xb xb preview redd naiwwsabu jpg width= format=pjpg auto=webp s=cabfbdb
[R] A New Efficient Transformer: PoNet: Pooling Network for Efficient Token Mixing in Long Sequences,0,qbokzp,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbokzp/r_a_new_efficient_transformer_ponet_pooling/,3,Paper on arxiv [https  arxiv org abs  ] https  arxiv org abs   ,1634688740.0,2021-10-20 02:12:20,paper arxiv [ arxiv org abs ] arxiv org abs
[D] Pretraining/transfer learning with SageMaker BlazingText (word2vec)?,0,qbeyas,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbeyas/d_pretrainingtransfer_learning_with_sagemaker/,2,"I have a training set consisting of a description and a binary label  From reading previous work  I know that using pretrained [fasttext embeddings] https  fasttext cc docs en english vectors html  should work well for my use case  I need to be able to make predictions on unseen words  OOV   My company is already using aws sagemaker  so using [SageMaker Blazing text with subword embedding] https  github com aws amazon sagemaker examples blob master introduction_to_amazon_algorithms blazingtext_wordvec_subwords_text blazingtext_wordvec_subwords_text ipynb  seems like a good approach 

However  they are providing their own training data in the example   does this means it tries to learn word embeddings from scratch from only this training data  I was expecting to be able to pass pretraining as a parameter  and then fine tune it with my own data so those words get added to the known dictionary 

But the way it looks to me now  is that I either use a lookup against a hardcoded list of pretrained embeddings  like GloVe or one of the word vector datasets from fasttext   which means I m not using my own data and also have to come up with a solution to handling OOV  Or I use Blazing text which can handle OOV  but then I don t take advantage of any pretrained model 

So my main question is this  can I use Blazing text to get pretrained OOV embeddings  And if so how 

Would be great if I could also understand how transfer learning would work in this case  so I can make use of my own classification data  However  the embeddings will be used in a downstream classification task  so I guess I could say the fine tuning happens there 

* Use pretrined blazing text model to get embeddings for both seen and unseen words  through subword embeddings 
* Combine these embeddings with other features
* Fit a model around the embeddings   features to get the classification

Appreciate any help ",1634660396.0,2021-10-19 18:19:56,training set consisting description binary label reading previous work know using pretrained [fasttext embeddings] fasttext cc docs en english vectors html work well use case need able make predictions unseen words oov company already using aws sagemaker using [sagemaker blazing text subword embedding] github com aws amazon sagemaker examples blob master introduction_to_amazon_algorithms blazingtext_wordvec_subwords_text blazingtext_wordvec_subwords_text ipynb seems like good approach however providing training data example means tries learn word embeddings scratch training data expecting able pass pretraining parameter fine tune data words get added known dictionary way looks either use lookup hardcoded list pretrained embeddings like glove one word vector datasets fasttext means using data also come solution handling oov use blazing text handle oov take advantage pretrained model main question use blazing text get pretrained oov embeddings would great could also understand transfer learning would work case make use classification data however embeddings used downstream classification task guess could say fine tuning happens * use pretrined blazing text model get embeddings seen unseen words subword embeddings * combine embeddings features * fit model around embeddings features get classification appreciate help
[D] Recommender systems,0,qbdxh7,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbdxh7/d_recommender_systems/,1,I would like to To integrate personalized recommendation systems in the website related to travel  I have basic knowledge in the domain  I would like to ask if I should create a model from scratch or are there any API that makes things work smarter ,1634657445.0,2021-10-19 17:30:45,would like integrate personalized recommendation systems website related travel basic knowledge domain would like ask create model scratch api makes things work smarter
[D] Impact of Memory and Core Count for GPU vs Neural Engine on M1 Max,14,qazxuk,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qazxuk/d_impact_of_memory_and_core_count_for_gpu_vs/,10,"Does anyone know M uses its GPU core for training and neural engine for inferencing  or it utilizes both for inferencing and training 

 xB 

Also would it be able to train large model since m max now supports up to gb memory ",1634603636.0,2021-10-19 02:33:56,anyone know uses gpu core training neural engine inferencing utilizes inferencing training xb also would able train large model since max supports gb memory
[P] `torch_cka` - Compare PyTorch models and gain insights,1,qbbrzo,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbbrzo/p_torch_cka_compare_pytorch_models_and_gain/,0,"GitHub Link   [torch\_cka] https  github com AntixK PyTorch Model Compare 

Comparing two neural networks can be a daunting task  Just comparing their performance isn’t always the best way  By comparing their internal representations   learned features  a lot more insight can be obtained  

I wrote a quick library to compare the features of PyTorch models by their similarity  This is done through the metric   Centered Kernel Alignment  CKA   CKA is based on HSIC  Hilber Schmidt Independence Criterion  for detecting non linear dependencies with scalability  [Reference ] https  arxiv org abs   

Compared to other similarity metrics  CKA is scalable as you can use a minibatched version over the entire dataset   I could even run on the whole ImageNet testset   The size of networks that can be compared depends on your memory but you can always select specific features of your interest 

CKA can compare quite different architectures like CNNs and ViTs  This can also be used for ablation studies where the internal representations are studied rather than just final performance  which comes with its own statistical significance issues   

Check out the ReadMe for some more examples  Hope it is useful to your work ",1634650923.0,2021-10-19 15:42:03,github link [torch\_cka] github com antixk pytorch model compare comparing two neural networks daunting task comparing performance isn’t always best way comparing internal representations learned features lot insight obtained wrote quick library compare features pytorch models similarity done metric centered kernel alignment cka cka based hsic hilber schmidt independence criterion detecting non linear dependencies scalability [reference ] arxiv org abs compared similarity metrics cka scalable use minibatched version entire dataset could even run whole imagenet testset size networks compared depends memory always select specific features interest cka compare quite different architectures like cnns vits also used ablation studies internal representations studied rather final performance comes statistical significance issues check readme examples hope useful work
[D] PhD Student Internship Big Tech Zurich - Low Pay,5,qaxyuh,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qaxyuh/d_phd_student_internship_big_tech_zurich_low_pay/,21,"Hi everyone 

I will soon start an internship in Zurich within one of the GAFAM  

The recruiter sent me my contract few weeks ago and I discovered that my yearly salary would be k chf   chf monthly stipend   a one time chf bonus   

From what I checked this is quite a low salary to live comfortably in Zurich  And so I asked him if this was really the right figures and if there could not be any mistake  I also told him about the salary I had during another internship in one of the other GAFAM  But he only replied to me that he did not have much control on those numbers and that he was asking his superiors about those numbers  He also told me that it was the right figures  

So my questions are the two following 
  Do you think this is a decent salary for an internship as a PhD student in a big tech company in Zurich  
  If not what do you think I should do  Maybe ask my manager about it 
  What is a good internship salary for Zurich 

Thanks for your help  😊",1634597231.0,2021-10-19 00:47:11,hi everyone soon start internship zurich within one gafam recruiter sent contract weeks ago discovered yearly salary would k chf chf monthly stipend one time chf bonus checked quite low salary live comfortably zurich asked really right figures could mistake also told salary another internship one gafam replied much control numbers asking superiors numbers also told right figures questions two following think decent salary internship phd student big tech company zurich think maybe ask manager good internship salary zurich thanks help 😊
[R] Graph Neural Networks with Learnable Structural and Positional Representations,63,qai1i7,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qai1i7/r_graph_neural_networks_with_learnable_structural/,8,"Hi all 

Presenting a general framework for Graph Neural Networks to learn positional encodings  PE  alongside structural representations  applicable to any MP GNNs  including  Graph  Transformers   


 **Graph Neural Networks with Learnable Structural and Positional Representations**   
Vijay Prakash Dwivedi  Anh Tuan Luu  Thomas Laurent  Yoshua Bengio and Xavier Bresson   


Paper  [https  arxiv org abs  ] https  arxiv org pdf   pdf   
Code  [https  github com vijaydwivedi gnn lspe] https  github com vijaydwivedi gnn lspe    


** minutebrief**  
Nodes in a graph do not have canonical positional information  like the global word positions in a sentence  This gives rise to limitations such as the lack of  global  structural information when message passing GNNs are applied to learn on graphs  As a result  such models cannot distinguish isomorphic nodes or other graph symmetries 

In this work  we consider this problem of graph PEs and propose a framework named LSPE that can be used with any MP GNNs to learn positional and structural feature representations at the same time  thus effectively capturing the two essential properties and tuning these w r t  to the task at hand   


[Fig  The general MPGNNs LSPE architecture ] https  preview redd it bczu png width= format=png auto=webp s=fabebccafddabe 

In brief  LSPE enhances capabilities of an MP GNN in the following way   
  At the input layer  PEs are initialized with k dimensional Random Walk that encodes the landing probabilities of a node to itself in  to k steps  This leads to unique node representations  at the input itself  for nodes which have unique k hop neighborhoods in the graph   
  At the GNN layers  both the structural and positional representations are updated with separate learnable parameters but following the same analytical update function of a GNN instance chosen   
  At the final layer  the learned structural and positional representations are fused to output the resultant node features which is then used for the learning task being dealt with  In addition  a positional loss is used to tune the final layer positional features   


Above simple steps improves several MP GNNs and Transformer GNNs providing a performance boost of up to   on molecular datasets  At the same time  we retain the efficient linear complexity of message passing while generating more expressive node embedding   


More background and details in the paper ",1634543447.0,2021-10-18 09:50:47,hi presenting general framework graph neural networks learn positional encodings pe alongside structural representations applicable mp gnns including graph transformers **graph neural networks learnable structural positional representations** vijay prakash dwivedi anh tuan luu thomas laurent yoshua bengio xavier bresson paper [ arxiv org abs ] arxiv org pdf pdf code [ github com vijaydwivedi gnn lspe] github com vijaydwivedi gnn lspe ** minutebrief** nodes graph canonical positional information like global word positions sentence gives rise limitations lack global structural information message passing gnns applied learn graphs result models cannot distinguish isomorphic nodes graph symmetries work consider problem graph pes propose framework named lspe used mp gnns learn positional structural feature representations time thus effectively capturing two essential properties tuning w r task hand [fig general mpgnns lspe architecture ] preview redd bczu png width= format=png auto=webp s=fabebccafddabe brief lspe enhances capabilities mp gnn following way input layer pes initialized k dimensional random walk encodes landing probabilities node k steps leads unique node representations input nodes unique k hop neighborhoods graph gnn layers structural positional representations updated separate learnable parameters following analytical update function gnn instance chosen final layer learned structural positional representations fused output resultant node features used learning task dealt addition positional loss used tune final layer positional features simple steps improves several mp gnns transformer gnns providing performance boost molecular datasets time retain efficient linear complexity message passing generating expressive node embedding background details paper
"[R] BigScience's first paper, T0: Multitask Prompted Training Enables Zero-Shot Task Generalization",24,qangm5,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qangm5/r_bigsciences_first_paper_t0_multitask_prompted/,2,"The first modeling paper out of BigScience  [https  bigscience huggingface co ] https  bigscience huggingface co  is here   

T shows zero shot task generalization on English natural language prompts  outperforming GPT  on many tasks while being x smaller  

 xB 

[Zero shot example] https  preview redd it imymsluu png width= format=png auto=webp s=dbdaeefcdef 

A very big collection of prompts  \~  prompts for   datasets  was released  [https  github com bigscience workshop promptsource] https  github com bigscience workshop promptsource  along with the model and the paper 

This was an international collaborative effort  with over  people across more than  organizations  

The group included dedicated researchers and engineers from different universities  companies  and think tanks   

Model  [https  huggingface co bigscience Tpp] https  huggingface co bigscience Tpp  

Repo  [https  github com bigscience workshop promptsource] https  github com bigscience workshop promptsource  

Paper  [https  arxiv org abs  ] https  arxiv org abs   

 xB 

Additionally  the T models were released in the Hugging Face Model Hub and you can try it out in your browser here  [https  huggingface co bigscience Tpp] https  t co QvEaqkfmgk amp= ",1634566252.0,2021-10-18 16:10:52,first modeling paper bigscience [ bigscience huggingface co ] bigscience huggingface co shows zero shot task generalization english natural language prompts outperforming gpt many tasks x smaller xb [zero shot example] preview redd imymsluu png width= format=png auto=webp s=dbdaeefcdef big collection prompts \~ prompts datasets released [ github com bigscience workshop promptsource] github com bigscience workshop promptsource along model paper international collaborative effort people across organizations group included dedicated researchers engineers different universities companies think tanks model [ huggingface co bigscience tpp] huggingface co bigscience tpp repo [ github com bigscience workshop promptsource] github com bigscience workshop promptsource paper [ arxiv org abs ] arxiv org abs xb additionally models released hugging face model hub try browser [ huggingface co bigscience tpp] co qveaqkfmgk amp=
[P] Open-Source Synthetic Data Generation Library,19,qan38i,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qan38i/p_opensource_synthetic_data_generation_library/,6,"Hi r MachineLearning community  We are the guys from YData and some time ago we have created an open source project exclusively for synthetic data   [https  github com ydataai ydata synthetic] https  github com ydataai ydata synthetic     Hopefully  this will be useful for you 

The purpose of the library is to help you creating synthetic data  It could be used when the original data is not enough or when you don t want any identifiable information to ensure individual s privacy  As it is open sourced  it s always nice if you guys want to contribute or give some feedback   

Some tutorial notebooks to get started with the library  [https  github com ydataai ydata synthetic tree master examples regular] https  github com ydataai ydata synthetic tree master examples regular 

If you ve got any ideas or you want to discuss the implementations  feel free to hangout in our friendly synthetic data slack community at [https  slack ydata ai] https  slack ydata ai   ",1634565048.0,2021-10-18 15:50:48,hi r machinelearning community guys ydata time ago created open source project exclusively synthetic data [ github com ydataai ydata synthetic] github com ydataai ydata synthetic hopefully useful purpose library help creating synthetic data could used original data enough want identifiable information ensure individual privacy open sourced always nice guys want contribute give feedback tutorial notebooks get started library [ github com ydataai ydata synthetic tree master examples regular] github com ydataai ydata synthetic tree master examples regular got ideas want discuss implementations feel free hangout friendly synthetic data slack community [ slack ydata ai] slack ydata ai
[P] Open-Source Implementation of EGNNs with DGL,3,qb01ym,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qb01ym/p_opensource_implementation_of_egnns_with_dgl/,3,"Wanting to use the awesome Equivariant Graph Neural Network  EGNN  layer type from Satorras et al   in your Deep Graph Library  DGL  project  Look no further  I am open sourcing my DGL implementation of the layer on GitHub  Enjoy  [\ GraphNNs] https  twitter com hashtag GraphNNs src=hashtag_click  [\ research] https  twitter com hashtag research src=hashtag_click 

https  github com amorehead EGNN DGL",1634604059.0,2021-10-19 02:40:59,wanting use awesome equivariant graph neural network egnn layer type satorras et al deep graph library dgl project look open sourcing dgl implementation layer github enjoy [\ graphnns] twitter com hashtag graphnns src=hashtag_click [\ research] twitter com hashtag research src=hashtag_click github com amorehead egnn dgl
[D] I'm bored in my industry. what is your industry and how do you like it?,72,qafpph,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qafpph/d_im_bored_in_my_industry_what_is_your_industry/,60,"I work in e commerce and have for about  years  There are pros  my company has good resources for ML  money  cloud computing  empowered to work on what I want  and supports me and my team  but I m bored as hell  seems to be an issue where the business hasn’t caught up to the ML  so we’re stuck waiting for parts of the site to be updated and fixed   You can only optimize so much to sell things to folks online before you re limited by the Data Engineering feeds or UI UX team on motivating the product 

All that said  I don t want to do OpenAI or some crazy C  purist  cutting edge ML   fuck  I don t even really want to do ML with vision  I would just like to work in an industry that is a bit more fun  or existentially fulfilling on some level  What else is out there 

I know about Finance  Hospitality and Healthcare  but where else are some interesting applications of ML ",1634532958.0,2021-10-18 06:55:58,work e commerce years pros company good resources ml money cloud computing empowered work want supports team bored hell seems issue business hasn’t caught ml we’re stuck waiting parts site updated fixed optimize much sell things folks online limited data engineering feeds ui ux team motivating product said want openai crazy c purist cutting edge ml fuck even really want ml vision would like work industry bit fun existentially fulfilling level else know finance hospitality healthcare else interesting applications ml
[D] New macbook chips for ML?,0,qbaosy,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbaosy/d_new_macbook_chips_for_ml/,11," Yesterday I saw the announcement of the new chips  and it looks too powerful  Training models with those chips must be a pleasure  Is this a new era for training machine learning in laptops 

I think I will grab the M pro  Just wanted to chat and know what you guys think ",1634647275.0,2021-10-19 14:41:15,yesterday saw announcement new chips looks powerful training models chips must pleasure new era training machine learning laptops think grab pro wanted chat know guys think
[D] Overcoming Survivorship bias?,5,qas2cr,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qas2cr/d_overcoming_survivorship_bias/,5,I have a biased training data for only  Good Customers  and need to make a predictor for any given customer  How do I go about it  If someone could point me in a direction  I know I havent elaborated but if someone can list down the standard industry approaches to solving this problem ,1634579782.0,2021-10-18 19:56:22,biased training data good customers need make predictor given customer go someone could point direction know havent elaborated someone list standard industry approaches solving problem
[R] Mention Memory: Incorporating Factual Knowledge From Various Sources Into Transformers Without Supervision,6,qanxac,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qanxac/r_mention_memory_incorporating_factual_knowledge/,1,"A research team from the University of Southern California and Google proposes TOME  a  mention memory  approach to factual knowledge extraction for NLU tasks  A transformer model with attention over a semi parametric representation of the entire Wikipedia text corpus  TOME can extract information without supervision and achieves strong performance on multiple open domain question answering benchmarks  

Here is a quick read  [Mention Memory  Incorporating Factual Knowledge From Various Sources Into Transformers Without Supervision ] https  syncedreview com    deepmind podracer tpu based rl frameworks deliver exceptional performance at low cost  

The paper *Mention Memory  Incorporating Textual Knowledge into Transformers Through Entity Mention Attention* is on [arXiv] https  arxiv org abs    ",1634567745.0,2021-10-18 16:35:45,research team university southern california google proposes tome mention memory approach factual knowledge extraction nlu tasks transformer model attention semi parametric representation entire wikipedia text corpus tome extract information without supervision achieves strong performance multiple open domain question answering benchmarks quick read [mention memory incorporating factual knowledge various sources transformers without supervision ] syncedreview com deepmind podracer tpu based rl frameworks deliver exceptional performance low cost paper *mention memory incorporating textual knowledge transformers entity mention attention* [arxiv] arxiv org abs
[P] Progress with OpenCL backend for pytorch,115,qa85d4,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qa85d4/p_progress_with_opencl_backend_for_pytorch/,23,"I finally managed to train on Pytorch with OpenCL several common vision networks  alexnet  resnet etc 

 https  github com artyom beilis pytorch_dlprim 

Performance is very good  Also lower than native pytorch and little bit lower than dlprimitives microframework  it seems to run mostly on par with TF giving   of TF performance in training and same performance in inference

Training or gtx  of batch =  images x  units ms per batch  Lower is better 

  Framework         alexnet    resnet   resnet    mobilenet  
 
 pytorch cuda                                 
 pytorch opencl                               
 dlprimitives                                 
 keras tf cuda                           ",1634506105.0,2021-10-17 23:28:25,finally managed train pytorch opencl several common vision networks alexnet resnet etc github com artyom beilis pytorch_dlprim performance good also lower native pytorch little bit lower dlprimitives microframework seems run mostly par tf giving tf performance training performance inference training gtx batch = images x units ms per batch lower better framework alexnet resnet resnet mobilenet pytorch cuda pytorch opencl dlprimitives keras tf cuda
[P] Colab Notebook: Log and visualize StyleGAN3 training runs,2,qatlio,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qatlio/p_colab_notebook_log_and_visualize_stylegan3/,0,"Hi everyone 

Wanted to share a Colab notebook that I thought folks here might appreciate  It allows you to log and visualize [StyleGAN] https  github com NVlabs stylegan  training runs  images  videos  metrics  hyperparams —using Comet for the logging visualization

Full disclaimer  I work at Comet as their Head of Community  and to log your own training runs  you do need a free Comet account  But you can run the notebook without the logging as well  and we thought something like this might be of use to folks who are currently experimenting with the new StyleGAN architecture 

Here s the link to the notebook if it s of any interest  [https  colab research google com drive \_Mk\_pPHlkLLvAs\_aeXjiNcaC scrollTo=HEaeYEdWiKPY] https  colab research google com drive _Mk_pPHlkLLvAs_aeXjiNcaC scrollTo=HEaeYEdWiKPY 

And I ve included a GIF below of what the visualization looks like in the Comet UI  Thanks  everyone    ",1634584086.0,2021-10-18 21:08:06,hi everyone wanted share colab notebook thought folks might appreciate allows log visualize [stylegan] github com nvlabs stylegan training runs images videos metrics hyperparams —using comet logging visualization full disclaimer work comet head community log training runs need free comet account run notebook without logging well thought something like might use folks currently experimenting new stylegan architecture link notebook interest [ colab research google com drive \_mk\_pphlkllvas\_aexjincac scrollto=heaeyedwikpy] colab research google com drive _mk_pphlkllvas_aexjincac scrollto=heaeyedwikpy included gif visualization looks like comet ui thanks everyone
[R] Frozen in Time: Learning a Joint Text-Video Embedding for Retrieval (+ live demo),8,qajx50,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qajx50/r_frozen_in_time_learning_a_joint_textvideo/,0,"Paper  [Frozen in Time  A Joint Video and Image Encoder for End to End Retrieval] https  arxiv org pdf   pdf 

**Live Demo ** [**http  meru robots ox ac uk frozen in time **] http  meru robots ox ac uk frozen in time  ** visual search over millions of videos  **

Project page  [https  www robots ox ac uk \~vgg research frozen in time ] https  www robots ox ac uk ~vgg research frozen in time 

Code  [https  github com m bain frozen in time] https  github com m bain frozen in time 

New Public Dataset  [https  m bain github io webvid dataset ] https  m bain github io webvid dataset    M captioned videos  M coming soon 

Summary 

 End to end encoder for visual retrieval that uses only self attention blocks  This allows flexible training of millions of variable length videos and images jointly 

Abstract 

 Our objective in this work is video text retrieval   in particular a joint embedding that enables efficient text to video retrieval  The challenges in this area include the design of the visual architecture and the nature of the training data  in that the available large scale video text training datasets  such as HowToM  are noisy and hence competitive performance is achieved only at scale through large amounts of compute  We address both these challenges in this paper We propose an end to end trainable model that is designed to take advantage of both large scale image and video captioning datasets  Our model is an adaptation and extension of the recent ViT and Timesformer architectures  and consists of attention in both space and time  The model is flexible and can be trained on both image and video text datasets  either independently or in conjunction  It is trained with a curriculum learning schedule that begins by treating images as  frozen  snapshots of video  and then gradually learns to attend to increasing temporal context when trained on video datasets  We also provide a new video text pretraining dataset WebVid M  comprised of over two million videos with weak captions scraped from the internet  Despite training on datasets that are an order of magnitude smaller  we show that this approach yields state of the art results on standard downstream video retrieval benchmarks including MSR VTT  MSVD  DiDeMo and LSMDC ",1634552848.0,2021-10-18 12:27:28,paper [frozen time joint video image encoder end end retrieval] arxiv org pdf pdf **live demo ** [**http meru robots ox ac uk frozen time **] http meru robots ox ac uk frozen time ** visual search millions videos ** project page [ www robots ox ac uk \~vgg research frozen time ] www robots ox ac uk ~vgg research frozen time code [ github com bain frozen time] github com bain frozen time new public dataset [ bain github io webvid dataset ] bain github io webvid dataset captioned videos coming soon summary end end encoder visual retrieval uses self attention blocks allows flexible training millions variable length videos images jointly abstract objective work video text retrieval particular joint embedding enables efficient text video retrieval challenges area include design visual architecture nature training data available large scale video text training datasets howtom noisy hence competitive performance achieved scale large amounts compute address challenges paper propose end end trainable model designed take advantage large scale image video captioning datasets model adaptation extension recent vit timesformer architectures consists attention space time model flexible trained image video text datasets either independently conjunction trained curriculum learning schedule begins treating images frozen snapshots video gradually learns attend increasing temporal context trained video datasets also provide new video text pretraining dataset webvid comprised two million videos weak captions scraped internet despite training datasets order magnitude smaller show approach yields state art results standard downstream video retrieval benchmarks including msr vtt msvd didemo lsmdc
[P] Trained an AI with ML to navigate an obstacle course from Rocket League,2149,kp5pxi,MachineLearning,https://gfycat.com/oldfashionedhorriblegreathornedowl,57,nan,1609621471.0,2021-01-02 22:04:31,nan
[P] Doing a clone of Rocket League for AI experiments. Trained an agent to air dribble the ball.,2943,klbvaw,MachineLearning,https://v.redd.it/379qv12hrs761,67,nan,1609104382.0,2020-12-27 22:26:22,nan
